\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 5: Unsupervised Learning}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning - Overview}
    \begin{block}{Overview of the Chapter}
        Unsupervised learning is a pivotal aspect of machine learning that focuses on identifying patterns and structures in unlabeled data without any predefined outcomes. 
    \end{block}

    In this chapter, we will explore the following key areas:
    \begin{enumerate}
        \item \textbf{Definition of Unsupervised Learning}
        \item \textbf{Key Techniques}
        \item \textbf{Applications}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Unsupervised Learning - Techniques and Applications}
    \begin{block}{Key Techniques}
        Common techniques include:
        \begin{itemize}
            \item Clustering
            \item Dimensionality Reduction
            \item Anomaly Detection
        \end{itemize}
    \end{block}

    \begin{block}{Applications}
        Unsupervised learning is widely utilized in various fields including:
        \begin{itemize}
            \item Marketing (customer segmentation)
            \item Biology (gene expression analysis)
            \item Image Recognition (feature extraction)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Unsupervised Learning in Machine Learning}
    \begin{itemize}
        \item \textbf{Data Discovery}: Enables the discovery of hidden patterns in data, crucial for exploratory data analysis.
        \item \textbf{Preprocessing}: Provides powerful ways to preprocess data for supervised learning, improving model performance.
        \item \textbf{Scenarios with Lack of Labels}: Allows for efficient work with vast amounts of unlabeled data.
        \item \textbf{Enhancing Understanding}: Aids in gaining insights about data for informed decision-making.
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Contrast with Supervised Learning
            \item Diverse Applications
            \item Technology Integration
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Example}
    Consider a customer database with information on purchasing behavior. Utilizing clustering techniques like K-Means can help segment customers into different groups based on similarities in their purchase patterns, allowing marketers to tailor specific campaigns for each group.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    As we proceed through this chapter, we will delve deeper into the specific methods, algorithms, and case studies that illustrate the power and utility of unsupervised learning. Our goal is to equip you with a comprehensive understanding of the concepts, enabling you to apply these techniques effectively in real-world scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Unsupervised Learning?}
    \begin{block}{Definition of Unsupervised Learning}
        Unsupervised learning is a type of machine learning where algorithms are trained on data without labeled outputs. The primary goal is to explore the underlying structure of the data and identify meaningful patterns.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Differences from Supervised Learning}
    \begin{block}{Comparison Table}
        \begin{tabular}{|l|l|l|}
            \hline
            Feature & Unsupervised Learning & Supervised Learning \\
            \hline
            Labeling & No labels or outputs are provided. & Labels are provided for training data. \\
            \hline
            Objective & Discover hidden patterns/structures. & Predict outcomes based on input data. \\
            \hline
            Examples of Tasks & Clustering, dimensionality reduction. & Classification, regression. \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Unsupervised vs Supervised Learning}
    \begin{itemize}
        \item \textbf{Unsupervised Learning Example:} 
        Clustering user data into segments based on purchasing behavior without knowing beforehand how many segments or which customers belong to which group.
        
        \item \textbf{Supervised Learning Example:} 
        Training a model to predict customer churn by using past data where each customer is labeled as 'churned' or 'not churned'.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Exploratory Analysis:} 
        Unsupervised learning is ideal for exploratory data analysis where the goal is to understand what data can reveal without pre-defined categories.
        
        \item \textbf{Applications:} 
        Can lead to insights such as customer segmentation, anomaly detection, and association mining.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Algorithms in Unsupervised Learning}
    \begin{enumerate}
        \item \textbf{K-means Clustering:} Groups data into K distinct clusters based on feature similarity.
            \begin{equation}
            \text{Minimize } J = \sum_{i=1}^{K} \sum_{j=1}^{n} || x_j^{(i)} - \mu_i ||^2 
            \end{equation}
        
        \item \textbf{Hierarchical Clustering:} Builds a tree of clusters and doesn’t require the number of clusters to be defined beforehand.
        
        \item \textbf{Principal Component Analysis (PCA):} Reduces dimensionality by transforming to a new set of variables (principal components).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Unsupervised learning plays a critical role in understanding complex data sets by providing insights that may not be readily apparent, making it a vital component of the data analysis toolkit. This overview distinguishes unsupervised learning from supervised learning and sets the stage for exploring its applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of Unsupervised Learning - Overview}
    \begin{block}{Overview}
        Unsupervised learning is a type of machine learning that finds patterns or groupings in data without prior labels or defined categories. 
        This approach is critical in many real-world applications where insights from unstructured data are essential.
    \end{block}

    Below we discuss two key applications: 
    \begin{itemize}
        \item Market segmentation
        \item Anomaly detection
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications - Market Segmentation}
    \begin{block}{Market Segmentation}
        \begin{itemize}
            \item \textbf{Definition:} Dividing a market into subsets of consumers with common needs and priorities.
            \item \textbf{Process:}
            \begin{itemize}
                \item \textbf{Clustering Techniques:} K-means, hierarchical clustering
                \item \textbf{Data Used:} Customer demographics, buying behavior, product preferences
            \end{itemize}
            \item \textbf{Example:} Retail stores identify segments like budget shoppers and luxury buyers for targeted marketing strategies.
            \item \textbf{Benefits:}
            \begin{itemize}
                \item Improved targeting for marketing campaigns
                \item Enhanced customer experience through tailored recommendations
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Applications - Anomaly Detection}
    \begin{block}{Anomaly Detection}
        \begin{itemize}
            \item \textbf{Definition:} Identifying rare items or events that differ significantly from the majority of data.
            \item \textbf{Process:}
            \begin{itemize}
                \item \textbf{Techniques:} Isolation Forest, One-Class SVM, DBSCAN
                \item \textbf{Data Used:} Sensor data, transaction records, user behavior data
            \end{itemize}
            \item \textbf{Example:} Financial institutions flag unusual credit card transactions for potential fraud.
            \item \textbf{Benefits:}
            \begin{itemize}
                \item Enhanced security by identifying potential threats
                \item Reduced downtime by detecting faulty machines early
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Clustering}
    \begin{block}{What is Clustering?}
        Clustering is a fundamental technique in unsupervised learning that involves grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. 
        The objective is to identify patterns or structures within a dataset without prior knowledge of the categories or labels.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Role of Clustering in Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Data Exploration:} Summarizes and gains insights from large datasets by grouping similar items.
        \item \textbf{Preprocessing Step:} Reduces dimensionality and improves performance for machine learning algorithms.
        \item \textbf{Feature Engineering:} Treats clusters as new features in supervised learning tasks.
        \item \textbf{Pattern Recognition:} Aids in recognizing patterns or trends that may not be obvious.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{No Labeled Data:} Clustering does not rely on labeled data and uncovers hidden patterns.
            \item \textbf{Diverse Applications:} Commonly used in marketing, image recognition, and document categorization.
            \item \textbf{Similarity Measures:} Effectiveness depends on the metric used (e.g., Euclidean distance, Manhattan distance).
        \end{itemize}
    \end{block}
    
    \begin{block}{Example of Clustering}
        \textbf{Customer Segmentation:}
        \begin{enumerate}
            \item Group A: Young, low-income, low spending.
            \item Group B: Middle-aged, high-income, high spending.
            \item Group C: Retired, fixed income, moderate spending.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula for Distance Calculation}
    The Euclidean distance between two points \( p = (x_1, y_1) \) and \( q = (x_2, y_2) \) in a 2D space is calculated as:
    \begin{equation}
        d(p, q) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Wrap-up and Next Steps}
    \begin{block}{Wrap-up}
        In summary, clustering is vital for uncovering meaningful groupings in unlabelled data, with applications ranging from market analysis to biological data interpretation. Understanding clustering lays the groundwork for exploring algorithms such as K-means.
    \end{block}
    
    \begin{block}{Next Steps}
        Prepare to learn about the K-means clustering algorithm—the steps involved and how it functions to determine clusters based on data attributes!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Introduction}
    K-means clustering is a popular unsupervised learning algorithm used to partition a dataset into K distinct, non-overlapping groups (clusters). 
    It is particularly effective for exploring data patterns and has applications in:
    \begin{itemize}
        \item Customer segmentation
        \item Image compression
        \item Market research
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Key Concepts}
    \begin{itemize}
        \item \textbf{Clustering}: The task of grouping similar data points together.
        \item \textbf{Centroid}: The center point of a cluster, calculated as the mean of all data points in that cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Steps}
    \begin{enumerate}
        \item \textbf{Initialization}:
        \begin{itemize}
            \item Choose the number of clusters (K).
            \item Randomly select K data points as initial centroids.
        \end{itemize}
        
        \item \textbf{Assignment Step}:
        \begin{itemize}
            \item Calculate the distance from each data point to each centroid.
            \item Assign each data point to the nearest centroid.
        \end{itemize}
        
        \item \textbf{Update Step}:
        \begin{itemize}
            \item Compute the new centroid for each cluster by taking the mean of all points assigned.
        \end{itemize}
        
        \item \textbf{Iteration}:
        \begin{itemize}
            \item Repeat assignment and update steps until convergence.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Example}
    Consider a dataset of customer ages and spending scores with K=3:
    \begin{itemize}
        \item Initialization: Randomly select three customers as initial centroids.
        \item Assignment: A customer aged 25 with a spending score of 80 may be assigned to the closest centroid.
        \item Update: Centroids are recalculated based on the average age and spending score of customers in each cluster.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Mathematical Representation}
    \begin{block}{Distance Formula (Euclidean distance)}
        \begin{equation}
            d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
        \end{equation}
    \end{block}

    \begin{block}{Centroid Calculation}
        If \( C_k \) is the centroid of cluster \( k \) and \( N_k \) is the number of points in cluster \( k \):
        \begin{equation}
            C_k = \frac{1}{N_k} \sum_{x_i \in Cluster_k} x_i
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Key Points}
    \begin{itemize}
        \item Sensitivity to the choice of K; can converge to local minima.
        \item Best with spherical clusters of similar sizes and densities.
        \item Proper initialization is critical; K-means++ is a popular method for better centroid placement.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering - Summary}
    K-means clustering is an intuitive and efficient algorithm for partitioning datasets into meaningful clusters based on similarity. 
    Key steps include:
    \begin{itemize}
        \item Initialization
        \item Assignment
        \item Update
    \end{itemize}
    Understanding these steps aids in better applications across various data analysis scenarios.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding K-means Algorithm - Overview}
    \begin{block}{Overview}
        The K-means algorithm is a popular unsupervised learning technique used for clustering data into groups based on feature similarity. Understanding its core components is essential for applying it effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding K-means Algorithm - Key Steps}
    \begin{enumerate}
        \item Initialization
        \item Assignment Step
        \item Update Step
        \item Repeat Steps
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Initialization}
    \begin{itemize}
        \item \textbf{Definition:} This first step involves selecting initial centroids, which are the center points of the clusters.
        \item \textbf{Method:} 
        \begin{itemize}
            \item Randomly select $K$ data points from the dataset as initial centroids.
            \item Alternatively, methods like K-means++ can be used to improve convergence speed by selecting better initial centroids.
        \end{itemize}
        \item \textbf{Example:} For a dataset of customer purchases, randomly selecting three customers as initial centroids sets the foundation for clustering based on their purchasing behavior.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Assignment Step}
    \begin{itemize}
        \item \textbf{Definition:} Each data point is assigned to the nearest centroid, forming distinct clusters.
        \item \textbf{Distance Metric:} Typically, Euclidean distance is used to measure how far each point is from the centroid.
        \item \textbf{Formula:}
        \begin{equation}
            \text{Cluster}(x_i) = \arg\min_{j} \| x_i - c_j \|^2
        \end{equation}
        where $c_j$ is the centroid of cluster $j$.
        \item \textbf{Example:} For 100 customer purchase amounts, each customer is grouped into Cluster A if their amount is closest to the centroid of Cluster A.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Update Step}
    \begin{itemize}
        \item \textbf{Definition:} Centroids are recalculated based on the mean of all points in each cluster after assigning all points.
        \item \textbf{Formula:}
        \begin{equation}
            c_j = \frac{1}{N_j} \sum_{x_i \in C_j} x_i
        \end{equation}
        where $N_j$ is the number of points in cluster $C_j$.
        \item \textbf{Example:} If Cluster A has five customers with amounts \$20, \$25, \$30, \$22, and \$28, the new centroid is calculated as:
        \begin{equation}
            c_A = \frac{20 + 25 + 30 + 22 + 28}{5} = 25
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Repeat Steps and Important Concepts}
    \begin{itemize}
        \item \textbf{Convergence:} Steps 2 and 3 are repeated until the centroids no longer change significantly, indicating convergence.
        \item \textbf{Completion:} Final clusters can be interpreted for insights.
    \end{itemize}

    \begin{block}{Important Concept: Centroid}
        \begin{itemize}
            \item \textbf{Definition:} The centroid is the average position of all points in a cluster, serving as the representative point for that cluster.
            \item \textbf{Key Points:}
            \begin{itemize}
                \item The number of clusters $K$ must be chosen carefully.
                \item Sensitive to initialization; multiple runs may be needed.
                \item Limitations: Assumes spherical clusters of similar size and density.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The K-means algorithm provides a simple yet powerful approach to clustering, enabling data analysis and segmentation in fields like marketing and finance. Understanding initialization, assignment, update steps, and the role of centroids is key to leveraging this tool effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Introduction}
    \begin{block}{What is Hierarchical Clustering?}
        Hierarchical clustering is an unsupervised learning method used to group data points into a hierarchy of clusters. 
        Unlike **K-means**, which partitions data into a predetermined number of clusters, hierarchical clustering builds a tree-like structure that represents nested groupings of data points. 
        This allows for exploration of data at various levels of granularity.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Two Main Approaches}
    \begin{block}{Agglomerative Clustering (Bottom-Up Approach)}
        \begin{itemize}
            \item \textbf{Process:} Starts with each data point as its own individual cluster and repeatedly merges the two closest clusters until all points are contained in a single cluster or a specified number of clusters is reached.
            \item \textbf{Example:} Imagine ten friends (data points) each standing alone at a party. They pair up based on how well they know each other (merging clusters) until everyone is in one large group.
            \item \textbf{Algorithm Steps:}
                \begin{enumerate}
                    \item Compute the distance matrix between all data points.
                    \item Merge the two closest clusters.
                    \item Update the distance matrix.
                    \item Repeat until all points are merged or a stopping criterion is satisfied.
                \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Divisive Clustering}
    \begin{block}{Divisive Clustering (Top-Down Approach)}
        \begin{itemize}
            \item \textbf{Process:} Begins with all data points in a single cluster and recursively splits them into smaller clusters until each data point is its own cluster or a stopping criterion is met.
            \item \textbf{Example:} Envision a sports team (data points) that starts as a single entity. The coach (algorithm) divides the team based on skill levels into specific positions, continually breaking down sub-groups until individual players are identified.
            \item \textbf{Algorithm Steps:}
                \begin{enumerate}
                    \item Start with all data points in one cluster.
                    \item Identify the most dissimilar subgroup.
                    \item Split this subgroup into smaller clusters.
                    \item Continue this process until all points are separate or until a desired structure is achieved.
                \end{enumerate}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Key Points}
    \begin{itemize}
        \item \textbf{Flexibility:} Hierarchical clustering does not require specifying the number of clusters in advance, making it a flexible choice for exploratory data analysis.
        \item \textbf{Dendrogram Representation:} Results can be visualized with a dendrogram, which displays the arrangement of clusters and the distances at which merges or splits occur.
        \item \textbf{Distance Metrics:} The choice of distance metric (Euclidean, Manhattan, etc.) can significantly influence clustering results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering - Formulas and Code}
    \begin{block}{Distance Calculation (Euclidean)}
        \begin{equation}
            d(p,q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
        \end{equation}
    \end{block}
    \begin{block}{Basic Pseudocode for Agglomerative Clustering}
        \begin{lstlisting}[language=Python]
def agglomerative_clustering(data):
    clusters = [[point] for point in data]  # Start with individual clusters
    while len(clusters) > 1:
        # Find closest clusters
        closest_clusters = find_closest(clusters)
        # Merge closest clusters
        clusters = merge_clusters(clusters, closest_clusters)
    return clusters
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dendrograms in Hierarchical Clustering - Overview}
    \begin{block}{Understanding Dendrograms}
        \begin{itemize}
            \item \textbf{Definition}: A dendrogram is a tree-like diagram that illustrates the arrangement of the clusters in hierarchical clustering.
            \item It shows how clusters are built based on their distances from each other.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Dendrograms Work}
    \begin{block}{Hierarchical Structure}
        \begin{enumerate}
            \item \textbf{Agglomerative Clustering}:
                \begin{itemize}
                    \item Starts with individual data points.
                    \item Merges them into larger clusters based on similarity.
                    \item Continues until all points are in a single cluster or a set number of clusters is reached.
                \end{itemize}
            \item \textbf{Divisive Clustering}:
                \begin{itemize}
                    \item Starts with a single cluster.
                    \item Splits it into smaller clusters.
                \end{itemize}
        \end{enumerate}
    \end{block}
    \begin{block}{Visual Representation}
        \begin{itemize}
            \item The x-axis typically represents the data points or clusters.
            \item The y-axis represents the distance or dissimilarity between clusters.
        \end{itemize}
    \end{block}
    \includegraphics[width=\textwidth]{URL-to-example-dendrogram}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reading a Dendrogram}
    \begin{block}{Understanding Key Components}
        \begin{itemize}
            \item \textbf{Nodes}: Each node represents a cluster. The height at which two clusters merge indicates the distance at which they were combined.
            \item \textbf{Branches}: The length of the branches shows the similarity between clusters – shorter branches indicate more similar clusters.
        \end{itemize}
    \end{block}

    \begin{block}{Cutting the Dendrogram}
        \begin{itemize}
            \item Selecting a height on the dendrogram allows determination of the number of clusters.
            \item The point at which the dendrogram is cut significantly influences the final clusters.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case & Conclusion}
    \begin{block}{Example Use Case}
        \begin{itemize}
            \item Consider a dataset with different species of flowers characterized by petal length, sepal width, etc.
            \item A dendrogram shows:
                \begin{itemize}
                    \item Species A and B are closely related.
                    \item Species C is distinctly separate.
                \end{itemize}
            \item By cutting the dendrogram at a certain level, species A and B can be grouped together, while species C stands alone.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        \begin{itemize}
            \item Dendrograms provide a clear visual representation of cluster relationships in hierarchical clustering.
            \item Understanding them helps in interpreting clustering outcomes and making data categorization decisions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading}
    \begin{itemize}
        \item Explore different linkage criteria such as single, complete, and average linkage for further understanding of dendrogram construction.
        \item Practice clustering with various datasets to see how adjusting the cut height impacts the resulting clusters.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Clustering Methods}
    \begin{itemize}
        \item Overview of clustering in unsupervised learning.
        \item Two prevalent methods: K-means and Hierarchical Clustering.
        \item Comparison dimensions: Speed, Scalability, and Complexity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-means Clustering}
    \begin{enumerate}
        \item \textbf{Speed}:
        \begin{itemize}
            \item Time complexity: \( O(n \cdot k \cdot i) \) where:
            \begin{itemize}
                \item \( n \) = number of data points,
                \item \( k \) = number of clusters,
                \item \( i \) = number of iterations until convergence.
            \end{itemize}
            \item Generally fast for large datasets when initialized well.
        \end{itemize}
        
        \item \textbf{Scalability}:
        \begin{itemize}
            \item Scalable to large datasets; performance scales linearly.
            \item Each iteration processes all points relative to cluster centroids.
        \end{itemize}

        \item \textbf{Complexity}:
        \begin{itemize}
            \item Requires specifying the number of clusters (k) beforehand.
            \item Sensitive to outliers and requires careful initialization.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering}
    \begin{enumerate}
        \item \textbf{Speed}:
        \begin{itemize}
            \item Generally slower than K-means with time complexity \( O(n^2) \) to \( O(n^3) \).
            \item Dendrogram construction is computationally intensive.
        \end{itemize}

        \item \textbf{Scalability}:
        \begin{itemize}
            \item Less scalable compared to K-means; high computational overhead.
            \item Performance degrades as dataset size increases.
        \end{itemize}

        \item \textbf{Complexity}:
        \begin{itemize}
            \item No need to specify the number of clusters in advance.
            \item Captures complex relationships through its tree structure.
            \item More robust to outliers than K-means.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Use Cases}:
        \begin{itemize}
            \item K-means ideal for large datasets with well-separated clusters.
            \item Hierarchical clustering provides insights through dendrograms.
        \end{itemize}
        
        \item \textbf{Nature of Data}:
        \begin{itemize}
            \item K-means requires numeric, uniformly scaled data.
            \item Hierarchical clustering accommodates varying data types.
        \end{itemize}

        \item \textbf{Interactivity}:
        \begin{itemize}
            \item Hierarchical clustering allows dynamic exploration via dendrograms.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Choice between K-means and Hierarchical Clustering depends on dataset characteristics and analysis goals.
        \item Understanding differences in speed, scalability, and complexity aids in method selection.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Considerations}
    Clustering is a powerful unsupervised learning technique for grouping similar data points. 
    However, it presents several challenges that need to be addressed for effective application:
    \begin{itemize}
        \item Choice of algorithm
        \item Determining the number of clusters (K)
        \item Scalability and computational efficiency
        \item Data characteristics and their impact
        \item Interpretability of clusters
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Clustering}
    \begin{enumerate}
        \item \textbf{Choice of Algorithm}
            \begin{itemize}
                \item Different algorithms have varied strengths and weaknesses.
                \item \textit{Example:} K-means vs. hierarchical clustering.
            \end{itemize}
        
        \item \textbf{Determining the Number of Clusters (K)}
            \begin{itemize}
                \item Subjective choice affecting clustering outcomes.
                \item Methods:
                    \begin{itemize}
                        \item Elbow Method
                        \item Silhouette Score
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Challenges and Key Points}
    \begin{enumerate}[resume]
        \item \textbf{Scalability and Computational Efficiency}
            \begin{itemize}
                \item Clustering algorithms become computationally expensive with large datasets.
                \item \textit{Example:} K-means performance issues with high-dimensional data.
            \end{itemize}

        \item \textbf{Data Characteristics}
            \begin{itemize}
                \item High dimensionality can obscure meaningful distance measures.
                \item Noise and outliers can skew clustering results.
            \end{itemize}

        \item \textbf{Interpretability of Clusters}
            \begin{itemize}
                \item Ensuring clusters are meaningful in context.
            \end{itemize}
    \end{enumerate}
    
    \textbf{Conclusion:} Understanding these challenges is critical to navigating clustering tasks effectively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example for Clustering}
    Here is a Python code snippet to illustrate the use of K-means and the Elbow method for determining the optimal number of clusters:
    \begin{block}{Python Code}
        \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Sample data
data = ...

# Elbow method to determine optimal clusters
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k).fit(data)
    sse.append(kmeans.inertia_)
    
plt.plot(range(1, 11), sse)
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.title('Elbow Method For Optimal k')
plt.show()

# Calculate silhouette score for a specific k
k = 3
kmeans = KMeans(n_clusters=k).fit(data)
score = silhouette_score(data, kmeans.labels_)
print(f'Silhouette Score for k={k}: {score}')
        \end{lstlisting}
    \end{block}
\end{frame}


\end{document}