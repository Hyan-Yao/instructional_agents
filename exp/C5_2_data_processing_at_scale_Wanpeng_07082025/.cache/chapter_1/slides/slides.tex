\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Introduction to Data Models]{Weeks 1-4: Introduction to Data Models and Query Processing}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Models and Query Processing}
    \begin{block}{Overview of Chapter Objectives}
        In this chapter, we will explore foundational concepts crucial for managing and retrieving data in databases. By the end, you should be able to:
        \begin{enumerate}
            \item \textbf{Define Data Models}: Understand data models and their role in database management.
            \item \textbf{Differentiate Types of Data Models}: Identify main types (hierarchical, network, relational, object-oriented).
            \item \textbf{Explain Query Processing}: Comprehend processes involved in querying and optimizing.
            \item \textbf{Connect Theory to Practice}: Write basic queries and understand their executions.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Models and Query Processing - Key Concepts}
    \begin{block}{Data Model}
        A data model is a conceptual representation of data structures. It specifies how data is stored, organized, and manipulated, crucial for aligning with specific use cases.
        \begin{itemize}
            \item \textbf{Example}: In a \textbf{Relational Model}, data is stored in tables that relate through foreign keys.
        \end{itemize}
    \end{block}

    \begin{block}{Query Processing}
        This process involves interpreting user queries, accessing relevant data, and returning results. Key steps include:
        \begin{itemize}
            \item \textbf{Parsing}: Analyzing query syntax.
            \item \textbf{Optimization}: Rewriting the query for performance improvement.
            \item \textbf{Execution}: Retrieving data from the database.
        \end{itemize}
        \begin{itemize}
            \item \textbf{Example}: A SQL query `SELECT * FROM users WHERE age > 18` undergoes parsing and optimization before execution.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Models and Query Processing - Importance and Takeaways}
    \begin{block}{Importance of Learning Data Models and Query Processing}
        \begin{itemize}
            \item \textbf{Data Integrity}: Helps maintain data integrity and consistency.
            \item \textbf{Efficient Querying}: Leads to efficient data retrieval, reducing resource use and improving application performance.
        \end{itemize}
    \end{block}

    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item Data models provide a blueprint for data organization and manipulation.
            \item Query processing is essential for efficient extraction of meaningful information from databases.
            \item Both concepts are interrelated and foundational for database management and design.
        \end{enumerate}
    \end{block}

    This chapter sets the groundwork for advanced topics such as database normalization, transaction management, and data warehousing, addressed in subsequent sections. Be prepared to apply these concepts in practical scenarios.
\end{frame}

\begin{frame}[fragile]{Understanding Data Models - Definition}
    \begin{block}{Definition of Data Models}
        A \textbf{data model} is a conceptual representation of data structures and relationships within a database. It serves as a blueprint for how data is stored, accessed, and processed.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Conceptual Data Model}: Focuses on high-level relationships and captures the overall structure of information. Less concerned with implementation details.
        \item \textbf{Logical Data Model}: Breaks down the structure into detail, defining attributes, keys, and relationships without considering physical storage.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Understanding Data Models - Importance}
    \begin{block}{Importance of Data Models in Database Management}
        \begin{enumerate}
            \item \textbf{Structured Approach}: Provides a methodology for organizing data logically and meaningfully.
            \item \textbf{Improved Communication}: Serves as a tool among stakeholders, ensuring a common understanding of data interrelations.
            \item \textbf{Facilitates Database Design}: Acts as a roadmap for developers during the database design process.
            \item \textbf{Enhances Data Integrity}: Enforces rules for data validity and integrity, reducing errors.
            \item \textbf{Adaptability to Changes}: Allows modifications with ease, accommodating changes in business requirements or technology.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Understanding Data Models - Example and Conclusion}
    \begin{block}{Example of a Data Model}
        Consider a library system with two entities: \textbf{Books} and \textbf{Authors}.
        \begin{itemize}
            \item \textbf{Books}: Attributes include BookID, Title, Genre, and AuthorID (foreign key).
            \item \textbf{Authors}: Attributes include AuthorID, Name, and Birthdate.
            \item \textbf{Relationship}: One Author can write multiple Books (one-to-many).
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Foundation in database management for data organization and access.
            \item Essential for efficient database design, data quality, and compliance.
            \item Understanding data models aids in handling complex structures and optimizing query processing.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding data models is critical for effective database management, clarifying data integrity, and facilitating communication.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Relational Databases}
  \frametitle{Overview of Relational Databases}
  Relational databases are a fundamental type of database management system that organizes data into structured formats using tables. This approach provides a means to manage and retrieve data efficiently, making it one of the most widely used database paradigms.
\end{frame}

\begin{frame}[fragile]{Key Concepts - Part 1}
  \frametitle{Key Concepts}
  \begin{enumerate}
    \item \textbf{Tables:}
      \begin{itemize}
        \item Data is organized in tables, also known as relations.
        \item Each table consists of rows (records) and columns (attributes).
        \item \textit{Example:} A table named \texttt{Students} might have columns like \texttt{StudentID}, \texttt{Name}, \texttt{Age}, and \texttt{Major}.
      \end{itemize}

    \item \textbf{Primary Key:}
      \begin{itemize}
        \item A unique identifier for each record in a table, ensuring no two rows are identical.
        \item \textit{Example:} In the \texttt{Students} table, \texttt{StudentID} can be used as the primary key.
      \end{itemize}

    \item \textbf{Foreign Key:}
      \begin{itemize}
        \item A field that uniquely identifies a row in another table, establishing relationships between tables.
        \item \textit{Example:} A \texttt{Courses} table might have a \texttt{ProfessorID} that links to a \texttt{Professors} table.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Concepts - Part 2}
  \frametitle{Key Concepts}
  \begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{SQL (Structured Query Language):}
      \begin{itemize}
        \item The standard programming language for managing and manipulating relational databases.
        \item \textit{Example Query:}
          \begin{lstlisting}[basicstyle=\ttfamily]
SELECT Name, Major FROM Students WHERE Age > 20;
          \end{lstlisting}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Structure of Relational Databases}
  \frametitle{Structure of Relational Databases}
  \begin{itemize}
    \item \textbf{Schema:} The framework that defines the structure of the database, including tables, fields, data types, and relationships.
    \item \textbf{Normalization:} The process of organizing data to minimize redundancy and improve data integrity by dividing large tables into smaller ones and establishing relationships.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Benefits of Relational Databases}
  \frametitle{Key Benefits}
  \begin{enumerate}
    \item \textbf{Data Integrity:} Enforces rules to ensure accuracy and consistency of data through constraints like primary key and foreign key relationships.
    \item \textbf{Flexibility:} Allows for the addition of new fields to tables without disrupting existing data.
    \item \textbf{Complex Queries:} Users can perform sophisticated queries to extract useful insights from data using SQL.
    \item \textbf{ACID Compliance:} Guarantees reliable transactions through Atomicity, Consistency, Isolation, and Durability.
    \item \textbf{Scalability:} Designed to handle larger volumes of data efficiently as the system grows.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Summary}
  \frametitle{Summary}
  Relational databases are crucial tools in data management, allowing businesses and organizations to store data systematically while ensuring data integrity and enabling complex querying capabilities. Their structured format and use of tables make them intuitive and powerful for various applications.
\end{frame}

\begin{frame}[fragile]{NoSQL Databases - Introduction}
    \begin{block}{Introduction to NoSQL Databases}
        NoSQL (Not Only SQL) databases are designed to handle a variety of data storage, management, and retrieval needs that traditional relational databases may struggle with. 
    \end{block}
    \begin{itemize}
        \item Unlike relational databases which store data in fixed schemas and utilize SQL for querying
        \item NoSQL databases offer a flexible approach, enabling scalability and performance improvements suitable for modern applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{NoSQL Databases - Key Characteristics}
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Schema Flexibility}: NoSQL databases do not require a fixed schema, allowing for a more dynamic organization of data.
            \item \textbf{Horizontal Scalability}: Easily scaled across distributed systems by adding more servers.
            \item \textbf{High Performance}: Optimized for speed and efficiency, suitable for big data and real-time applications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{NoSQL Databases - Types}
    \begin{block}{Types of NoSQL Databases}
        \begin{enumerate}
            \item \textbf{Document Stores}
                \begin{itemize}
                    \item Description: Store data in documents (e.g., JSON, BSON, XML).
                    \item Examples: MongoDB, Firebase Firestore.
                    \item Use Case: Content management systems, blogs.
                \end{itemize}
            \item \textbf{Key-Value Stores}
                \begin{itemize}
                    \item Description: Data is stored as key-value pairs.
                    \item Examples: Redis, Amazon DynamoDB.
                    \item Use Case: Caching, user session management.
                \end{itemize}
            \item \textbf{Column-family Stores}
                \begin{itemize}
                    \item Description: Store data in columns rather than rows.
                    \item Examples: Apache Cassandra, HBase.
                    \item Use Case: Time-series data, analytics applications.
                \end{itemize}
            \item \textbf{Graph Databases}
                \begin{itemize}
                    \item Description: Store data as graphs (nodes, edges).
                    \item Examples: Neo4j, Amazon Neptune.
                    \item Use Case: Social networks, recommendation systems.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{NoSQL Databases - Trade-offs and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Use Cases}: Ideal for applications requiring high scalability and varying data types (e.g., IoT data, user-generated content).
            \item \textbf{Trade-offs}: They may lack some features of relational databases, such as complex queries and strict ACID compliance.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        NoSQL databases provide a powerful alternative to traditional relational databases, addressing the challenges of modern data demands with their diverse data models and high-performance capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Graph Databases - Overview}
    \begin{block}{What Are Graph Databases?}
        Graph databases are a type of NoSQL database that use graph structures with nodes, edges, and properties to represent and store data.
        They excel in scenarios where relationships between entities are key to understanding the data. 
        Unlike traditional relational databases that store data in tables, graph databases model data as interconnected entities, allowing for more efficient query processing of complex relationships.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Nodes:} The primary entities in a graph (e.g., users, products, locations).
        \item \textbf{Edges:} The connections between nodes that represent relationships (e.g., friendships, purchases).
        \item \textbf{Properties:} Attributes providing additional information about nodes or edges (e.g., a user's age).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Graph Databases - Applications}
    \begin{block}{Applications of Graph Databases}
        \begin{enumerate}
            \item \textbf{Social Networks:} Analyze connections and interactions between users (e.g., Facebook).
            \item \textbf{Recommendation Engines:} Suggest products/content based on user behavior (e.g., Netflix).
            \item \textbf{Fraud Detection:} Identify suspicious patterns by examining relationships in transactions.
            \item \textbf{Network \& IT Operations:} Model IT infrastructure for better management and analysis.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example}
        Consider a simple social network graph:
        \begin{itemize}
            \item \textbf{Nodes:} Alice, Bob, Charlie
            \item \textbf{Edges:} Alice $\rightarrow$ Bob (friend), Alice $\rightarrow$ Charlie (follower), Bob $\rightarrow$ Charlie (follower)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Graph Databases - Advantages and Sample Query}
    \begin{block}{Advantages of Graph Databases}
        \begin{itemize}
            \item \textbf{Flexible Schema:} Easily accommodate new relationships or nodes.
            \item \textbf{Efficiency in Relationship Queries:} Fast retrieval through optimized structures.
            \item \textbf{Intuitive Modeling:} Natural representation of real-world problems.
        \end{itemize}
    \end{block}
    
    \begin{block}{Sample Query (using Cypher syntax)}
        To find all friends of Alice:
        \begin{lstlisting}
MATCH (a:Person {name: 'Alice'})-[:FRIEND]->(friend)
RETURN friend.name
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Ideal for complex relationships.
            \item Used across various industries.
            \item Enhance data modeling and query processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Database Schemas}
    \begin{block}{Definition of Database Schema}
        A \textbf{database schema} is a blueprint that defines the structure of a database. It specifies how data is organized, the relationships between data, and how data can be accessed. Essentially, the schema serves as a metadata layer describing the how's and what's of the data in a database.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Components of Database Schemas}
    \begin{enumerate}
        \item \textbf{Tables}: Core building blocks that store data in rows and columns.
            \begin{itemize}
                \item Example: "Students" table
                \begin{itemize}
                    \item \textbf{Columns}: StudentID, Name, Age, Major
                    \item \textbf{Rows}: Individual records for each student.
                \end{itemize}
            \end{itemize}
        \item \textbf{Relationships}: Define how different tables interact.
            \begin{itemize}
                \item One-to-One (1:1)
                \item One-to-Many (1:N)
                \item Many-to-Many (M:N), often with junction tables.
            \end{itemize}
        \item \textbf{Constraints}: Rules that enforce properties on data.
            \begin{itemize}
                \item Primary Key
                \item Foreign Key
                \item Unique \& Check Constraints
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Role in Data Organization}
    \begin{itemize}
        \item The database schema helps in \textbf{organizing data efficiently}, allowing for effective access, modification, and querying.
        \item It establishes a clear structure that supports data integrity and reduces redundancy.
    \end{itemize}
    
    \begin{block}{Example of a Simple Database Schema}
        \begin{verbatim}
        students_table: 
        -------------------------------------------------
        | StudentID (PK) | Name      | Age | Major      |
        -------------------------------------------------
        | 1              | Alice     | 22  | Computer Science |
        | 2              | Bob       | 20  | Mathematics  |
        -------------------------------------------------
        
        courses_table:
        -------------------------------------------------
        | CourseID (PK) | CourseName         | Credits |
        -------------------------------------------------
        | 101            | Data Structures     | 3      |
        | 102            | Database Systems     | 4      |
        -------------------------------------------------
        
        enrollments_table:
        -------------------------------------------------
        | EnrollmentID (PK) | StudentID (FK) | CourseID (FK) |
        -------------------------------------------------
        | 1                 | 1               | 101           |
        | 2                 | 1               | 102           |
        | 3                 | 2               | 101           |
        -------------------------------------------------
        \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Points to Emphasize and Conclusion}
    \begin{itemize}
        \item The schema serves as a foundation for data consistency and integrity.
        \item Understanding the layout of a schema is crucial for efficient querying and data manipulation.
        \item The design of a schema impacts storage efficiency and performance in data retrieval.
    \end{itemize}
    
    \begin{block}{Conclusion}
        A well-designed database schema is essential for effective data organization and management. Up next, we will explore the concept of \textbf{Normalization}, which further enhances the efficiency and integrity of databases.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normalization}
    Normalization is the process of organizing data in a database to minimize redundancy and improve data integrity. 
    It typically involves structuring a relational database to reduce dependency and eliminate duplicate data across related tables.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Normalization}
    \begin{itemize}
        \item \textbf{Reduces Data Redundancy:} 
        By spreading data across multiple related tables, normalization eliminates unnecessary copies of the same data.
        
        \item \textbf{Improves Data Integrity:} 
        Changes made in one location automatically propagate to all references, reducing stale or inconsistent data.
        
        \item \textbf{Enhances Data Organization:} 
        Normalized data is easier to manage and query, improving performance and streamlining data retrieval.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Levels of Normalization}
    Normalization is typically done in steps, known as \textbf{normal forms (NF)}.

    \begin{enumerate}
        \item \textbf{First Normal Form (1NF):} 
        Ensures all columns in a table contain atomic values.
        
        \item \textbf{Second Normal Form (2NF):} 
        Eliminates partial dependencies ensuring the whole key is needed for determining other columns.

        \item \textbf{Third Normal Form (3NF):} 
        Requires all attributes to be dependent only on the primary key, removing transitive dependency.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Normalization}
    Consider an unnormalized table:

    \begin{tabular}{|c|c|c|}
        \hline
        StudentID & StudentName & CourseTitle \\
        \hline
        1 & Alice & Math \\
        1 & Alice & English \\
        2 & Bob & History \\
        2 & Bob & Math \\
        \hline
    \end{tabular}

    \textbf{1NF:} Each course is in separate rows. 

    \textbf{2NF:} Create separate \textbf{Students} and \textbf{Courses} tables:

    \begin{tabular}{|c|c|}
        \hline
        StudentID & StudentName \\
        \hline
        1 & Alice \\
        2 & Bob \\
        \hline
    \end{tabular}

    \begin{tabular}{|c|c|}
        \hline
        StudentID & CourseTitle \\
        \hline
        1 & Math \\
        1 & English \\
        2 & History \\
        2 & Math \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Normalization is vital for maintaining data consistency and reducing redundancy.
        \item Each step addresses specific issues related to data integrity and design.
        \item Normalization may lead to performance trade-offs in some scenarios, especially with complex queries.
    \end{itemize}

    \textbf{Conclusion:} Normalization is essential for efficient database design. 

    \textbf{Next Steps:} Explore \textbf{Denormalization}, a strategy sometimes employed to enhance performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Denormalization - Definition}
    \begin{block}{Definition of Denormalization}
        Denormalization is the process of intentionally introducing redundancy into a database design by combining tables or duplicating data. It is aimed at improving data retrieval performance, especially in read-heavy applications. Unlike normalization, which minimizes redundancy and dependency, denormalization accepts some level of redundancy for efficiency.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Denormalization - Why?}
    \begin{itemize}
        \item \textbf{Performance Improvement:}
            \begin{itemize}
                \item Reduces the number of joins needed when querying data, speeding up query performance.
                \item Particularly beneficial for applications with a high read-to-write ratio.
            \end{itemize}
        
        \item \textbf{Simplified Queries:}
            \begin{itemize}
                \item Makes complex queries more straightforward, enhancing maintainability.
            \end{itemize}

        \item \textbf{Optimized Reporting:}
            \begin{itemize}
                \item Allows for pre-computed aggregates and summaries to be stored alongside detailed data.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Denormalization - Strategies}
    \begin{enumerate}
        \item \textbf{Combining Tables:}
            \begin{itemize}
                \item Merge frequently joined tables into a single table.
                \item \textit{Example:}
                \begin{lstlisting}
                Original Tables:
                Orders (OrderID, CustomerID, OrderDate)
                Customers (CustomerID, CustomerName, CustomerAddress)

                Denormalized Table:
                OrdersWithCustomerInfo (OrderID, CustomerName, CustomerAddress, OrderDate)
                \end{lstlisting}
            \end{itemize}

        \item \textbf{Adding Redundant Data:}
            \begin{itemize}
                \item Duplicate necessary fields across related tables to minimize joins.
            \end{itemize}

        \item \textbf{Creating Summary Tables:}
            \begin{itemize}
                \item Maintain separate summary tables with pre-aggregated data.
            \end{itemize}

        \item \textbf{Using Array/Data Structures:}
            \begin{itemize}
                \item Embed related records as arrays or nested structures within a single record.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Denormalization - Key Points}
    \begin{itemize}
        \item Denormalization is a design choice that compromises some data integrity for faster query performance.
        \item It's crucial to analyze query patterns and needs before denormalization, as it can complicate data management.
        \item Measure performance and find a balance between normalization and denormalization based on application requirements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Denormalization - Conclusion}
    Denormalization is a powerful database design technique that can significantly enhance performance when used wisely. However, it requires careful consideration of the trade-offs, especially regarding data integrity and maintenance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Data Models - Introduction}
    \begin{itemize}
        \item Data models are frameworks for storing, organizing, and manipulating data in a DBMS.
        \item Selecting the right data model influences:
        \begin{itemize}
            \item Efficiency 
            \item Performance 
            \item Scalability 
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Data Models - Key Models}
    \begin{enumerate}
        \item \textbf{Relational Model}
            \begin{itemize}
                \item Organized in tables with predefined schemas.
                \item Strengths:
                    \begin{itemize}
                        \item Data integrity through ACID properties
                        \item Powerful querying with SQL
                    \end{itemize}
                \item Use Cases: Banking, CRM applications, traditional business apps.
                \item \textit{Example}: Banking app with tables for `Customers`, `Accounts`, `Transactions`.
            \end{itemize}
            
        \item \textbf{Document Model}
            \begin{itemize}
                \item Data in documents (JSON, BSON, XML) with flexible schemas.
                \item Strengths:
                    \begin{itemize}
                        \item Flexible schema
                        \item Easy scaling and distribution
                    \end{itemize}
                \item Use Cases: Content management, catalogs, e-commerce.
                \item \textit{Example}: Product documents with `name`, `price`, `features`.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Data Models - More Models}
    \begin{enumerate}[resume]
        \item \textbf{Key-Value Store}
            \begin{itemize}
                \item Data stored as key-value pairs.
                \item Strengths:
                    \begin{itemize}
                        \item High speed, optimized for lookups
                        \item Great for caching/session management
                    \end{itemize}
                \item Use Cases: User sessions, e-commerce shopping carts.
                \item \textit{Example}: User preferences stored as `user_id123` with JSON string.

        \item \textbf{Column-Family Model}
            \begin{itemize}
                \item Data stored in columns for efficient querying.
                \item Strengths:
                    \begin{itemize}
                        \item Optimized for large datasets
                        \item Suitable for analytical queries
                    \end{itemize}
                \item Use Cases: Data warehousing, business intelligence.
                \item \textit{Example}: Analytics platform storing user data by `age`, `location`, and `activity`.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Performance vs. Scalability}
            \begin{itemize}
                \item Balance performance needs with scalability requirements.
            \end{itemize}
        \item \textbf{Data Integrity vs. Flexibility}
            \begin{itemize}
                \item Evaluate the importance of data integrity versus flexibility in data structures.
            \end{itemize}
        \item \textbf{Query Complexity}
            \begin{itemize}
                \item Consider the complexity of required queries.
            \end{itemize}
    \end{itemize}
    \begin{block}{Conclusion}
        The choice of data model should align with application requirements, including data nature, expected load, and ease of use.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Query Processing Basics - Overview}
    \begin{block}{What is Query Processing?}
        Query processing refers to the set of activities that take place when a query is submitted to a database management system (DBMS). The primary goal is to retrieve relevant data efficiently and effectively based on the userâ€™s request.
    \end{block}
    
    \begin{block}{Importance of Query Processing}
        \begin{itemize}
            \item \textbf{Efficiency}: Minimizes data retrieval time, crucial for real-time applications.
            \item \textbf{Accuracy}: Ensures correct and pertinent data retrieval, reducing errors.
            \item \textbf{Resource Management}: Optimizes system resources, lowering operational costs and enhancing performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Query Processing Basics - Key Steps}
    \begin{enumerate}
        \item \textbf{Parsing}: Checks syntax adherence to language rules (e.g., SQL).
        \item \textbf{Translation}: Converts parsed query into an internal representation or execution plan.
        \item \textbf{Optimization}: Analyzes the query to find the most efficient execution strategy.
        \item \textbf{Execution}: Executes the optimized plan to retrieve data.
        \item \textbf{Result Construction}: Compiles and returns results to the user.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Query Processing Basics - Illustration}
    \begin{block}{Illustration of the Query Processing Steps}
        \begin{center}
            User Query $\rightarrow$ Parsing $\rightarrow$ Translation $\rightarrow$ Optimization $\rightarrow$ Execution $\rightarrow$ Results
        \end{center}
    \end{block}
    
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Query processing is essential for performance and usability in database systems.
            \item Effective parsing, translation, and optimization are vital for efficient data retrieval.
            \item Understanding query processing is foundational for learning optimization techniques.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques}
    \begin{block}{Overview}
        Overview of optimization techniques for scalable query processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Query Optimization?}
    Query optimization involves transforming a given SQL query into a more efficient execution plan. The goal is to reduce the amount of resources (time and memory) required to retrieve results from a database.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Optimization}
    \begin{itemize}
        \item \textbf{Performance Improvement:} Reduces execution time for faster responses.
        \item \textbf{Resource Efficiency:} Minimizes CPU and memory usage.
        \item \textbf{Scalability:} Handles larger datasets without significant performance drops.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Optimization Techniques}
    \begin{enumerate}
        \item \textbf{Selectivity Estimation}
        \item \textbf{Index Use}
        \item \textbf{Join Optimization}
        \item \textbf{Query Rewrite}
        \item \textbf{Materialized Views}
        \item \textbf{Cost-Based Optimization}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Selectivity Estimation}
    \begin{itemize}
        \item \textbf{Description:} Estimate how many rows will match query predicates to optimize the execution plan.
        \item \textbf{Example:} If a query retrieves records with \texttt{WHERE salary > 50000}, the optimizer estimates the number of records likely to result to decide on the best access method (e.g., index scan vs. table scan).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Index Use}
    \begin{itemize}
        \item \textbf{Description:} Utilize indexes to improve speed; indexes are data structures that improve the speed of data retrieval operations.
        \item \textbf{Example:} A B-tree index on a column allows quick lookups instead of scanning the full table.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Join Optimization}
    \begin{itemize}
        \item \textbf{Description:} Determine the most efficient way to join multiple tables.
        \item \textbf{Techniques:} 
        \begin{itemize}
            \item \textit{Nested Loop Join:} Best for small datasets.
            \item \textit{Hash Join:} Efficient for large datasets with equality conditions.
            \item \textit{Merge Join:} Suitable for sorted datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Query Rewrite and Materialized Views}
    \begin{itemize}
        \item \textbf{Query Rewrite:} Transform the original query into a more efficient form without changing the result.
            \begin{itemize}
                \item \textbf{Example:} Instead of using \texttt{SELECT * FROM employees WHERE department\_id = 10}, use \texttt{SELECT name FROM employees WHERE department\_id = 10} if only the name is needed.
            \end{itemize}
        \item \textbf{Materialized Views:} Precompute and store complex queries to speed up access.
            \begin{itemize}
                \item \textbf{Example:} A view that summarizes sales by month can be queried directly.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cost-Based Optimization}
    \begin{itemize}
        \item \textbf{Description:} The query optimizer evaluates multiple execution strategies and selects the one with the lowest estimated cost based on available statistics.
        \item \textbf{Formula:} Cost estimates may include:
        \begin{equation}
            \text{Cost} = C_{\text{IO}} + C_{\text{CPU}} + C_{\text{Memory}}
        \end{equation}
        where \(C_{\text{IO}}\) is the input/output cost, \(C_{\text{CPU}}\) is the CPU processing cost, and \(C_{\text{Memory}}\) is the memory usage cost.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item The effectiveness of optimization techniques directly impacts system performance.
        \item Proper indexing and join types can significantly speed up query execution.
        \item Frequent use of selectivity estimations can lead to more optimized plans over time.
    \end{itemize}
    \begin{block}{Conclusion}
        Optimization is a critical component of efficient query processing in databases. By understanding and applying these techniques, we can ensure more scalable data retrieval processes that can handle larger volumes of data effectively.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Distributed Systems Overview - Introduction}
    \frametitle{Introduction to Distributed Systems}
    \begin{itemize}
        \item \textbf{Definition}: A distributed system is a model where components on networked computers communicate and coordinate actions by passing messages to achieve a common goal.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Distributed Systems Overview - Role in Data Processing}
    \frametitle{Role in Data Processing}
    \begin{itemize}
        \item \textbf{Data Storage}: Data is stored across multiple locations, enhancing durability and availability.
        \begin{itemize}
            \item \textit{Example}: Cloud storage like Google Drive replicates files across servers for data access.
        \end{itemize}
        
        \item \textbf{Scalability}: Distributing tasks across machines enables more efficient handling of increased loads.
        \begin{itemize}
            \item \textit{Example}: Web applications can scale by adding servers, distributing the query load.
        \end{itemize}
        
        \item \textbf{Fault Tolerance}: If one node fails, others can take over, ensuring continued operation.
        \begin{itemize}
            \item \textit{Example}: Distributed databases like Apache Cassandra replicate data, maintaining accessibility.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Distributed Systems Overview - Key Concepts}
    \frametitle{Key Concepts}
    \begin{itemize}
        \item \textbf{Node}: A single machine (physical or virtual) in a distributed system.
        \item \textbf{Communication}: Nodes exchange data using protocols (HTTP, TCP/IP), with synchronous or asynchronous interaction.
        \item \textbf{Consistency Models}:
        \begin{itemize}
            \item \textbf{Strong Consistency}: All nodes see the same data simultaneously (e.g., Google Spanner).
            \item \textbf{Eventual Consistency}: Guarantees that all accesses return the last updated value eventually (e.g., Amazon DynamoDB).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Distributed Systems Overview - Key Points and Conclusion}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Importance in modern data processing: foundational to cloud computing, big data, and large-scale web applications.
        \item Inherent complexity: Requires careful design to manage network latency, partitioning, and failure recovery.
        \item Real-world applications include cloud services, global data centers, IoT devices, and platforms for data sharing.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding distributed systems is critical for modern data processing and management. We will explore their implications and architectures in cloud database designs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cloud Database Architectures}
    \begin{block}{Design Principles for Cloud-Based Distributed Database Architectures}
        \begin{itemize}
            \item Scalability
            \item High Availability
            \item Data Consistency
            \item Partitioning
            \item Security
            \item Cost Efficiency
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Design Principle: Scalability and High Availability}
    \begin{itemize}
        \item \textbf{Scalability}:
        \begin{itemize}
            \item \textbf{Definition:} Ability to handle increased workload by adding resources.
            \item \textbf{Types:}
            \begin{itemize}
                \item Vertical Scaling: Increase resources of a single server.
                \item Horizontal Scaling: Add more servers to distribute the load.
            \end{itemize}
            \item \textbf{Example:} Amazon DynamoDB scales automatically based on workload demands.
        \end{itemize}

        \item \textbf{High Availability}:
        \begin{itemize}
            \item \textbf{Definition:} Ensures database is operational and accessible at all times.
            \item \textbf{Techniques:}
            \begin{itemize}
                \item Replication: Keep copies of data across servers.
                \item Sharding: Distribute data to avoid bottlenecks.
            \end{itemize}
            \item \textbf{Example:} Google Cloud Spanner uses replication to ensure availability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Other Important Principles}
    \begin{itemize}
        \item \textbf{Data Consistency}:
        \begin{itemize}
            \item \textbf{Definition:} Ensures all users see the same data.
            \item \textbf{Models:}
            \begin{itemize}
                \item Strong Consistency: All reads return the most recent write.
                \item Eventual Consistency: Updates propagate through the system.
            \end{itemize}
            \item \textbf{Example:} Amazon S3 offers eventual consistency.
        \end{itemize}

        \item \textbf{Partitioning}:
        \begin{itemize}
            \item \textbf{Definition:} Dividing a database into smaller pieces.
            \item \textbf{Benefits:}
            \begin{itemize}
                \item Reduces load on individual servers.
                \item Improves performance through parallel processing.
            \end{itemize}
            \item \textbf{Example:} Cassandra uses partitioning based on a partition key.
        \end{itemize}
        
        \item \textbf{Security}:
        \begin{itemize}
            \item Protects data from unauthorized access.
            \item Practices include encryption and role-based access control.
            \item \textbf{Example:} Azure SQL Database includes advanced threat protection.
        \end{itemize}

        \item \textbf{Cost Efficiency}:
        \begin{itemize}
            \item Managing database costs effectively.
            \item Strategies include pay-as-you-go models and appropriate storage solutions.
            \item \textbf{Example:} Google Cloud Firestore charges based on usage.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Overview}
    \frametitle{Key Steps in Developing Efficient Data Pipelines for Cloud Environments}
    \begin{enumerate}
        \item Define Objectives and Requirements
        \item Data Ingestion
        \item Data Processing
        \item Data Storage
        \item Data Orchestration
        \item Data Monitoring and Maintenance
        \item Security and Compliance
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Step 1: Define Objectives and Requirements}
    \begin{block}{Explanation}
        Start by clearly outlining the goals of your data pipeline. Understand what data needs to be collected, processed, and analyzed while considering performance, scalability, and security. 
    \end{block}
    
    \begin{block}{Example}
        For a retail business, the objective may be to analyze customer purchase patterns in real-time to improve stocking decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Step 2: Data Ingestion}
    \begin{block}{Explanation}
        This is the process of collecting and importing data from various sources into a centralized system.
    \end{block}
    
    \begin{block}{Examples of Sources}
        \begin{itemize}
            \item Databases (MySQL, MongoDB)
            \item Streaming platforms (Apache Kafka, AWS Kinesis)
            \item APIs (REST, GraphQL)
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Point}
        Choose between batch ingestion (slower, scheduled) and streaming ingestion (real-time, continuous).
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Steps 3 and 4: Data Processing & Storage}
    \begin{block}{Data Processing}
        \begin{itemize}
            \item Explanation: Once data is ingested, it needs to be processed. This step includes transformation (cleaning, formatting) and analysis (calculations, aggregations).
            \item Methodologies: 
                \begin{itemize}
                    \item ETL (Extract, Transform, Load)
                    \item ELT (Extract, Load, Transform)
                \end{itemize}
            \item Example: Use Apache Spark or AWS Glue for processing large datasets efficiently.
        \end{itemize}
    \end{block}
    
    \begin{block}{Data Storage}
        \begin{itemize}
            \item Explanation: Store processed data in a suitable data store. The choice of storage depends on access patterns, data types, and performance needs.
            \item Options:
                \begin{itemize}
                    \item Data Lakes (AWS S3, Azure Blob)
                    \item Data Warehouses (Snowflake, Google BigQuery)
                \end{itemize}
            \item Key Point: Ensure that the storage solution supports scalability as data volumes grow.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Step 5: Data Orchestration}
    \begin{block}{Explanation}
        Implement orchestrators to manage the workflow of data processing tasks. This ensures each part of the pipeline works in a synchronized manner.
    \end{block}
    
    \begin{block}{Tools}
        Apache Airflow, AWS Step Functions.
    \end{block}
    
    \begin{block}{Key Point}
        Good orchestration helps reduce operational overhead and improves reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Steps 6 and 7: Monitoring & Security}
    \begin{block}{Data Monitoring and Maintenance}
        \begin{itemize}
            \item Explanation: Set up monitoring to track performance, data quality, and pipeline health. 
            \item Tools: Prometheus and Grafana for monitoring metrics.
            \item Key Point: Continuous improvement through monitoring leads to increased efficiency.
        \end{itemize}
    \end{block}
    
    \begin{block}{Security and Compliance}
        \begin{itemize}
            \item Explanation: Ensure data pipelines comply with governance policies and security standards to protect sensitive information.
            \item Considerations:
                \begin{itemize}
                    \item Data encryption (at rest and in transit)
                    \item Access controls (IAM roles in cloud services)
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Conclusion}
    Developing efficient data pipelines in cloud environments involves a meticulous approach to planning, processes, and technology choices. By following these key steps, organizations can foster robust, scalable, and secure data architectures that cater to business needs.
\end{frame}

\begin{frame}[fragile]{Data Pipeline Development - Code Example}
    \frametitle{Code Snippet: Simple Data Ingestion using Python}
    \begin{lstlisting}[language=Python]
import requests
import pandas as pd

# Ingesting data from a hypothetical API
response = requests.get('https://api.example.com/data')
data = response.json()

# Transforming into a DataFrame
df = pd.DataFrame(data)

# Display the first five rows
print(df.head())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Industry Tools and Technologies - Introduction}
  \begin{block}{Introduction}
    In the rapidly evolving domain of data science and engineering, familiarity with industry-standard tools and technologies is essential. This slide introduces key tools that facilitate data modeling, querying, and efficient data processing.
  \end{block}

  \begin{itemize}
    \item Streamline workflows
    \item Enhance productivity
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Industry Tools and Technologies - Key Tools}
  \begin{enumerate}
    \item \textbf{AWS (Amazon Web Services)}
      \begin{itemize}
        \item \textbf{Description}: A comprehensive cloud platform offering computing power, storage options, and networking capabilities.
        \item \textbf{Key Services}:
          \begin{itemize}
            \item \textbf{Amazon S3}: Scalable object storage for data backup and archiving.
            \item \textbf{Amazon RDS}: Managed relational database service supporting various database engines like MySQL and PostgreSQL.
          \end{itemize}
        \item \textbf{Use Case}: Deploying scalable applications requiring high availability and flexibility.
      \end{itemize}
      
    \item \textbf{Kubernetes}
      \begin{itemize}
        \item \textbf{Description}: An open-source container orchestration platform that automates deployment, scaling, and management of containerized applications.
        \item \textbf{Key Features}:
          \begin{itemize}
            \item \textbf{Container Management}: Efficiently manage microservices with automatic scaling and healing.
            \item \textbf{Load Balancing}: Distributes traffic across containers to ensure high availability.
          \end{itemize}
        \item \textbf{Use Case}: Running complex applications, such as data processing pipelines, in a cloud-native environment.
      \end{itemize}
      
    \item \textbf{PostgreSQL}
      \begin{itemize}
        \item \textbf{Description}: An advanced open-source relational database known for its robust feature set and SQL compliance.
        \item \textbf{Key Features}:
          \begin{itemize}
            \item \textbf{ACID Compliance}: Ensures reliable transactions and data integrity.
            \item \textbf{Extensibility}: Support for custom data types and functions.
          \end{itemize}
        \item \textbf{Use Case}: Building enterprise-level applications that require strong data integrity.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Industry Tools and Technologies - Key Points and Examples}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Understanding these tools is critical for managing data workflows effectively.
      \item Proficiency with cloud services (e.g., AWS) and container orchestration (e.g., Kubernetes) enhances scalability and reliability.
      \item SQL-based databases (e.g., PostgreSQL) are essential for structured data and complex queries.
    \end{itemize}
  \end{block}

  \begin{block}{Example Tools in Action}
    \begin{itemize}
      \item \textbf{Data Pipeline on AWS}: Using AWS Glue to extract data from S3, transform it, and load it into a PostgreSQL instance.
      \item \textbf{Kubernetes in Practice}: Deploying a containerized application processing streaming data from Kafka, scaling based on traffic.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Gaining hands-on experience with these industry-standard tools is beneficial for implementing data models and optimizing query processing.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Management - Introduction}
    In todayâ€™s data-driven world, ethical considerations in data management are pivotal. Organizations must navigate challenges related to privacy and data integrity. Ethical data practices involve ensuring that data collection, storage, and usage respect individuals' rights and comply with legal and moral standards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Data Privacy}  
        The right of individuals to control how their personal information is collected and used. Ethical data management ensures transparent practices concerning consent and data use.
        \begin{itemize}
            \item \textit{Example:} Users signing up for a newsletter should be informed about how their email addresses will be used and have the option to opt-out.
        \end{itemize}

        \item \textbf{Data Integrity}  
        Refers to the accuracy and consistency of data over its lifecycle. Organizations must protect data from unauthorized access or manipulation.
        \begin{itemize}
            \item \textit{Example:} A healthcare provider must maintain the integrity of patient records to provide accurate diagnoses and treatments.
        \end{itemize}

        \item \textbf{Informed Consent}  
        Users should be fully aware of and agree to how their data will be used, empowering them and building trust.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Ethical Data Practices}
    \begin{itemize}
        \item \textbf{Building Trust:} Establishing trust with customers is essential for customer loyalty and brand reputation.
        \item \textbf{Compliance with Regulations:} Adhering to regulations like GDPR avoids legal penalties and promotes ethical practices.
        \item \textbf{Long-term Sustainability:} Organizations prioritizing ethical data management are better positioned for sustainable growth and avoid reputational damage from data breaches.
    \end{itemize}

    \begin{block}{Conclusion}
        Ethical considerations in data management are foundational to modern data practices. By understanding and implementing principles like data privacy, integrity, and informed consent, organizations can navigate complexities while respecting individuals' rights.
    \end{block}
\end{frame}


\end{document}