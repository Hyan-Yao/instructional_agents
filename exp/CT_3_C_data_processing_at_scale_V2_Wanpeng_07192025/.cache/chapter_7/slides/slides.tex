\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark}
    \begin{block}{Overview of Apache Spark}
        Apache Spark is a highly efficient, open-source framework designed for large-scale data processing. It offers a versatile platform for big data analytics and machine learning thanks to its speed and user-friendly APIs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Features of Apache Spark}
    \begin{enumerate}
        \item \textbf{Speed}:
            \begin{itemize}
                \item Processes data up to 100 times faster than Hadoop MapReduce in memory and 10 times faster on disk.
            \end{itemize}
            
        \item \textbf{Ease of Use}:
            \begin{itemize}
                \item High-level APIs in Java, Scala, Python, and R simplify development, making it accessible to data scientists and developers.
            \end{itemize}
    
        \item \textbf{Unified Engine}:
            \begin{itemize}
                \item Supports various processing tasks like SQL queries, streaming data, machine learning, and graph processing through a single platform.
            \end{itemize}
    
        \item \textbf{Resilient Distributed Datasets (RDD)}:
            \begin{itemize}
                \item Core data structure allowing efficient in-memory computations on large data volumes. Fault-tolerant and enables parallel processing.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Use Case}
    An e-commerce company analyzes customer purchase patterns in real time using Apache Spark:
    \begin{itemize}
        \item \textbf{Stream Data}: Ingest real-time transaction data from web logs and user interactions.
        \item \textbf{Process Data}: Utilize Spark SQL to query customer behavior and generate insights on sales trends.
        \item \textbf{Machine Learning}: Apply algorithms using MLlib for predicting future purchasing behaviors and personalizing marketing strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet}
    Here’s a simple example illustrating how to create an RDD and perform a transformation in Python:
    \begin{lstlisting}[language=Python]
from pyspark import SparkContext

# Initialize Spark Context
sc = SparkContext("local", "Example App")

# Create an RDD from a list
data = [1, 2, 3, 4, 5]
numbers_rdd = sc.parallelize(data)

# Perform a transformation to calculate the square of each number
squared_numbers = numbers_rdd.map(lambda x: x ** 2).collect()

print(squared_numbers)
# Output: [1, 4, 9, 16, 25]
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Versatility}: Applicable in a wide range of big data scenarios from batch processes to real-time analytics.
        \item \textbf{Community Support}: As an open-source project, Spark benefits from a vast community contributing to its continuous improvement.
        \item \textbf{Integration}: Seamlessly integrates with various data sources, enhancing functionality in diverse environments.
    \end{itemize}
    This slide serves as an introduction to the powerful capabilities of Apache Spark, demonstrating its significance in modern data processing workflows.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Overview}
    \begin{itemize}
        \item Understand the core concepts of Apache Spark
        \item Gain hands-on experience with Spark's components
        \item Develop real-world applications
        \item Enhance problem-solving and analytical skills
        \item Prepare for future learning and career opportunities
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Core Concepts}
    \begin{block}{Understand the Core Concepts of Apache Spark}
        \begin{itemize}
            \item **Big Data Processing Introduction**
                \begin{itemize}
                    \item Learn what Apache Spark is and its role in Big Data ecosystems.
                    \item Understand the benefits of Spark: speed, ease of use, in-memory computation.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Hands-on Experience}
    \begin{block}{Gain Hands-on Experience with Spark's Components}
        \begin{itemize}
            \item **Spark Core**
                \begin{itemize}
                    \item Develop foundational knowledge in RDDs (Resilient Distributed Datasets).
                    \item Example: Use RDDs to read and process large text files.
                \end{itemize}
            \item **Spark SQL**
                \begin{itemize}
                    \item Learn to query structured data using SQL.
                    \item Example: Load a CSV file and perform filtering and aggregation.
                \end{itemize}
            \item **Spark Streaming**
                \begin{itemize}
                    \item Explore real-time data processing capabilities.
                    \item Example: Analyze Twitter feeds in real-time.
                \end{itemize}
            \item **MLlib**
                \begin{itemize}
                    \item Discover Spark's machine learning library.
                    \item Example: Build a model for predicting movie ratings.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Real-world Application}
    \begin{block}{Real-world Application Development}
        \begin{itemize}
            \item **Project Work**
                \begin{itemize}
                    \item Collaborate in teams to develop a complete application using Spark.
                    \item Focus on a relevant use case like data analysis or predictive modeling.
                    \item Example: Create a data pipeline that ingests, processes, and visualizes data from an API.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Skills Enhancement}
    \begin{block}{Enhance Problem-solving and Analytical Skills}
        \begin{itemize}
            \item **Troubleshooting Spark Applications**
                \begin{itemize}
                    \item Learn best practices for debugging and optimizing Spark jobs.
                    \item Discuss common pitfalls, like shuffling and ways to mitigate performance issues.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Objectives - Career Preparation}
    \begin{block}{Prepare for Future Learning and Career Opportunities}
        \begin{itemize}
            \item **Portfolio Development**
                \begin{itemize}
                    \item Create a portfolio piece demonstrating hands-on experience with Spark.
                \end{itemize}
            \item **Networking and Collaboration**
                \begin{itemize}
                    \item Build connections with peers and instructors for future projects.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item **Hands-on Learning**: Prioritizes practical application of principles.
        \item **Collaboration**: Team projects mimic real-world scenarios.
        \item **Preparation for Industry**: Skills acquired are directly applicable to career paths in data science and big data analytics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Prerequisites for the Intensive Workshop on Apache Spark}
    \begin{block}{Overview}
        To ensure participants are well-prepared for the workshop, it is essential to possess certain skills and knowledge. This slide outlines the prerequisites needed for readiness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Necessary Programming Skills}
    \begin{enumerate}
        \item \textbf{Proficiency in a Programming Language}
            \begin{itemize}
                \item \textbf{Recommended Languages:}
                \begin{itemize}
                    \item \textbf{Python}: Simple and readable for data manipulation.
                    \item \textbf{Scala}: Native language for Apache Spark.
                \end{itemize}
                \item \textbf{Example:} Basic understanding of data structures like lists and dictionaries in Python or Arrays and Maps in Scala.
            \end{itemize}
        
        \item \textbf{Basic Understanding of Functional Programming}
            \begin{itemize}
                \item \textbf{Concepts to Know:} First-class functions, higher-order functions, immutability.
                \item \textbf{Example:} Using the \texttt{map()} function for transformations.
            \end{itemize}
    \end{enumerate}
    \begin{block}{Key Point}
        Familiarity with functional programming concepts enhances Spark's capabilities.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Familiarity with Data Processing Concepts}
    \begin{enumerate}
        \item \textbf{Data Formats}
            \begin{itemize}
                \item \textbf{Common Formats:} CSV, JSON, Parquet, Avro.
                \item \textbf{Example:} Reading from and writing to these formats in Spark.
            \end{itemize}
        
        \item \textbf{Data Operations}
            \begin{itemize}
                \item \textbf{Key Concepts:} ETL processes, DataFrames, RDDs.
                \item \textbf{Example:} Using DataFrames for filtering and aggregations.
            \end{itemize}
    \end{enumerate}
    \begin{block}{Key Point}
        Understanding data transformation concepts facilitates effective application development in Spark.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Software Tools Required}
    \begin{enumerate}
        \item \textbf{Apache Spark}
            \begin{itemize}
                \item Ensure Spark is installed (local or cloud).
                \item Knowledge of Spark 3.x or latest is recommended.
            \end{itemize}

        \item \textbf{Development Environment}
            \begin{itemize}
                \item \textbf{Recommended Platforms:}
                \begin{itemize}
                    \item Jupyter Notebooks for Python.
                    \item IDE for Scala like IntelliJ IDEA or Eclipse.
                \end{itemize}
                \item \textbf{Example:} Setting up Jupyter for PySpark.
            \end{itemize}

        \item \textbf{Data Storage Solutions}
            \begin{itemize}
                \item Familiarity with Hadoop, BigQuery, or Amazon S3 is advantageous.
            \end{itemize}
    \end{enumerate}
    \begin{block}{Key Point}
        Comfortable utilization of these software tools enhances efficiency during coding sessions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item A solid foundation in programming, data processing concepts, and software tools is critical for workshop success.
        \item If any prerequisites are unfamiliar, consider reviewing relevant materials before the workshop.
    \end{itemize}
    \begin{block}{Next Steps}
        Look forward to the Workshop Structure slide, detailing the schedule and key sessions for application development.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Structure - Overview}
    The Intensive Workshop on Apache Spark is designed to provide hands-on experience in building robust data processing applications. The following structure outlines the schedule and key sessions, focusing on essential concepts and practical activities throughout the week.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Structure - Schedule Breakdown: Day 1 & 2}
    \begin{itemize}
        \item \textbf{Day 1: Introduction to Apache Spark}
        \begin{itemize}
            \item \textbf{Session 1: What is Apache Spark?}
            \begin{itemize}
                \item Overview of Spark's architecture and components (Driver, Executors, Cluster Manager)
                \item \textit{Key Point:} Understanding how Spark differs from traditional data processing frameworks.
            \end{itemize}
            \item \textbf{Session 2: Setting Up the Environment}
            \begin{itemize}
                \item Install Spark and required libraries.
                \item \textit{Example:} Setting up a local Spark cluster using a Docker container.
            \end{itemize}
        \end{itemize}

        \item \textbf{Day 2: Resilient Distributed Datasets (RDDs)}
        \begin{itemize}
            \item \textbf{Session 3: Introduction to RDDs}
            \begin{itemize}
                \item Discuss RDDs as the fundamental data structure in Spark and their characteristics (immutable, distributed).
                \item \textit{Illustration:} Accessing data through RDD transformations (map, filter).
            \end{itemize}
            \item \textbf{Session 4: RDD Operations}
            \begin{itemize}
                \item Perform transformations and actions on RDDs.
                \item \textit{Example:} Writing a simple Spark application to analyze a dataset using RDD operations.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Structure - Schedule Breakdown: Days 3 to 5}
    \begin{itemize}
        \item \textbf{Day 3: DataFrames and Spark SQL}
        \begin{itemize}
            \item \textbf{Session 5: DataFrames Overview}
            \begin{itemize}
                \item Introduction to DataFrames and their advantages over RDDs (schema, optimization).
                \item \textit{Key Point:} Leveraging Spark SQL for data processing.
            \end{itemize}
            \item \textbf{Session 6: Querying DataFrames}
            \begin{itemize}
                \item Querying and manipulating DataFrames using Spark SQL.
                \item \textit{Example Code:} Loading CSV data into a DataFrame and performing SQL queries.
            \end{itemize}
        \end{itemize}

        \item \textbf{Day 4: Machine Learning with Spark MLlib}
        \begin{itemize}
            \item \textbf{Session 7: Introduction to MLlib}
            \begin{itemize}
                \item Overview of Spark's machine learning library and its features.
                \item \textit{Key Point:} Importance of scalability in machine learning tasks.
            \end{itemize}
            \item \textbf{Session 8: Building a Simple ML Model}
            \begin{itemize}
                \item Hands-on activity to implement a classification model using MLlib.
                \item \textit{Example Code:} Fitting a logistic regression model on a dataset.
            \end{itemize}
        \end{itemize}

        \item \textbf{Day 5: Advanced Topics and Best Practices}
        \begin{itemize}
            \item \textbf{Session 9: Streaming Data with Spark}
            \begin{itemize}
                \item Introduction to Spark Streaming and its applications.
                \item \textit{Key Point:} Real-time data processing using Spark.
            \end{itemize}
            \item \textbf{Session 10: Best Practices and Optimization Techniques}
            \begin{itemize}
                \item Discuss performance tuning, resource management, and avoiding common pitfalls.
                \item \textit{Example Tip:} Efficient partitioning of data for optimal performance.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Workshop Structure - Conclusion}
    Participants will gain hands-on experience in coding, troubleshooting, and optimizing Spark applications. Emphasis will be placed on real-world applications and problem-solving best practices to ensure every attendee gains a comprehensive understanding of Spark's capabilities.

    By following this structured approach, we aim to delve into both theoretical foundations and practical applications of Apache Spark, empowering participants to apply their learning in real-world data scenarios effectively. This ensures an enriching learning experience throughout the workshop.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Activities}
    In this intensive workshop on \textbf{Apache Spark}, participants will engage in a series of hands-on activities designed to build foundational knowledge and practical skills in building basic Spark applications. 
    Each activity will guide you through key concepts while allowing you to implement what you have learned.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity 1: Setting Up Your Spark Environment}
    \textbf{Objective:} Configure a Spark application environment to run local Spark jobs.

    \begin{enumerate}
        \item Install Apache Spark on your local machine or cluster.
        \item Set up the necessary dependencies, including Java and Scala (if applicable).
        \item Verify the installation by running a sample Spark shell command.
    \end{enumerate}

    \begin{block}{Example:}
        \begin{lstlisting}
$ spark-shell
        \end{lstlisting}
    This command opens an interactive shell to execute Spark commands.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity 2: Basic RDD Operations}
    \textbf{Objective:} Understand and manipulate Resilient Distributed Datasets (RDDs).

    \begin{enumerate}
        \item Create an RDD from an existing dataset (e.g., text file).
        \item Perform transformations such as \texttt{map}, \texttt{filter}, and \texttt{reduce}.
    \end{enumerate}

    \begin{block}{Example Code Snippet:}
        \begin{lstlisting}
val data = sc.textFile("path/to/textfile.txt")
val words = data.flatMap(line => line.split(" "))
val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _)
wordCounts.collect().foreach(println)
        \end{lstlisting}
    This snippet counts the occurrences of each word in the specified text file.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity 3: DataFrame Creation and Manipulation}
    \textbf{Objective:} Learn to work with DataFrames for structured data processing.

    \begin{enumerate}
        \item Create a DataFrame from a JSON or CSV file.
        \item Execute SQL queries using Spark SQL.
    \end{enumerate}

    \begin{block}{Example Code Snippet:}
        \begin{lstlisting}
val df = spark.read.json("path/to/data.json")
df.createOrReplaceTempView("people")
val results = spark.sql("SELECT name, age FROM people WHERE age > 21")
results.show()
        \end{lstlisting}
    Here, we filter the DataFrame to show only individuals older than 21.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activity 4: Writing Spark Applications}
    \textbf{Objective:} Build a simple Spark application in Scala, Python, or Java.

    \begin{enumerate}
        \item Define a Spark application structure using your preferred language.
        \item Write logic to read data, process it, and save the output.
    \end{enumerate}

    \begin{block}{Example Architecture:}
        \begin{itemize}
            \item \textbf{Main function:} to set up the Spark session
            \item \textbf{Data Processing:} implement transformations using DataFrames or RDDs
            \item \textbf{Output:} save the result to a desired file format (e.g., Parquet, CSV)
        \end{itemize}
    \end{block}

    \begin{block}{Example Code Snippet: (Scala Application)}
        \begin{lstlisting}
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val data = spark.read.json("path/to/input.json")
    data.write.parquet("path/to/output.parquet")
    spark.stop()
  }
}
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{RDDs vs. DataFrames:} Understand the differences between these two core abstractions in Spark.
        \item \textbf{Execution Model:} Spark's lazy evaluation and optimization through the directed acyclic graph (DAG).
        \item \textbf{Performance:} The usage of in-memory processing to improve the performance for big data analytics.
    \end{itemize}
    \textbf{By the end of these activities, you'll have a foundational understanding of Spark’s architecture and its capabilities to process large datasets efficiently. Let’s dive into the hands-on experience!}
\end{frame}

\begin{frame}{Game-Changing Features of Spark}
  \begin{itemize}
    \item In-memory computing for rapid processing
    \item Efficiency in handling large datasets
    \item Support for multiple programming languages
  \end{itemize}
\end{frame}

\begin{frame}{1. In-Memory Computing}
  \begin{block}{Definition}
    In-memory computing processes data in RAM instead of disk retrieval.
  \end{block}
  
  \begin{block}{Benefit}
    Significantly speeds up data processing times, enabling near real-time analytics.
  \end{block}
  
  \begin{block}{Example}
    Spark can outperform Hadoop by up to 100 times for certain workloads due to its ability to store intermediate data in memory.
  \end{block}
\end{frame}

\begin{frame}{2. Efficiency with Large Datasets}
  \begin{block}{Definition}
    Designed to handle massive datasets across distributed systems.
  \end{block}
  
  \begin{block}{Key Features}
    \begin{itemize}
      \item Optimizes task scheduling and resource allocation.
      \item Efficiently handles petabytes of data.
    \end{itemize}
  \end{block}
  
  \begin{block}{Example}
    A retail company can analyze transaction data from millions of customers quickly, generating insights without delays.
  \end{block}
\end{frame}

\begin{frame}{3. Unified Engine for Diverse Workloads}
  \begin{block}{Programming Language Support}
    Spark supports:
    \begin{itemize}
      \item **Python** (via PySpark)
      \item **Scala** (native language)
      \item **Java**
      \item **R** (via SparkR)
    \end{itemize}
  \end{block}
  
  \begin{block}{Key Point}
    Multi-language support makes Spark accessible to diverse developers.
  \end{block}

  \begin{block}{Example}
    Data scientists may use R or Python for analytics, while engineers build scalable applications in Scala or Java.
  \end{block}
\end{frame}

\begin{frame}[fragile]{Code Snippet Example}
  \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("Example").getOrCreate()

# Load data into a DataFrame
data = spark.read.csv("data/customer_data.csv", header=True, inferSchema=True)

# Perform a transformation
filtered_data = data.filter(data.age > 30)

# Show results
filtered_data.show()
  \end{lstlisting}
\end{frame}

\begin{frame}{Summary Highlights}
  \begin{itemize}
    \item \textbf{Speed}: In-memory computing accelerates analytics processes.
    \item \textbf{Scalability}: Efficient handling of vast datasets.
    \item \textbf{Versatility}: Supports multiple programming languages.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Practical Applications of Apache Spark - Overview}
    \begin{block}{Overview of Apache Spark in Industry}
        Apache Spark is a powerful open-source framework designed for large-scale data processing and analytics. 
        It has transformed data analytics and machine learning across various industries by providing fast, flexible, and scalable solutions.
    \end{block}
\end{frame}

\begin{frame}{Practical Applications of Spark}
    \begin{block}{Key Applications of Spark in Different Industries}
        \begin{enumerate}
            \item \textbf{E-Commerce: Customer Behavior Analytics}
            \item \textbf{Healthcare: Patient Data Analytics}
            \item \textbf{Finance: Fraud Detection}
            \item \textbf{Telecommunications: Network Optimization}
            \item \textbf{Media: Audience Engagement and Content Recommendation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]{E-Commerce and Healthcare Applications}
    \begin{itemize}
        \item \textbf{E-Commerce: Customer Behavior Analytics}
            \begin{itemize}
                \item \textit{Example}: A leading online retailer analyzed user data using Spark.
                \item \textit{Impact}: Enhanced recommendation systems and increased conversion rates.
            \end{itemize}
        
        \item \textbf{Healthcare: Patient Data Analytics}
            \begin{itemize}
                \item \textit{Example}: Hospitals processed patient records and wearable data with Spark.
                \item \textit{Impact}: Improved patient outcomes through predictive analytics.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Finance, Telecommunications, and Media Applications}
    \begin{itemize}
        \item \textbf{Finance: Fraud Detection}
            \begin{itemize}
                \item \textit{Example}: Banks used Spark for real-time anomaly detection.
                \item \textit{Impact}: Significant reduction in fraud losses.
            \end{itemize}

        \item \textbf{Telecommunications: Network Optimization}
            \begin{itemize}
                \item \textit{Example}: Analysis of millions of interactions to optimize networks.
                \item \textit{Impact}: Enhanced service delivery and fewer customer complaints.
            \end{itemize}
            
        \item \textbf{Media: Audience Engagement and Content Recommendation}
            \begin{itemize}
                \item \textit{Example}: Processing viewer data to recommend content.
                \item \textit{Impact}: Personalized experiences leading to user retention.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Code Snippet Example}
    \begin{block}{PySpark Example}
        Here’s a simple example in Python using PySpark to create a DataFrame from a CSV file and calculate the average value:
    \end{block}
    
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("AverageExample").getOrCreate()

# Load the CSV file into a DataFrame
data = spark.read.csv("path/to/data.csv", header=True, inferSchema=True)

# Calculate the average of a numeric column
average_value = data.select("numeric_column").groupBy().avg().first()[0]

print(f"The average value is: {average_value}")
    \end{lstlisting}
\end{frame}

\begin{frame}{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Apache Spark's diverse applications illustrate its significance in data analytics and machine learning.
        Industries leverage its capabilities to enhance operational efficiency, innovate, and improve decision-making.
    \end{block}
    
    \begin{block}{Next Steps}
        In the upcoming slide, we will introduce a group project exploring ethical data handling and reporting in the context of Apache Spark applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Project Overview - Introduction}
    In this intensive workshop, students will collaborate on a group project that emphasizes hands-on experience with Apache Spark while addressing critical themes of ethical data handling and reporting. 
    \begin{itemize}
        \item Apply theoretical knowledge in practical scenarios
        \item Build skills in big data processing and analytics
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Group Project Overview - Key Objectives}
    \begin{itemize}
        \item \textbf{Familiarization with Apache Spark}:
        \begin{itemize}
            \item Utilize Spark’s capabilities for big data processing
        \end{itemize}
        \item \textbf{Ethical Data Handling}:
        \begin{itemize}
            \item Consider ethical implications in data collection and reporting
        \end{itemize}
        \item \textbf{Collaboration}:
        \begin{itemize}
            \item Enhance teamwork skills with diverse perspectives
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Themes}
    Participants will choose from the following themes that interlace data analytics with ethical considerations:
    \begin{enumerate}
        \item \textbf{Data Privacy Compliance}
            \begin{itemize}
                \item \textit{Objective}: Analyze a dataset while ensuring compliance with data privacy laws (e.g., GDPR, CCPA).
                \item \textit{Example}: Use anonymization techniques to protect user identities in a customer dataset.
            \end{itemize}
        
        \item \textbf{Bias in Data Analytics}
            \begin{itemize}
                \item \textit{Objective}: Examine bias in data collection or model building and propose solutions.
                \item \textit{Example}: Evaluate a model for biases based on demographic variables.
            \end{itemize}
        
        \item \textbf{Transparency in Reporting}
            \begin{itemize}
                \item \textit{Objective}: Develop a reporting framework emphasizing transparency in analytics.
                \item \textit{Example}: Create visualizations that clearly present findings and assumptions.
            \end{itemize}

        \item \textbf{Data Stewardship}
            \begin{itemize}
                \item \textit{Objective}: Focus on responsible data management.
                \item \textit{Example}: Develop a policy document outlining data storage and access.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Format and Timeline}
    \begin{itemize}
        \item \textbf{Team Formation}: Groups of 4-5 participants.
        \item \textbf{Deliverables}:
            \begin{itemize}
                \item A report (5-7 pages) detailing your approach, findings, and ethical considerations.
                \item A presentation sharing key insights and ethical practices.
            \end{itemize}
        \item \textbf{Timeline}: The project will span several weeks with milestones for progress.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Collaborative Learning}: Engage with peers, sharing ideas and feedback.
        \item \textbf{Critical Thinking}: Evaluate ethical considerations at every step to mitigate risks.
        \item \textbf{Real-world Relevance}: The skills and insights gained are applicable across various industries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This group project encourages you to deepen your understanding of Apache Spark and prepares you to be conscientious data practitioners. 
    \begin{itemize}
        \item Embrace the opportunity to learn collaboratively.
        \item Make a meaningful impact through your work!
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Introduction}
    \begin{block}{What is Data Ethics?}
        Data ethics refers to principles that govern the collection, usage, and dissemination of data, especially personal data. It aims to ensure that:
    \end{block}
    \begin{itemize}
        \item User privacy is respected
        \item Integrity is upheld
        \item Fairness is promoted
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Key Concepts}
    \begin{itemize}
        \item \textbf{Data Privacy:} Right to control personal information and be informed about its processing.
        \item \textbf{Data Security:} Protection against unauthorized access and data breaches.
        \item \textbf{Transparency:} Openness about data collection practices and purposes.
        \item \textbf{Fairness:} Avoiding bias in data processing for equitable outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Legal Framework}
    \begin{block}{Privacy Laws}
        Understanding privacy laws is crucial:
    \end{block}
    \begin{itemize}
        \item \textbf{GDPR:} 
        \begin{itemize}
            \item Applies to EU
            \item Emphasizes user consent, data access rights, right to be forgotten
        \end{itemize}
        \item \textbf{CCPA:} 
        \begin{itemize}
            \item Grants Californians rights over personal information
            \item Includes knowing what data is collected and ability to opt-out of sales
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Ethical Considerations}
    \begin{itemize}
        \item \textbf{Informed Consent:} Always obtain prior consent from data subjects.
        \item \textbf{Minimization Principle:} Collect only necessary data, avoiding excessive collection.
        \item \textbf{Data Anonymization:} Anonymize data to protect identities when possible.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Examples of Violations}
    \begin{itemize}
        \item \textbf{Misuse of Data:} Using personal data without consent for marketing.
        \item \textbf{Bias in Algorithms:} Unrepresentative data leading to unfair outcomes.
    \end{itemize}
    \begin{block}{Example Case}
        A recruitment algorithm trained on biased historical data might disadvantage certain demographic groups.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Best Practices}
    \begin{itemize}
        \item \textbf{Transparency:} Clearly state purposes of data collection.
        \item \textbf{Documentation:} Keep detailed records of data sources and consent forms.
        \item \textbf{Regular Audits:} Conduct reviews of data handling to ensure ethical compliance.
    \end{itemize}
    \begin{block}{Key Takeaway}
        Ethical considerations enhance credibility and trust in projects.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethics in Data Processing - Conclusion}
    Emphasizing ethics in data processing fosters respect and responsibility in today's data-driven society. Allow ethical considerations to guide your data practices in your workshop projects.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools and Resources Overview}
    This workshop on Apache Spark utilizes various essential software tools and computing resources 
    to facilitate learning and project execution. Below, we discuss these tools and their functions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Software Tools}
    \begin{itemize}
        \item \textbf{Apache Spark}
        \begin{itemize}
            \item \textbf{Description}: An open-source distributed computing system designed for fast data processing.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item Fast processing using in-memory caching
                \item Support for multiple languages (Python, Java, Scala)
                \item Advanced analytics capabilities (streaming, machine learning, graph processing)
            \end{itemize}
        \end{itemize}
        
        \item \textbf{JIRA}
        \begin{itemize}
            \item \textbf{Description}: A project management tool for tracking issues and project progress.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item Customizable workflows for Agile practices
                \item Integration with development tools (Git, Bitbucket)
                \item Real-time collaboration features
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Trello}
        \begin{itemize}
            \item \textbf{Description}: A visual tool for organizing tasks using boards and cards.
            \item \textbf{Key Features}:
            \begin{itemize}
                \item Intuitive drag-and-drop interface
                \item Ability to share boards among team members
                \item Supports checklists, due dates, and attachments for tasks
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Computing Resources}
    \begin{itemize}
        \item \textbf{Computational Hardware}
        \begin{itemize}
            \item \textbf{Description}: High-performance computing resources are necessary for data processing.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{Local Machines}: Ensure participants have laptops/desktops with at least 16 GB RAM.
                \item \textbf{Cloud Platforms}: AWS, Azure, or Google Cloud for scalable resources.
            \end{itemize}
        \end{itemize}

        \item \textbf{Data Storage Solutions}
        \begin{itemize}
            \item \textbf{Description}: Effective data storage is crucial for large data volumes.
            \item \textbf{Examples}:
            \begin{itemize}
                \item \textbf{HDFS}: Best for large-scale storage in a distributed environment.
                \item \textbf{Amazon S3}: A scalable cloud storage option compatible with Spark.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Code Snippet}
    \begin{itemize}
        \item Familiarize yourself with each software tool's interface before the workshop.
        \item Utilize cloud resources for tasks requiring significant computational power.
        \item Maintain ethical standards in data handling.
    \end{itemize}
    
    \begin{block}{Code Snippet}
    Here's an example of initializing a Spark session in Python:
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize a Spark session
spark = SparkSession.builder \
    .appName("ExampleApp") \
    .getOrCreate()

# Run a basic operation
data = spark.range(100).collect()
print(data)
    \end{lstlisting}
    \end{block}    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedback and Reflection - Importance of Student Feedback}
    \begin{enumerate}
        \item \textbf{Continuous Improvement}
        \begin{itemize}
            \item Student feedback helps identify strengths and weaknesses in teaching strategies and workshop organization.
            \item Enables tangible improvements in curriculum design.
        \end{itemize}
        
        \item \textbf{Enhancing Engagement}
        \begin{itemize}
            \item Encourages a sense of ownership over learning.
            \item Increased motivation and investment in education.
        \end{itemize}
        
        \item \textbf{Tailoring Instructional Methods}
        \begin{itemize}
            \item Helps identify preferred instructional methods (hands-on projects, lectures, etc.).
            \item Customizes instructional approach based on feedback.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedback and Reflection - Reflection as a Tool for Learning}
    \begin{enumerate}
        \item \textbf{Personal Growth}
        \begin{itemize}
            \item Reflection encourages critical thinking about learning and performance.
            \item Allows assessment of effective strategies and areas for improvement.
        \end{itemize}
        
        \item \textbf{Building a Community of Practice}
        \begin{itemize}
            \item Sharing reflections fosters collaboration and deeper understanding.
            \item Suggestion: Introduce a "feedback circle" for group sharing.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Feedback and Reflection - Key Points and Conclusion}
    \begin{itemize}
        \item \textbf{Regular Feedback Collection}
        \begin{itemize}
            \item Use surveys or informal discussions throughout the workshop.
        \end{itemize}

        \item \textbf{Encourage Honest Responses}
        \begin{itemize}
            \item Assure students that all feedback (positive or negative) is welcome.
        \end{itemize}

        \item \textbf{Action on Feedback}
        \begin{itemize}
            \item Share how previous feedback has been implemented and invite future suggestions.
        \end{itemize}
    \end{itemize}
    
    \vspace{0.5cm}
    \textbf{Conclusion:}  
    Fostering a culture of feedback and reflection enhances the learning experience and promotes critical thinking among students, benefiting both instructors and students alike.
\end{frame}


\end{document}