\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Week 8: Introduction to Neural Networks]{Week 8: Introduction to Neural Networks}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{block}{Overview}
        Neural networks are computational models inspired by the human brain's architecture and functioning.
        They are integral to many data mining and artificial intelligence (AI) applications due to their ability to recognize patterns, learn from data, and generalize to new inputs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Key Concepts}
    \begin{itemize}
        \item \textbf{Neuron}: The basic unit of a neural network, analogous to a biological neuron. Each neuron receives input, processes it, and passes the output to the next layer.
        
        \item \textbf{Layers}: 
        \begin{itemize}
            \item \textbf{Input Layer}: Receives the initial data.
            \item \textbf{Hidden Layers}: Intermediate layers that transform inputs into something the model can use. These layers can have multiple neurons.
            \item \textbf{Output Layer}: Produces the final predictions or classifications.
        \end{itemize}
        
        \item \textbf{Weights and Bias}: Each connection between neurons has an associated weight that signifies its importance, while each neuron has a bias that shifts the activation function. These parameters are adjusted during the training process to minimize error.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Significance and Applications}
    \begin{block}{Significance in Data Mining and AI}
        \begin{enumerate}
            \item \textbf{Pattern Recognition}: Neural networks excel at identifying patterns in complex data sets, crucial in image processing, natural language processing, and financial forecasting.
            \item \textbf{Non-linearity}: Unlike traditional algorithms, neural networks can capture non-linear relationships in data, allowing for more flexible modeling.
            \item \textbf{Scalability}: They can handle vast amounts of data and adapt efficiently as more data becomes available.
        \end{enumerate}
    \end{block}

    \begin{block}{Real-World Applications}
        \begin{itemize}
            \item \textbf{ChatGPT}: Utilizes deep learning to understand and generate human-like text, showcasing neural networks in natural language processing.
            \item \textbf{Image Recognition}: Widely employed in facial recognition and medical imaging analysis, demonstrating their capacity to analyze and classify visual data accurately.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Summary and Next Steps}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Neural networks learn from data through training, enhancing performance for specific tasks.
            \item Flexibility, scalability, and power make neural networks a cornerstone technology in modern AI applications.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        Neural networks represent a transformative approach to processing and analyzing data. They form the basis for many innovative applications in AI and data mining.
    \end{block}

    \begin{block}{Next Steps}
        Explore the motivations behind using neural networks and further delve into specific applications and their impacts in the following slides.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Neural Networks?}
    \begin{block}{Introduction}
        Neural networks have emerged as a cornerstone of modern AI and data mining. But why do we specifically employ neural networks? Let’s explore their fundamental motivations and real-world examples.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Motivations Behind Using Neural Networks}
    \begin{enumerate}
        \item \textbf{High Dimensionality Handling}
            \begin{itemize}
                \item Neural networks can process large datasets with many features, learning complex patterns.
                \item Example: Image recognition involves high-dimensional data (thousands to millions of pixels).
            \end{itemize}

        \item \textbf{Non-linear Relationships}
            \begin{itemize}
                \item Neural networks model non-linear relationships using activation functions.
                \item Example: House prices often depend on non-linear factors like size and location.
            \end{itemize}

        \item \textbf{Feature Learning}
            \begin{itemize}
                \item They autonomously extract features from raw data, minimizing preprocessing needs.
                \item Example: ChatGPT learns representations directly from text, improving context and semantics.
            \end{itemize}

        \item \textbf{Scalability and Adaptability}
            \begin{itemize}
                \item Neural networks can scale and be fine-tuned for specific tasks.
                \item Example: ChatGPT can be fine-tuned on specific conversational data to enhance dialogue capacity.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    \begin{enumerate}
        \item \textbf{ChatGPT}
            \begin{itemize}
                \item Uses a transformer neural network architecture to generate human-like responses.
                \item Benefit: Provides interactive, context-aware replies, enhancing user experience across various domains.
            \end{itemize}

        \item \textbf{Image Recognition}
            \begin{itemize}
                \item Convolutional Neural Networks (CNNs) excel in analyzing visual data for tasks like facial recognition and object detection.
                \item Benefit: Applied in security systems, healthcare imaging, and social media tagging.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Neural networks effectively manage complexity and high-dimensional data. 
        \item They capture non-linear relationships, making them adaptable beyond traditional models.
        \item Learning features autonomously from data reduces manual extraction efforts, enhancing efficiency.
        \item Real-world applications like ChatGPT and image recognition highlight the transformative impact across industries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Neural networks present profound capabilities essential for addressing complex AI problems today. Understanding their significance prepares us for deeper explorations of their components and architectures in subsequent slides.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Components of Neural Networks}
    Neural networks are inspired by the structure and function of the human brain. Understanding their key components is crucial for deeper insights into their functionalities.
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Neurons}
    \begin{itemize}
        \item \textbf{Definition}: The fundamental building blocks of a neural network, similar to biological neurons.
        \item \textbf{Functionality}: A neuron receives inputs, processes them, and produces an output.
        \item \textbf{Example}: 
        In a multilayer perceptron, a neuron takes multiple inputs with corresponding weights, adds a bias, and applies an activation function.
    \end{itemize}
    \begin{block}{Formula}
        \begin{equation}
        output = f\left(\sum (inputs \times weights) + bias\right)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Activation Functions}
    \begin{itemize}
        \item \textbf{Purpose}: Introduces non-linearity, allowing the model to learn complex patterns.
        \item \textbf{Common Types}:
            \begin{itemize}
                \item \textbf{Sigmoid}: Outputs values between 0 and 1. 
                  \[
                  f(x) = \frac{1}{1 + e^{-x}}
                  \]
                \item \textbf{ReLU (Rectified Linear Unit)}: Outputs \(x\) if \(x > 0\); else, 0. 
                  \[
                  f(x) = \max(0, x)
                  \]
                \item \textbf{Softmax}: Outputs a probability distribution for multi-class classification.
            \end{itemize}
    \end{itemize}
    \begin{block}{Key Point}
        The choice of activation function can significantly affect the performance and learning capability of the model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Layers and Architecture}
    \begin{itemize}
        \item \textbf{Layers}:
            \begin{itemize}
                \item \textbf{Input Layer}: Receives input features (e.g., pixels in an image).
                \item \textbf{Hidden Layers}: Intermediate layers where computation occurs. 
                \item \textbf{Output Layer}: Provides the final prediction (e.g., class label).
            \end{itemize}
        \item \textbf{Architecture}:
            \begin{itemize}
                \item \textbf{Feedforward Neural Networks}: Neurons connect without cycles.
                \item \textbf{Convolutional Neural Networks (CNN)}: Designed for structured grid data like images.
                \item \textbf{Recurrent Neural Networks (RNN)}: Suitable for sequential data.
            \end{itemize}
    \end{itemize}
    \begin{block}{Illustration}
        \begin{verbatim}
        Input Layer
           ↓
        Hidden Layer 1
           ↓
        Hidden Layer 2
           ↓
        Output Layer
        \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    \begin{itemize}
        \item \textbf{Neurons}: Process inputs and produce outputs.
        \item \textbf{Activation Functions}: Introduce non-linearity for complex pattern learning.
        \item \textbf{Layers}: Define pathways of data flow.
        \item \textbf{Architecture}: Determines component organization affecting performance.
    \end{itemize}
    \begin{block}{Next Steps}
        In our next session, we will explore various \textbf{Types of Neural Networks} and their specific use cases.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Overview}
    \begin{block}{Overview}
        Neural networks are at the core of many modern AI applications. They are specialized for different tasks and data structures. 
        We will explore three primary types:
        \begin{itemize}
            \item Feedforward Neural Networks (FNNs)
            \item Convolutional Neural Networks (CNNs)
            \item Recurrent Neural Networks (RNNs)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Feedforward Neural Networks}
    \begin{block}{1. Feedforward Neural Networks (FNNs)}
        \begin{itemize}
            \item \textbf{Description}: Information moves in one direction—from input to output.
            \item \textbf{Key Characteristics}:
                \begin{itemize}
                    \item No cycles or loops; outputs do not feedback.
                    \item Static input-output mappings.
                \end{itemize}
            \item \textbf{Use Cases}:
                \begin{itemize}
                    \item Regression tasks (e.g., predicting house prices).
                    \item Classification tasks (e.g., multi-class predictions).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Convolutional Neural Networks}
    \begin{block}{2. Convolutional Neural Networks (CNNs)}
        \begin{itemize}
            \item \textbf{Description}: Specialized for grid-like data, especially images.
            \item \textbf{Key Characteristics}:
                \begin{itemize}
                    \item Convolutional layers with filters to detect features.
                    \item Pooling layers that reduce dimensionality.
                \end{itemize}
            \item \textbf{Use Cases}:
                \begin{itemize}
                    \item Image classification (e.g., identifying objects).
                    \item Video analysis (e.g., tracking movement).
                    \item Medical imaging (e.g., detecting tumors).
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Recurrent Neural Networks}
    \begin{block}{3. Recurrent Neural Networks (RNNs)}
        \begin{itemize}
            \item \textbf{Description}: Designed for sequence prediction and time-series tasks.
            \item \textbf{Key Characteristics}:
                \begin{itemize}
                    \item Cycles in connections for persistent information.
                    \item Maintains 'memory' through hidden states.
                \end{itemize}
            \item \textbf{Use Cases}:
                \begin{itemize}
                    \item Natural Language Processing (NLP).
                    \item Time-series forecasting (e.g., stock prices).
                    \item Speech recognition systems.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        Understanding different types of neural networks helps in selecting the right architecture for AI applications, enhancing model performance.
    \end{block}
    \begin{block}{Next Steps}
        Delve into the learning processes of these networks, focusing on training, loss functions, and optimization techniques. 
        Consider recent advancements such as transformers, which combine properties of RNNs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The Learning Process}
    \begin{block}{Motivation}
        Understanding the learning process in neural networks is crucial for developing advanced AI applications, such as those seen in technologies like ChatGPT. These applications leverage rich datasets for effective communication and responses.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Training Neural Networks}
    Training a neural network involves teaching it to make predictions based on input data. This systematic process encompasses:
    \begin{itemize}
        \item Forward propagation
        \item Loss functions
        \item Optimization algorithms
    \end{itemize}
    Understanding these concepts is foundational for creating effective neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation}
    Forward propagation is the first step where input data is processed through the network to produce output predictions.
    
    \begin{block}{How it Works}
        \begin{itemize}
            \item Input features are multiplied by weights and passed through activation functions.
            \item The output from one layer serves as input for the next until the final output layer is reached.
        \end{itemize}
    \end{block}
    
    \begin{block}{Mathematical Representation}
        \begin{equation}
            y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $y$ = output
            \item $f$ = activation function (e.g., sigmoid, ReLU)
            \item $w_i$ = weights
            \item $x_i$ = input features
            \item $b$ = bias term
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Functions}
    The loss function measures the discrepancy between predicted and actual outcomes, guiding improvements in the neural network’s performance.
    
    \begin{block}{Common Loss Functions}
        \begin{itemize}
            \item \textbf{Mean Squared Error} (for regression):
            \begin{equation}
                Loss = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
            \end{equation}
            \item \textbf{Cross-Entropy Loss} (for classification):
            \begin{equation}
                Loss = -\sum_{i=1}^{C} y_i \log(\hat{y_i})
            \end{equation}
        \end{itemize}
        Where:
        \begin{itemize}
            \item $y_i$ = ground truth labels
            \item $\hat{y_i}$ = predicted probabilities
            \item $C$ = number of classes
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Algorithms}
    Optimization algorithms are used to minimize the loss function by adjusting the weights of the neural network.

    \begin{block}{Common Algorithms}
        \begin{itemize}
            \item \textbf{Stochastic Gradient Descent (SGD)}: Updates weights based on the gradient of the loss function.
            \item \textbf{Adam (Adaptive Moment Estimation)}: Combines AdaGrad and RMSProp benefits and adapts learning rates for individual parameters.
        \end{itemize}
    \end{block}

    \begin{block}{Weight Update Formula}
        \begin{equation}
            w := w - \eta \nabla J(w)
        \end{equation}
        Where:
        \begin{itemize}
            \item $w$ = weights
            \item $\eta$ = learning rate
            \item $\nabla J(w)$ = gradient of the loss function
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item \textbf{Forward Propagation:} Generates predictions from input data.
        \item \textbf{Loss Function:} Quantifies errors in predictions, guiding improvements.
        \item \textbf{Optimization Algorithms:} Adjust weights to minimize loss, ensuring effective learning.
    \end{itemize}
    
    Understanding these foundational concepts prepares students for advanced topics like backpropagation and weight adjustments, crucial for refining neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation and Weight Adjustment - Introduction}
    \begin{block}{Introduction to Backpropagation}
        Backpropagation is a key algorithm used in training neural networks. It enables the model to learn from errors and optimize weight adjustments to minimize the loss function. The process has two main steps:
    \end{block}
    \begin{enumerate}
        \item \textbf{Forward Propagation}: 
        \begin{itemize}
            \item Input data is fed into the neural network to compute outputs using current weights and activation functions.
            \item The error (loss) is calculated with a loss function, quantifying the difference between predicted outputs and actual target outputs.
        \end{itemize}
        \item \textbf{Backward Propagation}: 
        \begin{itemize}
            \item Computes the gradient of the loss function with respect to each weight using the chain rule of calculus.
            \item Efficiently calculates gradients for all weights in one pass through the network.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation and Weight Adjustment - Key Concepts}
    \begin{block}{Key Concepts in Backpropagation}
        \begin{itemize}
            \item \textbf{Gradient Descent}: 
            \begin{itemize}
                \item An optimization algorithm for adjusting weights in the direction that reduces the loss.
                \item Formula: 
                \[
                \theta = \theta - \alpha \nabla L(\theta)
                \]
                where \( \theta \) are the weights, \( \alpha \) is the learning rate, and \( \nabla L(\theta) \) is the gradient of the loss function.
            \end{itemize}
            \item \textbf{Chain Rule}:
            \begin{itemize}
                \item Used to compute derivatives of the loss function across each layer of the network.
                \begin{equation}
                \frac{dz}{dx} = \frac{dz}{dg} \cdot \frac{dg}{dx}
                \end{equation}
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation and Weight Adjustment - Weight Adjustment Process}
    \begin{block}{Weight Adjustment Process}
        \begin{enumerate}
            \item \textbf{Calculating Gradients}: 
            \begin{itemize}
                \item For each output node, compute the gradient of the loss concerning the node’s output, propagating errors back through the network.
            \end{itemize}
            \item \textbf{Updating Weights}:
            \begin{itemize}
                \item Adjust each weight based on the gradient and learning rate.
                \begin{equation}
                w \leftarrow w - \alpha \cdot \frac{\partial L}{\partial w}
                \end{equation}
                where \( \frac{\partial L}{\partial w} \) is the computed gradient for that weight.
            \end{itemize}
        \end{enumerate}
    \end{block}
    \begin{block}{Conclusion}
        Understanding backpropagation is crucial for implementing effective neural networks, allowing them to capture complex patterns in data efficiently.
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    \begin{itemize}
        \item Neural networks have transformed various fields such as image recognition and natural language processing.
        \item In this section, we will learn to implement simple neural network models using Python libraries: TensorFlow and PyTorch.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{1. Why Implement Neural Networks?}
    \begin{itemize}
        \item \textbf{Real-World Applications:} Utilized in technologies such as self-driving cars and virtual assistants like ChatGPT.
        \item \textbf{Accessibility:} Libraries like TensorFlow and PyTorch simplify implementation, making deep learning accessible even to beginners.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{2. Key Concepts}
    \begin{itemize}
        \item \textbf{Neural Network Structure:} Composed of input, hidden, and output layers.
        \begin{itemize}
            \item \textit{Weights and Biases:} Parameters learned during training.
            \item \textit{Activation Functions:} Introduce non-linearity (e.g., ReLU, Sigmoid).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Implementing a Simple Neural Network - TensorFlow/Keras}
    \begin{block}{Example: A Multi-Layer Perceptron (MLP) for Classifying Digits (MNIST Dataset)}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Load the dataset
mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Scale data

# Build the model
model = keras.Sequential([
    layers.Flatten(input_shape=(28, 28)),  # Input layer
    layers.Dense(128, activation='relu'),   # Hidden layer
    layers.Dense(10, activation='softmax')   # Output layer
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=5)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_accuracy}')
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points of the TensorFlow Code}
    \begin{itemize}
        \item The dataset consists of handwritten digits from the MNIST database.
        \item The model architecture includes:
        \begin{itemize}
            \item \textbf{Flatten Layer:} Converts 2D images to 1D vectors.
            \item \textbf{Dense Layer:} Hidden layer with ReLU activation.
            \item \textbf{Output Layer:} Uses softmax activation to predict probabilities.
        \end{itemize}
        \item The model is compiled using the Adam optimizer and evaluated for accuracy post-training.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Implementing a Simple Neural Network - PyTorch}
    \begin{block}{Using PyTorch}
    \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# Load the dataset
transform = transforms.Compose([transforms.ToTensor()])

train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)

# Define the model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer
        self.fc2 = nn.Linear(128, 10)  # Output layer

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # Flatten
        x = torch.relu(self.fc1(x))  # Hidden layer with ReLU
        x = self.fc2(x)  # Output layer
        return x

model = MLP()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Train the model
for epoch in range(5):
    for images, labels in train_loader:
        optimizer.zero_grad()
        output = model(images)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

print("Training complete.")
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points of the PyTorch Code}
    \begin{itemize}
        \item A Multi-Layer Perceptron is defined using a custom class in PyTorch.
        \item The model uses cross-entropy loss and the Adam optimizer for training.
        \item The data is loaded with transformations to prepare for training.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary and Key Takeaways}
    \begin{itemize}
        \item \textbf{Libraries:} TensorFlow and PyTorch provide significant tools for building and training neural networks.
        \item \textbf{Process:} Implementing neural networks involves:
        \begin{itemize}
            \item Loading data
            \item Defining model architecture
            \item Compiling, training, and evaluating the model
        \end{itemize}
        \item Experimentation is crucial: Use this guide to explore with your datasets and different architectures.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation and Performance Metrics - Overview}
    \begin{block}{Understanding Neural Network Performance Evaluation}
        Evaluating neural networks is crucial to ensure they meet intended goals. This process helps ascertain the model's predictive capabilities based on unseen data.
    \end{block}

    \begin{block}{Key Metrics for Evaluation}
        Key metrics include Accuracy, Precision, Recall, and F1-Score. Each metric provides unique insights into model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation and Performance Metrics - Key Metrics}
    \begin{enumerate}
        \item **Accuracy**
        \begin{itemize}
            \item Measures the percentage of correct predictions.
            \item Formula: 
            \[
            \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}
            \]
            \item Key Point: May not be reliable with imbalanced datasets.
        \end{itemize}
        
        \item **Precision**
        \begin{itemize}
            \item Indicates the quality of positive predictions.
            \item Formula:
            \[
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}}
            \]
            \item Key Point: Critical where false positives are costly.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation and Performance Metrics - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue from the previous frame
        \item **Recall**
        \begin{itemize}
            \item Measures the ratio of true positives to actual positives.
            \item Formula:
            \[
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}
            \]
            \item Key Point: Important for capturing all positive instances.
        \end{itemize}
        
        \item **F1-Score**
        \begin{itemize}
            \item Harmonic mean of precision and recall.
            \item Formula:
            \[
            \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision + Recall}}
            \]
            \item Key Point: Balances precision and recall, useful for imbalanced datasets.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks}
    
    \begin{itemize}
        \item Overview of common challenges in neural networks
        \item Key points to be covered:
        \begin{enumerate}
            \item Overfitting
            \item Underfitting
            \item Vanishing Gradient Problem
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Overfitting}
    
    \begin{block}{Definition}
        Overfitting occurs when a neural network learns the training data too well, capturing noise rather than underlying patterns. 
    \end{block}
    
    \begin{itemize}
        \item High model complexity leads to poor generalization.
        \item Example: A high-degree polynomial regression fitting all training points.
    \end{itemize}
    
    \begin{block}{Key Points to Mitigate Overfitting}
        \begin{itemize}
            \item Regularization Techniques (L1 or L2)
            \item Dropout
            \item Early Stopping
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Underfitting}
    
    \begin{block}{Definition}
        Underfitting occurs when a model is too simple to capture the underlying trend of the data. 
    \end{block}
    
    \begin{itemize}
        \item Low model complexity results in poor performance.
        \item Example: A linear model fitting a quadratic dataset.
    \end{itemize}
    
    \begin{block}{Key Points to Address Underfitting}
        \begin{itemize}
            \item Increase Model Complexity
            \item Feature Engineering
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Vanishing Gradient Problem}
    
    \begin{block}{Definition}
        The vanishing gradient problem arises during backpropagation where gradients become very small, affecting learning.
    \end{block}
    
    \begin{itemize}
        \item Gradients can shrink exponentially in deep networks.
    \end{itemize}
    
    \begin{block}{Key Points to Overcome}
        \begin{itemize}
            \item Initialization Techniques (e.g., He, Xavier)
            \item Activation Functions (e.g., ReLU)
            \item Batch Normalization
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    
    \begin{itemize}
        \item Understanding challenges helps in designing effective neural network architectures.
        \item Addressing:
        \begin{itemize}
            \item Overfitting ensures a good fit to the data.
            \item Underfitting enhances model capability.
            \item Vanishing gradient problem enables deeper learning.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Transition}
        Next, we will explore emerging trends in neural networks, including advancements in deep learning and generative models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Introduction}
    \begin{block}{Overview}
        The field of neural networks is constantly evolving. This presentation covers:
        \begin{itemize}
            \item Advancements in deep learning
            \item The concept of transfer learning
            \item The rising prominence of generative models
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Advancements in Deep Learning}
    \begin{block}{Key Advancements}
        Deep learning continues to push the boundaries with:
        \begin{itemize}
            \item \textbf{Improved Architectures}
                \begin{itemize}
                    \item New models like Transformers and EfficientNets enhance performance in various tasks.
                    \item Example: Transformer models excel in NLP, as seen with ChatGPT.
                \end{itemize}
            \item \textbf{Scalability}
                \begin{itemize}
                    \item Innovations in distributed computing and hardware (GPUs, TPUs) enable training on massive datasets.
                \end{itemize}
            \item \textbf{Explainability}
                \begin{itemize}
                    \item Focusing on making models interpretable to enhance understanding of their decisions.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Key Point}
        Advancements drive efficiency and capabilities in neural networks for complex tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Transfer Learning}
    \begin{block}{Concept of Transfer Learning}
        Transfer learning allows models to adapt from one task to another, reducing data and resource requirements.
        \begin{itemize}
            \item \textbf{Pre-trained Models}
                \begin{itemize}
                    \item Models like BERT and GPT can be fine-tuned efficiently.
                    \item Example: Adapting a language model for sentiment analysis with a smaller dataset.
                \end{itemize}
            \item \textbf{Applications}
                \begin{itemize}
                    \item Particularly useful in areas with limited labeled data such as medical imaging.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Key Point}
        Transfer learning allows for rapid deployment of models across tasks despite limited data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Generative Models}
    \begin{block}{Generative Models Overview}
        Generative models create new data samples resembling the training dataset.
        \begin{itemize}
            \item \textbf{Key Types}
                \begin{itemize}
                    \item \textbf{GANs}: Compete between two networks (generator and discriminator) for realistic output.
                    \item \textbf{VAEs}: Learn enhanced latent representations, aiding in data generation.
                \end{itemize}
            \item \textbf{Applications}
                \begin{itemize}
                    \item Art and Music: Original creations.
                    \item Synthetic Data Generation: For training models in data-scarce environments.
                \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Key Point}
        Generative models are reshaping creative industries and advancing data augmentation techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Conclusion}
    \begin{block}{Conclusion}
        Neural networks' evolution is influenced by key trends:
        \begin{itemize}
            \item Advancements in deep learning, 
            \item Utilization of transfer learning,
            \item Rise of generative models.
        \end{itemize}
        Understanding these trends allows us to harness their potential in future applications.
    \end{block}
    \begin{block}{Additional Notes}
        \begin{itemize}
            \item Ethical implications are crucial as we develop more autonomous systems.
            \item Staying informed on advancements benefits both students and professionals.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Introduction}
    \begin{block}{Introduction to Ethical Implications}
        As neural networks are increasingly adopted in data mining, it is imperative to consider the ethical implications surrounding their use. This involves evaluating the potential for bias and establishing accountability for the outcomes generated by these algorithms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias in Neural Networks}
    \begin{block}{Key Ethical Considerations}
        \begin{enumerate}
            \item \textbf{Bias in Neural Networks}
            \begin{itemize}
                \item \textbf{Definition}: Bias occurs when the models produce prejudiced outcomes due to unrepresentative training data.
                \item \textbf{Examples}:
                \begin{itemize}
                    \item Facial recognition systems failing on underrepresented demographic groups.
                    \item Job recruitment algorithms favoring candidates based on biased historical data.
                \end{itemize}
                \item \textbf{Impact}: Biased outcomes exacerbate social inequalities and lead to unfair treatment of individuals or groups.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Illustration}
        Consider a neural network trained to predict loan approvals. If historical data reflects a bias against certain racial groups, the model may unjustly deny loans to applicants from those groups.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Accountability}
    \begin{block}{Accountability}
        \begin{enumerate}
            \item \textbf{Definition}: Accountability refers to who is responsible for the decisions made by AI systems that can negatively impact lives.
            \item \textbf{Challenges}:
            \begin{itemize}
                \item Difficulty tracing the decision-making process of deep learning models, known as the "black box" problem.
                \item Lack of clear guidelines on who to hold accountable: developers, companies, or the AI itself?
            \end{itemize}
            \item \textbf{Example}: In a misdiagnosis case by a health diagnostic AI, determining liability between software developers and medical practitioners can be complex.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Importance of diverse data to mitigate bias.
            \item Need for transparency and explainability in neural networks.
            \item Establishing policies on accountability and ethical standards in AI development.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Conclusion}
    Understanding the ethical implications of neural networks is crucial as these technologies become more integrated into critical sectors like healthcare, finance, and law enforcement. An ethical approach promotes fairness and justice, fostering trust in AI systems.

    By focusing on these themes, we can better navigate the challenges posed by neural networks in data mining and ensure that these powerful tools serve society positively.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Overview}
    Neural networks have revolutionized the field of data mining. Below is a summary of the key points covered in this chapter, emphasizing their significance in the broader context of data mining.
    
    \begin{itemize}
        \item Data mining is essential for extracting valuable insights.
        \item Core concepts of neural networks improve data analysis.
        \item Applications of neural networks demonstrate their versatility.
        \item Ethical considerations are crucial for responsible usage.
    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Key Points}
    
    \begin{enumerate}
        \item \textbf{Why We Need Data Mining}
            \begin{itemize}
                \item Extract valuable insights from large unstructured data sets.
                \item Enables informed decision-making (e.g., customer behavior predictions, fraud detection, personalized recommendations).
            \end{itemize}
        
        \item \textbf{Core Concepts of Neural Networks}
            \begin{itemize}
                \item Architecture: Layers (input, hidden, output) process input data.
                \item Activation Functions: Introduce non-linearity for complex pattern learning (e.g., ReLU, sigmoid).
                \item Learning Process: Backpropagation and gradient descent optimize accuracy.
            \end{itemize}
    \end{enumerate}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Applications and Ethics}
    
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        
        \item \textbf{Applications of Neural Networks}
            \begin{itemize}
                \item Deep Learning: Utilizes multi-layered neural networks for tasks like image and speech recognition.
                \item Recent AI Applications: Tools like ChatGPT enhance human-computer interaction through neural networks.
            \end{itemize}

        \item \textbf{Ethical Considerations}
            \begin{itemize}
                \item Address bias, accountability, and transparency.
                \item Responsible usage ensures data mining positively impacts society.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Final Summary}
        Neural networks form the foundation for advanced data mining techniques, driving insights in a data-driven world. Understanding ethical implications is critical for responsible practices.
    \end{block}

\end{frame}


\end{document}