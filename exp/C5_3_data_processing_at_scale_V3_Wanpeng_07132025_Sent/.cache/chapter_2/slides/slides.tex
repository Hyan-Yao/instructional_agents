\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 2: Tools and Libraries for Data Processing]{Week 2: Tools and Libraries for Data Processing}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Processing at Scale}
    \begin{block}{Overview of Data Processing}
        Data processing at scale refers to the handling and analyzing of vast amounts of data efficiently. Traditional methods are insufficient due to the exponential growth of data. Tools and libraries for large-scale data processing allow organizations to derive insights and drive innovation.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Processing Tools}
    \begin{enumerate}
        \item \textbf{Efficiency}
            \begin{itemize}
                \item Example: Batch processing in Hadoop can analyze terabytes of data overnight, which would take too long manually.
            \end{itemize}
        
        \item \textbf{Scalability}
            \begin{itemize}
                \item Example: Spark distributes tasks across clusters, allowing it to handle increasing data volumes.
            \end{itemize}
        
        \item \textbf{Data Variety}
            \begin{itemize}
                \item Example: Hadoop processes diverse formats (JSON, XML, text files).
            \end{itemize}
        
        \item \textbf{Fault Tolerance}
            \begin{itemize}
                \item Example: Hadoop replicates data across nodes to prevent data loss during failures.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Frameworks: Apache Hadoop}
    \begin{itemize}
        \item  \textbf{Overview:} Open-source framework for big data storage and processing.
        \item \textbf{Key Components:}
            \begin{itemize}
                \item \textbf{HDFS}: Hadoop Distributed File System for storing large files.
                \item \textbf{MapReduce}: A model for processing large datasets using parallel algorithms.
            \end{itemize}
        \item \textbf{Use Case:} Analyzing satellite imagery for environmental changes over time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Frameworks: Apache Spark}
    \begin{itemize}
        \item \textbf{Overview:} Fast, in-memory data processing engine with a user-friendly API.
        \item \textbf{Key Features:}
            \begin{itemize}
                \item Supports languages like Python, Java, Scala.
                \item Libraries for SQL, streaming data, machine learning, and graph processing.
            \end{itemize}
        \item \textbf{Use Case:} Real-time data processing from social media to analyze trends and sentiments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Points}
    \begin{itemize}
        \item The ability to process large datasets is crucial in today's data-driven world.
        \item \textbf{Hadoop and Spark} are key tools that offer efficiency, scalability, and versatility.
        \item Understanding when to utilize these frameworks aids in designing effective data architectures.
    \end{itemize}
    \begin{block}{Key Takeaway}
        Incorporating effective data processing tools transforms raw data into actionable insights, fostering innovation and strategic decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Data Processing Concepts}
    Effective data processing at scale involves several fundamental concepts essential for handling large datasets efficiently:
    \begin{itemize}
        \item Data Ingestion
        \item Data Transformation
        \item Data Storage
        \item Performance Optimization
    \end{itemize}
    Understanding these is crucial for leveraging tools like Apache Hadoop and Spark in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Data Ingestion}
    \begin{block}{Definition}
        Data ingestion is the process of acquiring data from various sources and bringing it into a processing environment.
    \end{block}
    \begin{itemize}
        \item \textbf{Examples:}
        \begin{itemize}
            \item Streaming data from IoT devices using Apache Kafka.
            \item Batch data loaded from databases, CSV files, or APIs.
        \end{itemize}
        \item \textbf{Key Point:} Ingestion can be either:
        \begin{itemize}
            \item \textbf{Batch-based} (data processed all at once)
            \item \textbf{Real-time} (data processed as it comes in)
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Data Transformation}
    \begin{block}{Definition}
        Data transformation refers to the conversion of raw data into a more usable format through various processes.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Processes:}
        \begin{itemize}
            \item Data Cleaning: Removing duplicates or correcting errors.
            \item Data Enrichment: Integrating additional data sources for context (e.g., geo-tagging).
            \item Data Structuring: Organizing data into formats suitable for analysis (e.g., converting JSON to tabular format).
        \end{itemize}
        \item \textbf{Illustration:}
        \begin{itemize}
            \item Input: CSV with customer transactions.
            \item Transformation: Standardizing dates, grouping transactions by customer, summing spending.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Data Storage}
    \begin{block}{Definition}
        Data storage involves selecting appropriate systems to retain data after ingestion and transformation.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Considerations:}
        \begin{itemize}
            \item Scalability: Handling increasing volumes of data.
            \item Access Speed: Ensuring fast data retrieval.
            \item Cost Efficiency: Balancing access speed and storage cost.
        \end{itemize}
        \item \textbf{Examples:}
        \begin{itemize}
            \item HDFS (Hadoop Distributed File System) for batch processing storage.
            \item NoSQL databases like MongoDB for unstructured data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. Performance Optimization}
    \begin{block}{Definition}
        Performance optimization refers to techniques used to enhance the speed and efficiency of data processing.
    \end{block}
    \begin{itemize}
        \item \textbf{Techniques:}
        \begin{itemize}
            \item Partitioning: Distributing workloads to parallelize processing.
            \item Caching: Storing intermediate results to avoid redundant computations.
            \item Load Balancing: Distributing tasks evenly across resources.
        \end{itemize}
        \item \textbf{Code Snippet (Spark Example):}
        \begin{lstlisting}[language=python]
# Caching a DataFrame in Spark
df = spark.read.csv("input_data.csv")
df.cache()  # Caching to speed up future actions
        \end{lstlisting}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways}
    \begin{itemize}
        \item Understanding data ingestion, transformation, storage, and optimization is essential for efficient data processing.
        \item The choice of tools and methods affects scalability and performance.
        \item Familiarity with these concepts will enhance your ability to deploy data processing frameworks effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Hadoop}
    \begin{block}{Overview of Apache Hadoop}
        Apache Hadoop is an open-source framework that allows for distributed storage and processing of large datasets across clusters of computers using simple programming models. It is designed to scale from a single server to thousands of machines, with a built-in fault tolerance mechanism.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Hadoop Architecture}
    \begin{enumerate}
        \item \textbf{Hadoop Common}: The common utilities and libraries that support the other Hadoop modules.
        \item \textbf{Hadoop Distributed File System (HDFS)}: A distributed file system that stores data across multiple machines, handling large files by breaking them into smaller blocks and replicating them for resilience.
        \item \textbf{Yet Another Resource Negotiator (YARN)}: The cluster resource management system that enhances scalability and resource utilization by allocating system resources to various applications.
        \item \textbf{MapReduce}: The programming model for processing large datasets with a distributed algorithm, splitting tasks into smaller sub-tasks for parallel processing.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{How Hadoop Enables Scalable Storage and Processing}
    \begin{itemize}
        \item \textbf{Scalability}: Hadoop clusters can easily expand by adding more nodes to manage increasing data loads without downtime.
        \item \textbf{Fault Tolerance}: HDFS replicates data across multiple nodes, ensuring data can be accessed from another node if one fails.
        \item \textbf{Cost-Effectiveness}: Runs on commodity hardware, making it a low-cost solution for big data storage and processing.
    \end{itemize}
    
    \begin{block}{Illustration of Hadoop Architecture}
    \begin{verbatim}
        +--------------------+
        |       Client       |
        +--------------------+
                  |
                  |
      +-----------+-----------+
      |      Resource Manager  | 
      |       (YARN)          |
      +-----------+-----------+
                  |
  +---------------+---------------+
  |               |               |
+---+          +---+          +---+ 
|NM |          |NM |          |NM |
|   |          |   |          |   |
+---+          +---+          +---+
(Workers for MapReduce Jobs)
  |               |               |
  +---------------+---------------+
                  |
           +-------------+
           | HDFS Block  |
           +-------------+
    \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Apache Hadoop is fundamental for dealing with big data challenges through distributed processing and storage.
        \item Understanding the architecture (HDFS, YARN, MapReduce) is vital for leveraging Hadoop's full potential.
        \item Hadoop supports various data formats and integrates with multiple solutions, showcasing its versatility.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Apache Hadoop offers a potent solution for efficiently storing and processing large volumes of data. By utilizing its architecture, organizations can effectively manage big data challenges, leading to better insights and decision-making.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Using Apache Hadoop}
    \begin{block}{Introduction to Hadoop}
        Apache Hadoop is an open-source framework that enables the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale from a single server to thousands of machines, each offering local computation and storage.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Components of Hadoop}
    \begin{itemize}
        \item \textbf{Hadoop Distributed File System (HDFS)}
        \begin{itemize}
            \item Purpose: Storage layer for large files across multiple machines.
            \item How It Works:
            \begin{itemize}
                \item Data is split into blocks (default 128 MB).
                \item Blocks are replicated (default factor is 3) across nodes for fault tolerance.
            \end{itemize}
            \item Example: A 512 MB file is stored as four blocks of 128 MB each, replicated on different nodes.
        \end{itemize}
        
        \item \textbf{MapReduce}
        \begin{itemize}
            \item Purpose: Processing framework for distributed data.
            \item How It Works:
            \begin{itemize}
                \item Map Phase: Divides job into sub-jobs (mappers) that process data in parallel.
                \item Reduce Phase: Mappers output key-value pairs; reducers aggregate results.
            \end{itemize}
            \item Example: In a word count program, mappers output each word with a count of 1, and reducers sum the counts for each word.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Walkthrough: Data Ingestion and Storage}
    \begin{itemize}
        \item \textbf{Ingesting Data:}
        \begin{itemize}
            \item Use commands to put data into HDFS.
            \item Example Command:
            \begin{lstlisting}[language=bash]
hdfs dfs -put localfile.txt /hadoop/user/data/
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Storing and Retrieving Data:}
        \begin{itemize}
            \item To list files in a directory:
            \begin{lstlisting}[language=bash]
hdfs dfs -ls /hadoop/user/data/
            \end{lstlisting}
            \item To read a file from HDFS:
            \begin{lstlisting}[language=bash]
hdfs dfs -cat /hadoop/user/data/localfile.txt
            \end{lstlisting}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Example: Basic Word Count with MapReduce}
    \begin{lstlisting}[language=java]
public class WordCount {
   public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();

      public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
         StringTokenizer itr = new StringTokenizer(value.toString());
         while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);
         }
      }
   }

   public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
      private IntWritable result = new IntWritable();
      public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
         int sum = 0;
         for (IntWritable val : values) {
            sum += val.get();
         }
         result.set(sum);
         context.write(key, result);
      }
   }
}
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Scalability:} 
        \begin{itemize}
            \item Hadoop scales easily from a single-node setup to thousands of nodes, accommodating increasing data and processing needs.
        \end{itemize}
        \item \textbf{Fault Tolerance:}
        \begin{itemize}
            \item HDFS replicates data blocks, ensuring data remains accessible even if some nodes fail.
        \end{itemize}
        \item \textbf{Flexibility in Processing:}
        \begin{itemize}
            \item MapReduce allows various data processing methods, making Hadoop versatile for different applications.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Overview}
    \begin{block}{What is Apache Spark?}
        Apache Spark is an open-source distributed computing system focused on speed, ease of use, and sophisticated analytics.
        \begin{itemize}
            \item Originated at UC Berkeley's AMP Lab
            \item Popular for big data processing
            \item Handles large datasets across various workloads
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Key Advantages}
    \begin{block}{Advantages over Hadoop}
        \begin{enumerate}
            \item \textbf{Speed:}
                \begin{itemize}
                    \item Processes data in-memory, reducing retrieval time
                    \item Up to 100 times faster for certain applications
                \end{itemize}
            \item \textbf{Ease of Use:}
                \begin{itemize}
                    \item Supports multiple programming languages (Scala, Python, Java, R)
                    \item User-friendly DataFrame API
                \end{itemize}
            \item \textbf{Advanced Analytics:}
                \begin{itemize}
                    \item Integrated libraries for streaming, machine learning, graph processing
                    \item Simplifies machine learning model implementation with MLlib
                \end{itemize}
            \item \textbf{Unified Engine:}
                \begin{itemize}
                    \item Supports batch processing, interactive queries, and stream processing in one platform
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Architecture and Components}
    \begin{block}{Architecture of Apache Spark}
        \begin{itemize}
            \item \textbf{Driver Program:} Main function converting input data into distributed data
            \item \textbf{Cluster Manager:} Manages resources like Mesos or YARN
            \item \textbf{Workers:} Nodes executing data processing tasks
            \item \textbf{Executor:} Process on worker nodes executing tasks
            \item \textbf{Tasks:} Smallest unit of work scheduled on executors
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Components}
        \begin{itemize}
            \item \textbf{Resilient Distributed Dataset (RDD):} Immutable collections facilitating fault tolerance
            \item \textbf{DataFrames:} Higher-level abstraction compared to RDDs
            \item \textbf{Spark SQL:} Enables SQL queries and Hive integration
            \item \textbf{Spark Streaming:} Processes live data streams
            \item \textbf{MLlib:} Contains algorithms for machine learning tasks
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Apache Spark - Example Code Snippet}
    \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize a Spark session
spark = SparkSession.builder \
    .appName("Sample Spark Application") \
    .getOrCreate()

# Load DataFrame from a CSV file
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Display the DataFrame
df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Apache Spark - Introduction}
    \begin{block}{Introduction to Apache Spark}
        Apache Spark is a powerful open-source distributed computing system designed for fast processing of large data sets across clusters of computers. 
        It provides several key components for data processing, including:
        \begin{itemize}
            \item Resilient Distributed Datasets (RDDs)
            \item DataFrames
        \end{itemize}
        Spark supports both batch and streaming data operations, making it versatile for various use cases.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Apache Spark - Key Concepts}
    \begin{block}{Resilient Distributed Datasets (RDDs)}
        \begin{itemize}
            \item Fundamental data structure in Spark.
            \item Represents a distributed collection of objects providing fault tolerance.
            \item Key Features:
            \begin{itemize}
                \item Immutable: Cannot be modified after creation.
                \item Lazy Evaluation: Executed only when an action is called.
                \item Supports parallel processing.
            \end{itemize}
        \end{itemize}
        \begin{lstlisting}[language=Python]
from pyspark import SparkContext
sc = SparkContext("local", "RDD Example")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)  # Create an RDD from a list
print(rdd.collect())  # Action: Fetch all data
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Apache Spark - DataFrames and Transformations}
    \begin{block}{DataFrames}
        \begin{itemize}
            \item A distributed collection of data organized into named columns.
            \item Similar to tables in relational databases.
            \item Key Features:
            \begin{itemize}
                \item Has a schema that defines its columns and types.
                \item Integrated with SQL for executing queries.
                \item Utilizes optimized execution engines.
            \end{itemize}
        \end{itemize}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()
df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
df.show()  # Display the DataFrame
df.select("name").filter(df.id > 1).show()  # SQL-like query
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Apache Spark - Transformations and Actions}
    \begin{block}{Transformations and Actions}
        \begin{itemize}
            \item \textbf{Transformations:} Create new RDDs or DataFrames from existing ones (e.g., map, filter).
            \begin{itemize}
                \item Operations are lazy until an action is triggered.
            \end{itemize}
            \item \textbf{Actions:} Trigger execution and return results (e.g., collect, count).
        \end{itemize}
        \begin{lstlisting}[language=Python]
# Transformation example
rdd2 = rdd.map(lambda x: x * 2)  # Double each element
print(rdd2.collect())  # Action: Fetch results after transformation
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Using Apache Spark - Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item RDDs provide control and flexibility, while DataFrames allow for simpler operations.
            \item Spark's lazy evaluation enhances performance by delaying execution.
            \item Integrated SQL capability makes Spark accessible for those familiar with databases.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Apache Spark is essential for large-scale data processing, providing efficient and fault-tolerant methods. 
        Leverage these components for effective data analytics and exploration of Spark's powerful features!
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Data Ingestion Techniques}
    \begin{block}{Overview of Data Ingestion}
        Data ingestion is the process of importing data for immediate use or storage in a database. In big data frameworks like Hadoop and Spark, effective data ingestion is crucial for efficient data processing. This presentation covers two main techniques: \textbf{Batch} and \textbf{Streaming}.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Batch Data Ingestion}
    \begin{itemize}
        \item \textbf{Definition}: Involves collecting and processing data in large blocks; data is gathered over time and processed as a single job.
        
        \item \textbf{Use Case Example}:
            \begin{enumerate}
                \item \textbf{Scenario}: Loading historical sales data into a data warehouse for analysis.
                \item \textbf{Process}:
                    \begin{itemize}
                        \item Data is exported from sources (e.g., databases, CSV files).
                        \item A batch job is scheduled to run nightly to load the dataset into the analytical platform using tools like Apache Sqoop or Hadoopâ€™s FileSystem API.
                    \end{itemize}
            \end{enumerate}
        
        \item \textbf{Key Characteristics}:
            \begin{itemize}
                \item High throughput: Efficient for large datasets.
                \item Latency: Can have higher latency due to bulk processing.
                \item Tools: Apache Hive, Apache Flume, Apache Sqoop.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Batch Data Ingestion - Code Example}
    \begin{block}{Example: Spark Batch Ingestion}
        \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Batch Ingestion").getOrCreate()
data = spark.read.csv("hdfs://path/to/sales_data.csv", header=True, inferSchema=True)
data.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Streaming Data Ingestion}
    \begin{itemize}
        \item \textbf{Definition}: Involves continuous data flow, capturing data in real-time as it is created. Best for applications requiring immediate insights.
        
        \item \textbf{Use Case Example}:
            \begin{enumerate}
                \item \textbf{Scenario}: Monitoring live Twitter feeds for sentiment analysis.
                \item \textbf{Process}:
                    \begin{itemize}
                        \item Data is ingested in real-time using tools like Apache Kafka.
                        \item Spark Streaming processes data in micro-batches.
                    \end{itemize}
            \end{enumerate}
        
        \item \textbf{Key Characteristics}:
            \begin{itemize}
                \item Real-time processing: Analyzes data instantaneously as it flows.
                \item Lower latency: Suitable for applications needing fresh information.
                \item Tools: Apache Kafka, Apache Spark Streaming, Apache Flink.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Streaming Data Ingestion - Code Example}
    \begin{block}{Example: Spark Streaming Ingestion}
        \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Streaming Ingestion").getOrCreate()
kafkaStream = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "twitter_topics").load()
kafkaStream.selectExpr("CAST(value AS STRING)").writeStream.format("console").start()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Understanding Needs}: Choose batch ingestion for high-volume static datasets; streaming for real-time data processes.
        \item \textbf{Tools and Integrations}: Familiarize with tools enhancing data ingestion (e.g., Sqoop, Kafka).
        \item \textbf{Performance Considerations}: Balance data freshness with processing overhead and system resources.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Effective data ingestion is foundational for successful big data analysis. Understanding and employing both batch and streaming techniques equips data engineers to handle diverse data workloads.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Transformation Processes - Introduction to ETL}
    \begin{block}{What is ETL?}
        ETL stands for \textbf{Extract, Transform, Load}. It is a fundamental process in data warehousing and data integration.
    \end{block}
    \begin{enumerate}
        \item \textbf{Extract}: Retrieve data from various data sources (databases, APIs, files).
        \item \textbf{Transform}: Cleanse, aggregate, and convert data to fit operational needs.
        \item \textbf{Load}: Insert the transformed data into a target system (data warehouse, database, etc.).
    \end{enumerate}
    Understanding ETL is crucial for efficient data management and analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{ETL in Hadoop - Tools and Example}
    \begin{block}{Hadoop ETL Tools}
        Key tools available in Hadoop for ETL processes include:
        \begin{itemize}
            \item \textbf{Apache Sqoop}: Facilitates bulk data transfer between Hadoop and relational databases.
            \item \textbf{Apache Pig}: A high-level platform for creating programs that run on Hadoop, useful for transforming data.
            \item \textbf{Apache Hive}: Allows SQL-like queries to be executed, great for loading data into Hadoop.
        \end{itemize}
    \end{block}
    \begin{block}{Example ETL Process}
        \textbf{Extract:} Use Sqoop to pull data from MySQL:
        \begin{lstlisting}[language=bash]
sqoop import --connect jdbc:mysql://mysql-server/db --username user --password pass --table my_table --target-dir /user/hadoop/my_table
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ETL in Hadoop - Transformation and Loading}
    \begin{block}{Transform and Load Using Hadoop}
        \textbf{Transform:} Use Pig to filter and clean data:
        \begin{lstlisting}[language=Pig]
data = LOAD '/user/hadoop/my_table' USING PigStorage(',') AS (name:chararray, age:int);
filtered_data = FILTER data BY age > 21;
STORE filtered_data INTO '/user/hadoop/transformed_table';
        \end{lstlisting}
        
        \textbf{Load:} Use Hive to create a table and load data:
        \begin{lstlisting}[language=sql]
CREATE TABLE my_final_table (name STRING, age INT);
LOAD DATA INPATH '/user/hadoop/transformed_table' INTO TABLE my_final_table;
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ETL in Spark - Tools and Example}
    \begin{block}{Spark ETL Tools}
        Apache Spark provides an advanced platform for ETL processes:
        \begin{itemize}
            \item \textbf{Spark SQL}: Allows querying structured data using SQL.
            \item \textbf{DataFrames}: Optimize transformations through Spark's DataFrame API.
            \item \textbf{Spark Streaming}: Handles real-time data processing.
        \end{itemize}
    \end{block}
    \begin{block}{Example ETL Process}
        \textbf{Extract:} Reading data from a CSV file:
        \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ETL Example").getOrCreate()
df = spark.read.csv("path/to/input.csv", header=True)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ETL in Spark - Transformation and Loading}
    \begin{block}{Transform and Load Using Spark}
        \textbf{Transform:} Using DataFrame operations to clean data:
        \begin{lstlisting}[language=python]
transformed_df = df.filter(df['age'] > 21).select("name", "age")
        \end{lstlisting}
        
        \textbf{Load:} Writing data to a database:
        \begin{lstlisting}[language=python]
transformed_df.write.format("jdbc").option("url", "jdbc:mysql://mysql-server/db").option("dbtable", "transformed_table").option("user", "user").option("password", "pass").save()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item ETL is essential for data integration from multiple sources.
            \item Hadoop's tools like Sqoop, Pig, and Hive facilitate the ETL process effectively.
            \item Spark enhances ETL with flexibility, speed, and real-time processing capabilities.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Both Hadoop and Spark provide robust frameworks for executing ETL processes, each suited for different contexts. Understanding how to utilize these tools effectively is critical for data engineers and scientists.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{APIs and System Integration}
    % Discussion on integrating various data processing systems through APIs, ensuring efficient data flow across platforms using Hadoop and Spark.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding APIs in Data Processing}
    \begin{itemize}
        \item \textbf{Definition}: APIs are sets of rules and protocols for building and integrating applications. They define the methods and data formats for communication.
        \item \textbf{Importance}: Enable seamless interoperability between disparate systems, facilitating smooth data flows and enhancing processing capabilities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Benefits of Integration through APIs}
    \begin{itemize}
        \item \textbf{Scalability}: Facilitates integration of additional services and resources without disrupting existing workflows.
        \item \textbf{Flexibility}: Allows developers to change and update systems independently, easing the adoption of new technologies.
        \item \textbf{Real-time Data Access}: Ensures immediate access to current data from various sources.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating Hadoop and Spark Using APIs}
    \begin{block}{Hadoop}
        \begin{itemize}
            \item \textbf{REST API}: Interfacing with components like HDFS and YARN.
            \item Example:
            \begin{lstlisting}
curl -X GET "http://<namenode>:9870/api/v1/nameservices"
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integrating Hadoop and Spark Using APIs}
    \begin{block}{Spark}
        \begin{itemize}
            \item \textbf{Spark API}: Allows integration with diverse data sources.
            \item Example of writing a DataFrame to MySQL:
            \begin{lstlisting}
df.write \
    .format("jdbc") \
    .option("url", "jdbc:mysql://<host>:<port>/<db>") \
    .option("dbtable", "table_name") \
    .option("user", "username") \
    .option("password", "password") \
    .save()
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points for Effective Integration through APIs}
    \begin{itemize}
        \item \textbf{Design for Interoperability}: Support multiple data formats and protocols (e.g., JSON, XML).
        \item \textbf{Error Handling}: Implement robust error handling and logging to monitor API calls.
        \item \textbf{Documentation}: Maintain clear API documentation for developers.
        \item \textbf{Testing}: Regular testing of API integrations to ensure efficiency and reliability.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{itemize}
        \item Leveraging APIs for integrating Hadoop and Spark enhances data processing capabilities.
        \item Ensures efficient data flows and maintains a highly adaptable technology stack.
        \item Understanding these integration points is crucial for developing effective data processing methodologies.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Performance Optimization Strategies}
    \begin{block}{Introduction}
        Performance optimization is crucial for efficient data processing tasks, especially with large datasets. Both Hadoop and Spark provide unique techniques to improve processing efficiency and reduce costs.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Performance Optimization Techniques}
    \begin{itemize}
        \item \textbf{Data Locality}
        \begin{itemize}
            \item Minimizes network I/O by processing data on the node where it resides.
            \item Example: Utilizing Hadoop's scheduling to run tasks on the data's local node.
        \end{itemize}
        
        \item \textbf{Memory Management}
        \begin{itemize}
            \item \textit{Spark RDDs}: Supports in-memory processing, reducing disk I/O.
            \item \textit{Hadoop YARN}: Allows fine-tuning memory allocation for improved application throughput.
        \end{itemize}
        
        \item \textbf{Efficient Data Formats}
        \begin{itemize}
            \item Columnar formats like Parquet or ORC enhance read operations.
            \item Optimized serialization libraries (e.g., Avro, Protocol Buffers) improve data transfer speed.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Optimization Techniques in Action}
    
    \textbf{Using Spark Caching:}
    \begin{lstlisting}[language=Python]
# Example: Caching RDD for reuse in Spark
rdd = spark.textFile("data.txt")
rdd.cache()  # Cache the RDD to memory for faster access
result = rdd.map(lambda line: line.split()).count()
    \end{lstlisting}
    
    \textbf{Hadoop MapReduce Tuning:}
    \begin{itemize}
        \item Optimize parameters such as:
        \begin{itemize}
            \item \texttt{mapreduce.map.memory.mb}
            \item \texttt{mapreduce.reduce.memory.mb}
        \end{itemize}
        \item Implement combiner functions to minimize data transfer between map and reduce phases.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Prioritize data locality to minimize data transfer costs.
        \item Optimize memory usage based on application requirements.
        \item Choose appropriate data formats to enhance read efficiency.
        \item Leverage Spark's in-memory computation capabilities to reduce latency.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Performance optimization is vital in large-scale data processing. 
        Understanding the tools and techniques available in both Hadoop and Spark can lead to significant improvements in execution time and resource utilization. 
        By focusing on these strategies, data engineers can enhance the ability to process large datasets efficiently while maintaining performance and cost-effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing}
    % Overview of ethical considerations in data processing, focusing on implications and industry best practices.

    \begin{block}{Definition}
        Ethical considerations in data processing refer to the moral principles guiding the collection, analysis, and utilization of data, especially large datasets with sensitive information.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Concerns}
    
    \begin{enumerate}
        \item \textbf{Data Privacy and Consent:}
        \begin{itemize}
            \item Individuals must be informed about data use.
            \item Example: GDPR mandates explicit consent from users.
        \end{itemize}
        
        \item \textbf{Data Security:}
        \begin{itemize}
            \item Protect data from unauthorized access and breaches.
            \item Example: Encryption and access controls.
        \end{itemize}

        \item \textbf{Bias and Fairness:}
        \begin{itemize}
            \item Algorithms can perpetuate existing biases.
            \item Example: Hiring algorithms may disadvantage certain groups.
        \end{itemize}

        \item \textbf{Accountability and Transparency:}
        \begin{itemize}
            \item Organizations need to be transparent about practices.
            \item Example: Documenting decision-making processes in public services.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Industry Best Practices}
    
    \begin{itemize}
        \item \textbf{Data Minimization:} Collect only necessary data; avoid excess.
        \item \textbf{Anonymization Techniques:} Use aggregation and anonymization to safeguard identities.
        \item \textbf{Regular Audits:} Ensure compliance with ethical standards through periodic reviews.
        \item \textbf{Diverse Data Teams:} Foster diversity to mitigate biases in data interpretation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Case: Facebook-Cambridge Analytica}
    
    \begin{itemize}
        \item \textbf{Background:} Unauthorized harvesting of personal data influenced electoral outcomes, raising ethical concerns.
        
        \item \textbf{Implication:} Highlighted need for robust ethical frameworks in data processing to prevent misuse.
        
        \item \textbf{Key Takeaways:}
        \begin{itemize}
            \item Ethical considerations should be prioritized in data processing.
            \item Technologies must uphold data privacy and security.
            \item Engage in ongoing ethical discussions in data science.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    \begin{block}{Ethical Considerations in Data Processing}
        Addressing ethical considerations reflects a commitment to responsible data stewardship, respect for individual rights, and promoting social good.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Studies - Introduction}
    \begin{block}{Introduction to Data Processing at Scale}
        Data processing at scale involves managing and analyzing large volumes of data efficiently and effectively. Various industries leverage advanced tools and libraries to extract meaningful insights, improve operations, and drive decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Studies - Retail Industry}
    \begin{block}{Case Study 1: Retail Industry - Walmart}
        \begin{itemize}
            \item \textbf{Challenge:} Managing and analyzing massive datasets from millions of transactions daily to optimize inventory and enhance customer experience.
            \item \textbf{Solution:} 
            Walmart implemented a data processing platform leveraging \textbf{Apache Hadoop} to handle large-scale data storage and processing. The platform processed vast amounts of data in real-time to 
            predict trends and consumer demands.
            \item \textbf{Outcome:} Improved inventory management led to a 10\% reduction in waste and enhanced customer satisfaction due to better product availability.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Case Studies - Healthcare & Finance}
    \begin{block}{Case Study 2: Healthcare Industry - Mount Sinai Health System}
        \begin{itemize}
            \item \textbf{Challenge:} Integrating diverse healthcare data sources (EHRs, lab results, and patient feedback) to improve patient care and operational efficiency.
            \item \textbf{Solution:} 
            Utilizing \textbf{Apache Spark}, Mount Sinai developed a unified data processing system that analyzes patient data at speed. Apache Spark's in-memory computing capabilities enabled rapid insights.
            \item \textbf{Outcome:} Enhanced patient outcomes with personalized treatments and significant reductions in readmission rates, leading to improved operational efficiency.
        \end{itemize}
    \end{block}

    \begin{block}{Case Study 3: Financial Services - Goldman Sachs}
        \begin{itemize}
            \item \textbf{Challenge:} Processing vast amounts of transaction data to detect fraudulent activities and comply with regulatory requirements.
            \item \textbf{Solution:} 
            Implemented \textbf{Apache Flink} for real-time data streaming and processing, allowing analysis of transaction patterns to identify fraud while ensuring compliance through real-time reporting.
            \item \textbf{Outcome:} Significant improvement in fraud detection capabilities and compliance reporting, reducing the time from detection to response by 30\%.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{enumerate}
            \item \textbf{Scalability is Crucial:} Choosing the right tools and libraries like Hadoop, Spark, and Flink is key to managing large datasets efficiently.
            \item \textbf{Real-Time Insights:} The ability to process data in real-time significantly enhances decision-making and operational efficiency.
            \item \textbf{Cross-Industry Applications:} Data processing solutions are adaptable across various sectors, demonstrating the versatility of modern data processing technologies.
        \end{enumerate}
    \end{block}

    \begin{block}{Conclusion}
        These case studies illustrate that effective data processing at scale not only enhances operational efficiencies but also drives innovation and improved outcomes across industries. By harnessing the power of advanced tools, organizations can unlock the full potential of their data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Data Processing with PySpark}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Data Processing Example").getOrCreate()

# Load data into a DataFrame
df = spark.read.csv("transactions.csv", header=True, inferSchema=True)

# Perform data transformation and aggregation
result_df = df.groupBy("product_id").agg({"sales": "sum", "quantity": "avg"})

# Show the results
result_df.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Project Overview}
    \begin{block}{Introduction}
        In this segment, we will explore a series of hands-on projects aimed at enhancing your proficiency in Apache Hadoop and Spark. These projects will provide an opportunity for you to apply theoretical concepts in practical scenarios, thereby solidifying your understanding of data processing frameworks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives}
    \begin{itemize}
        \item \textbf{Hands-On Experience}: Engage with real-world data processing challenges using Hadoop and Spark.
        \item \textbf{Skill Evaluation}: Assess your ability to utilize the core functionalities of both frameworks.
        \item \textbf{Guided Explorations}: Follow structured exercises to navigate through complex data tasks successfully.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Outline}
    \begin{enumerate}
        \item \textbf{Data Ingestion with Apache Hadoop}
            \begin{itemize}
                \item \textbf{Objective}: Learn how to ingest large datasets into Hadoop's HDFS (Hadoop Distributed File System).
                \item \textbf{Exercise}:
                    \begin{lstlisting}[language=bash]
hdfs dfs -put localfile.csv /user/hadoop/
                    \end{lstlisting}
                \item \textbf{Key Points}:
                    \begin{itemize}
                        \item Familiarize with HDFS commands for file management.
                        \item Understand data format compatibility and directory structure in Hadoop.
                    \end{itemize}
            \end{itemize}
        \item \textbf{Data Processing with Apache MapReduce}
            \begin{itemize}
                \item \textbf{Objective}: Implement a basic MapReduce job to process the ingested data.
                \item \textbf{Exercise}:
                    \begin{lstlisting}[language=bash]
cat input.txt | python mapper.py | sort | python reducer.py > output.txt
                    \end{lstlisting}
                \item \textbf{Key Points}:
                    \begin{itemize}
                        \item Understand the roles of the Mapper and Reducer.
                        \item Grasp the importance of intermediate data and how it's shuffled and sorted.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Outline Continued}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Real-Time Data Processing with Apache Spark}
            \begin{itemize}
                \item \textbf{Objective}: Utilize Spark for batch and stream processing to handle real-time data.
                \item \textbf{Exercise}:
                    \begin{lstlisting}[language=python]
from pyspark import SparkContext
sc = SparkContext("local", "TwitterStream")
lines = sc.socketTextStream("localhost", 9999)
wordCounts = lines.flatMap(lambda line: line.split(" ")) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)
                    \end{lstlisting}
                \item \textbf{Key Points}:
                    \begin{itemize}
                        \item Understand the concept of RDDs (Resilient Distributed Datasets).
                        \item Explore the advantages of in-memory computation for fast processing.
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Expected Outcomes and Conclusion}
    \begin{itemize}
        \item \textbf{Proficiency in Hadoop and Spark}: Apply learned skills to efficiently ingest and process large datasets.
        \item \textbf{Analytical Thinking}: Enhance problem-solving skills by tackling data challenges in realistic contexts.
        \item \textbf{Integration Insights}: Know how to integrate different systems for optimized data processing.
    \end{itemize}
    \begin{block}{Conclusion}
        Through these hands-on projects, you will build a robust foundation in using Apache Hadoop and Spark for data processing. Engaging in these exercises will not only reinforce your learning but also prepare you for real-world data management challenges.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Introduction}
    \begin{block}{Introduction}
        In this chapter, we explored essential tools and libraries that enable effective data processing at scale. 
        Handling vast datasets requires powerful, reliable systems, making it crucial for practitioners to become proficient in the tools available.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Key Points Recap}
    \begin{enumerate}
        \item \textbf{Importance of Tools and Libraries:}
            \begin{itemize}
                \item Data processing at scale necessitates specialized frameworks that can handle large volumes of data efficiently.
                \item Knowledge of these tools helps streamline workflows, minimizes processing time, and optimizes resource use.
            \end{itemize}
        
        \item \textbf{Apache Hadoop:}
            \begin{itemize}
                \item Distributed storage and processing framework using a cluster of computers.
                \item \textbf{Key Components:}
                    \begin{itemize}
                        \item \textbf{HDFS:} Stores data across multiple nodes for fault tolerance and high availability.
                        \item \textbf{MapReduce:} Framework that processes data in parallel via a two-step approach (Map and Reduce).
                    \end{itemize}
                \item \textbf{Example Usage:} Analyzing large datasets for e-commerce using transactional logs in HDFS.
            \end{itemize}
        
        \item \textbf{Apache Spark:}
            \begin{itemize}
                \item Powerful engine for large-scale data processing emphasizing speed and ease of use.
                \item \textbf{Key Features:}
                    \begin{itemize}
                        \item \textbf{In-Memory Processing:} Significantly reduces processing time compared to Hadoop's disk-based processing.
                        \item \textbf{Unified Analytics Engine:} Supports batch processing, streaming, machine learning, and graph processing.
                    \end{itemize}
                \item \textbf{Example Usage:} Real-time data analytics for social media by processing feeds instantly.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion - Integration and Best Practices}
    \begin{enumerate}
        \setcounter{enumi}{3}  % Continue the enumeration
        \item \textbf{Integration of Tools:}
            \begin{itemize}
                \item Understanding how to integrate these libraries is essential. For example, Spark can run on Hadoop, utilizing HDFS.
            \end{itemize}
            \begin{block}{Architecture Overview}
                \begin{lstlisting}
                +-------------------+
                |     Data Source    |
                +--------+----------+
                         |
                +--------v----------+
                |   HDFS (Hadoop)   |<--- Importing Data
                +-------------------+
                         |
                +--------v----------+
                |      Spark        |<--- Processing
                +-------------------+
                         |
                +--------v----------+
                |     Data Output    |
                +-------------------+
                \end{lstlisting}
            \end{block}
        
        \item \textbf{Best Practices:}
            \begin{itemize}
                \item Familiarize yourself with both high-level APIs (Spark DataFrame) and low-level APIs (RDD) for optimization.
                \item Keep scalability and efficiency in mind when configuring cluster resources.
            \end{itemize}

        \item \textbf{Conclusion:}
            Mastering tools like Hadoop and Spark is indispensable for modern data professionals, enabling effective handling of data processing tasks.
    \end{enumerate}
\end{frame}


\end{document}