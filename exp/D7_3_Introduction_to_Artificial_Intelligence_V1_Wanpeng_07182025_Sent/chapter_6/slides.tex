\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Critical Metrics for AI Algorithm Evaluation}
    \begin{block}{Learning Objectives}
        \begin{itemize}
            \item Understand the role of performance metrics in evaluating AI algorithms.
            \item Identify key performance metrics commonly used in the AI community.
            \item Recognize the implications of metric choice on algorithm assessment.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Evaluating AI Algorithms}
    Evaluating AI algorithms is critical to ensure they perform effectively and meet user expectations. 
    Performance metrics provide a quantitative basis for comparing different models and their ability to achieve specific tasks. 
    When developing AI systems, choosing the right metric can significantly influence decisions about training, optimization, and deployment.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Performance Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item Definition: The ratio of correctly predicted instances to the total instances.
            \item Formula: 
            \begin{equation}
                \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}
            \end{equation}
            \item Example: In a binary classification task with 90 correct predictions out of 100 total instances, the accuracy is 90\%.
        \end{itemize}
        
        \item \textbf{Precision}
        \begin{itemize}
            \item Definition: The ratio of true positive predictions to the sum of true positives and false positives.
            \item Formula:
            \begin{equation}
                \text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}}
            \end{equation}
            \item Example: If a model identifies 30 positives, of which 20 are true positives, then precision is \( \frac{20}{30} = 0.67\) or 67\%.
        \end{itemize}

        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item Definition: The ratio of true positives to the sum of true positives and false negatives.
            \item Formula:
            \begin{equation}
                \text{Recall} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}
            \end{equation}
            \item Example: If there are 25 actual positives and the model predicts 20 true positives (and misses 5), recall is \( \frac{20}{25} = 0.8\) or 80\%.
        \end{itemize}
        
        \item \textbf{F1 Score}
        \begin{itemize}
            \item Definition: The harmonic mean of precision and recall, useful for measuring model performance when classes are imbalanced.
            \item Formula:
            \begin{equation}
                F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision + Recall}}
            \end{equation}
            \item Example: If the precision is 67\% and recall is 80\%, the F1 score would be \( F1 = 2 \cdot \frac{0.67 \cdot 0.8}{0.67 + 0.8} \approx 0.73\).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Choosing the appropriate metric is crucial based on the problem context (e.g., precision is often prioritized in medical diagnosis).
        \item Performance metrics provide insight into different aspects of model performance, which can help inform decisions about model improvements or deployments.
        \item Evaluating models against metrics helps in continuously monitoring their effectiveness after deployment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By understanding these metrics, you can critically assess AI algorithms and make informed decisions about their practicality and reliability in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What are Performance Metrics?}
    \begin{block}{Definition}
        Performance metrics are quantitative measures used to evaluate the effectiveness of AI algorithms. They allow developers and researchers to assess how well their models are performing and identify areas for improvement.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Performance Metrics}
    \begin{itemize}
        \item \textbf{Objective Assessment}: Metrics provide a standardized means to assess algorithm performance, translating complex AI behavior into understandable numbers.
        
        \item \textbf{Model Comparison}: They facilitate comparison between multiple models, guiding the selection of the best-performing algorithm for a given task.
        
        \item \textbf{Informed Decision-Making}: Metrics inform stakeholders about the strengths and weaknesses of an AI system, guiding development and operational decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Common Metrics}
    \begin{enumerate}
        \item \textbf{Performance is Context-Dependent}: The choice of metrics may vary based on application and goals.
        
        \item \textbf{Common Metrics}:
        \begin{itemize}
            \item \textbf{Accuracy}: \( \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \)
            \item \textbf{Precision}: \( \text{Precision} = \frac{TP}{TP + FP} \)
            \item \textbf{Recall (Sensitivity)}: \( \text{Recall} = \frac{TP}{TP + FN} \)
            \item \textbf{F1 Score}: Harmonic mean of precision and recall.
            \item \textbf{AUC-ROC}: Area Under the Receiver Operating Characteristic curve for evaluating classification problems.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example}
    \begin{block}{Binary Classification Task}
        Consider a task of predicting whether an email is spam or not:
        \begin{itemize}
            \item \textbf{True Positives (TP)}: Emails correctly labeled as spam.
            \item \textbf{False Positives (FP)}: Legitimate emails incorrectly labeled as spam.
            \item \textbf{False Negatives (FN)}: Spam emails not detected.
        \end{itemize}
        Using these results, you could calculate:
        \begin{itemize}
            \item Accuracy: \( \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \)
            \item Precision: \( \text{Precision} = \frac{TP}{TP + FP} \)
            \item Recall: \( \text{Recall} = \frac{TP}{TP + FN} \)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item Performance metrics are vital for quantifying and improving AI algorithms.
        \item They help ensure AI systems meet specified requirements and adapt to changing needs.
        \item Selecting appropriate metrics aligned with project objectives is essential for meaningful evaluation and optimization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Commonly Used Performance Metrics}
    % Introduction
    \begin{block}{Introduction to Performance Metrics}
        Performance metrics are essential tools for evaluating the effectiveness of AI algorithms. They provide insight into model performance and areas for improvement. In this section, we will review five key metrics: 
        \begin{itemize}
            \item Accuracy
            \item Precision
            \item Recall
            \item F1 Score
            \item AUC-ROC
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Accuracy}
    % Accuracy Overview
    \begin{block}{Definition}
        Accuracy is the ratio of correctly predicted instances to the total instances evaluated.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
        \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
        \end{equation}
    \end{block}

    \begin{block}{Example}
        If a model predicts 90 out of 100 outcomes correctly, the accuracy is 90\%.
    \end{block}
    
    \begin{block}{Key Point}
        Accuracy can be misleading in imbalanced datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Precision and 3. Recall}
    % Precision Overview
    \begin{block}{Precision}
        Precision measures the proportion of true positive predictions to the total positive predictions made by the model.
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
        \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}
    \end{block}

    \begin{block}{Example}
        In a spam filter with 80 emails labeled as spam (30 correct, 50 incorrect), Precision = \( \frac{30}{30 + 50} = 0.375 \) or 37.5\%.
    \end{block}

    \begin{block}{Key Point}
        High precision reduces false positives, critical in applications like diagnosis.
    \end{block}
    
    \begin{block}{Recall}
        Recall measures the proportion of true positives to the total actual positives in the dataset.
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
        \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
    \end{block}

    \begin{block}{Example}
        In a medical test scenario with 70 actual sick patients identified (50 true positive, 20 missed), Recall = \( \frac{50}{50 + 20} = 0.714 \) or 71.4\%.
    \end{block}
    
    \begin{block}{Key Point}
        High recall is essential when missing a positive instance is costly.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{4. F1 Score and 5. AUC-ROC}
    % F1 Score Overview
    \begin{block}{F1 Score}
        The F1 Score is the harmonic mean of precision and recall.
    \end{block}

    \begin{block}{Formula}
        \begin{equation}
        \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{block}

    \begin{block}{Example}
        If Precision is 0.8 and Recall is 0.6, then F1 Score = \( 2 \times \frac{0.8 \times 0.6}{0.8 + 0.6} = 0.686 \) or 68.6\%.
    \end{block}
    
    \begin{block}{Key Point}
        The F1 Score balances precision and recall, useful in imbalanced classes.
    \end{block}

    \begin{block}{AUC-ROC}
        AUC measures separability between classes via the ROC curve.
    \end{block}

    \begin{block}{Interpretation}
        AUC values range from 0 to 1, with 1 indicating perfect separability.
    \end{block}

    \begin{block}{Example}
        A model with an AUC of 0.85 indicates good separability between classes.
    \end{block}
    
    \begin{block}{Key Point}
        AUC-ROC is valuable in binary classification, summarizing performance across thresholds.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    % Conclusion
    Understanding these performance metrics is crucial for evaluating AI algorithms effectively. 
    Utilizing these metrics empowers practitioners to make informed decisions regarding model selection and refinements, ultimately enhancing performance in real-world applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Overview}
    \begin{block}{What is Accuracy?}
        Accuracy is a metric for evaluating the performance of machine learning algorithms. It measures the proportion of correctly classified instances among the total instances examined. 
    \end{block}
    \begin{block}{Formula}
        \begin{equation}
        \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} 
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Calculation}
    \begin{block}{How is Accuracy Calculated?}
        \begin{enumerate}
            \item \textbf{Identify Correct Predictions}: Count True Positives (TP) and True Negatives (TN).
            \item \textbf{Total Predictions}: Count all instances: TP + TN + False Positives (FP) + False Negatives (FN).
            \item \textbf{Apply the Formula}: Insert counts into the accuracy formula.
        \end{enumerate}
    \end{block}
    \begin{block}{Example Calculation}
        Given a dataset with 100 instances:
        \begin{itemize}
            \item TP: 70
            \item TN: 20
            \item FP: 5
            \item FN: 5
        \end{itemize}
        Applying the formula:
        \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{70 + 20}{100} = 0.9 \text{ or } 90\%
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Accuracy - Misleading Scenarios}
    \begin{block}{When Can Accuracy Be Misleading?}
        \begin{enumerate}
            \item \textbf{Imbalanced Datasets}: Accuracy may be high while the model fails to identify minority classes.
            \item \textbf{No Information Gain}: Doesn't account for the consequences of different types of errors (e.g., False Negatives in medical diagnosis).
            \item \textbf{Different Contexts}: Accuracy can mislead in specific applications (like spam detection).
        \end{enumerate}
    \end{block}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item Accuracy should not be the sole measure of model performance.
            \item Consider other metrics such as precision, recall, and F1 score.
            \item Evaluate accuracy within the specific application context.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Definitions}
    \begin{block}{Definitions & Concepts}
        Precision and recall are critical metrics for evaluating classification algorithms, especially with unbalanced class distributions.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Precision}
        \begin{itemize}
            \item Definition: Measures the accuracy of positive predictions.
            \item Formula: 
            \begin{equation}
                \text{Precision} = \frac{TP}{TP + FP}
            \end{equation}
            \item Where:
            \begin{itemize}
                \item \( TP \) = True Positives
                \item \( FP \) = False Positives
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Recall}
        \begin{itemize}
            \item Definition: Measures the model's ability to identify all relevant instances.
            \item Formula:
            \begin{equation}
                \text{Recall} = \frac{TP}{TP + FN}
            \end{equation}
            \item Where:
            \begin{itemize}
                \item \( FN \) = False Negatives
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Precision and Recall - Interdependence and Importance}
    \begin{block}{Interdependent Relationship}
        Precision and recall are interdependent metrics. Improving one can often lead to a decrease in the other. This trade-off is significant in applications where the costs of false positives and false negatives differ.
    \end{block}

    \begin{block}{Importance in Classification Problems}
        \begin{itemize}
            \item \textbf{Imbalanced Data:} In scenarios like fraud detection, accuracy can be misleadingâ€”precision and recall provide clearer insights.
            
            \item \textbf{Use Cases:}
            \begin{itemize}
                \item \textbf{Medical Diagnosis:} High recall is essential to identify most patients with a disease.
                \item \textbf{Spam Detection:} Higher precision reduces the risk of marking legitimate emails as spam.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example of Precision and Recall}
    \begin{block}{Confusion Matrix Example}
        Consider the following confusion matrix:
        
        \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            & \text{Predicted Positive} & \text{Predicted Negative} \\
            \hline
            \text{Actual Positive} & 70 (TP) & 30 (FN) \\
            \hline
            \text{Actual Negative} & 10 (FP) & 90 (TN) \\
            \hline
        \end{tabular}
        \end{center}
        
        \begin{itemize}
            \item \textbf{Precision:} 
            \begin{equation}
                \text{Precision} = \frac{70}{70 + 10} = 0.875 \, (87.5\%)
            \end{equation}
            \item \textbf{Recall:} 
            \begin{equation}
                \text{Recall} = \frac{70}{70 + 30} = 0.7 \, (70\%)
            \end{equation}
        \end{itemize}
    \end{block}
    
    \begin{block}{Final Note}
        In the next slide, we will explore the \textbf{F1 Score}, which combines precision and recall into a useful single metric.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Definition}
    \begin{block}{Definition}
        The \textbf{F1 score} is a crucial metric used to evaluate the performance of classification algorithms, particularly in scenarios where the class distribution is imbalanced. 
        It is defined as the harmonic mean of \textbf{precision} and \textbf{recall}, providing a single score that reflects both false positives and false negatives.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Key Concepts}
    \begin{itemize}
        \item \textbf{Precision}: Measures the accuracy of positive predictions.
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
        \end{equation}
        
        \item \textbf{Recall}: Measures the ability of a model to identify all relevant instances.
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
        \end{equation}

        \item \textbf{F1 Score}: Balances precision and recall, especially important in cases of class imbalance.
        \begin{equation}
            F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Example}
    \begin{block}{Example}
        Imagine a medical test for a rare disease:
        \begin{itemize}
            \item \textbf{True Positives (TP)}: 80 patients correctly identified as having the disease
            \item \textbf{False Positives (FP)}: 20 patients incorrectly identified as having the disease
            \item \textbf{False Negatives (FN)}: 10 patients who have the disease but were not identified
        \end{itemize}
        
        \textbf{Calculating Precision and Recall:}
        \begin{align*}
            \text{Precision} &= \frac{80}{80 + 20} = 0.8 \\
            \text{Recall} &= \frac{80}{80 + 10} \approx 0.89
        \end{align*}

        \textbf{Calculating F1 Score:}
        \begin{align*}
            F1 &\approx 2 \times \frac{0.8 \times 0.89}{0.8 + 0.89} \approx 0.84
        \end{align*}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - When to Use}
    \begin{block}{When to Use F1 Score}
        \begin{itemize}
            \item \textbf{Imbalanced Datasets}: Use when the positive class is rare (e.g., fraud detection, disease detection).
            \item \textbf{Importance of Both Precision and Recall}: When the costs of false positives and false negatives are equal or both are highly critical.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The F1 score is valuable for balancing precision and recall.
            \item It mitigates the limitations of using accuracy as a sole metric, especially where one class dominates.
            \item Aim for a higher F1 score (closer to 1) to indicate better model performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{F1 Score - Conclusion}
    \begin{block}{Conclusion}
        The F1 score is a vital tool for accurately assessing classification algorithms in complex scenarios,
        where understanding the trade-off between precision and recall can lead to better model deployment in real-world applications.
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Understanding AUC-ROC}
    \begin{block}{Definition}
        **AUC-ROC** (Area Under the Curve - Receiver Operating Characteristics) is a performance measurement for classification problems at various threshold settings, especially useful for binary classification tasks.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Definitions}
    \begin{itemize}
        \item \textbf{ROC Curve:} A graphical representation illustrating the diagnostic ability of a binary classifier system as the discrimination threshold varies.
        \item \textbf{True Positive Rate (TPR):} Also known as sensitivity or recall, defined as \( \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}} \).
        \item \textbf{False Positive Rate (FPR):} Defined as \( \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}} \).
    \end{itemize}
    
    The ROC curve plots the TPR against the FPR at various threshold settings.
\end{frame}

\begin{frame}
    \frametitle{Area Under Curve (AUC)}
    \begin{itemize}
        \item \textbf{AUC:} Represents the degree of separability achieved by the model.
        \item \textbf{Value Interpretations:}
            \begin{itemize}
                \item AUC = 1: Perfect model.
                \item AUC = 0.5: No discriminative power (similar to random guessing).
                \item AUC < 0.5: Performs worse than random guessing.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{enumerate}
        \item \textbf{Threshold Independence:} AUC-ROC is threshold-independent, providing an aggregate performance measure across all thresholds.
        \item \textbf{Interpretation of AUC Values:}
            \begin{itemize}
                \item AUC 0.90-1.00: Excellent
                \item AUC 0.80-0.90: Good
                \item AUC 0.70-0.80: Fair
                \item AUC 0.60-0.70: Poor
                \item AUC 0.50: No discrimination
            \end{itemize}
        \item \textbf{Visualization:} ROC curve shape indicates model performance; a curve closer to the upper left corner signifies better performance.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example: Spam Classification}
    Consider a binary classifier for determining if an email is spam:
    \begin{itemize}
        \item At a threshold of 0.5, classify emails with a probability of being spam higher than 50% as spam.
        \item Adjusting the threshold influences TPR and FPR, resulting in a ROC curve.
        \item An AUC of 0.85 implies the model effectively distinguishes between spam and non-spam emails.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Code Example}
    \begin{lstlisting}[language=Python]
from sklearn import metrics
import matplotlib.pyplot as plt

# Assuming y_test are the true labels and y_scores are the predicted probabilities
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_scores)
roc_auc = metrics.auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='blue', label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Introduction}
    In the evaluation of AI algorithms, selecting the right performance metric is crucial.  
    The metric you choose influences how you interpret your model's effectiveness and its performance in real-world applications.  
    This slide outlines key factors to consider when selecting metrics tailored to specific AI tasks and problem contexts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Key Factors}
    \begin{enumerate}
        \item \textbf{Nature of the Problem Type}
            \begin{itemize}
                \item \textbf{Classification:} Metrics like Accuracy, Precision, Recall, and F1 score.
                \item \textbf{Regression:} Metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE).
            \end{itemize}
        \item \textbf{Class Imbalance}
            \begin{itemize}
                \item Accuracy can be misleading; consider F1 Score or AUC-ROC for better insights.
                \item Example of class imbalance: 95\% negatives and 5\% positives results in high accuracy but poor model performance for the minority class.
            \end{itemize}
        \item \textbf{Business Objectives}
            \begin{itemize}
                \item Metrics should align with application goals (e.g., prioritize Recall in disease detection).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Choosing the Right Metric - Examples and Conclusion}
    \begin{block}{Examples of Metrics and Their Contexts}
        \begin{tabular}{|l|l|l|}
            \hline
            \textbf{Metric} & \textbf{Task Type} & \textbf{Usage Scenario} \\
            \hline
            Accuracy & Classification & General effectiveness across balanced classes \\
            Precision & Classification & High cost of false positives (spam detection) \\
            Recall & Classification & High cost of false negatives (disease diagnosis) \\
            F1 Score & Classification & Balancing Precision and Recall \\
            AUC-ROC & Classification & Performance across thresholds for imbalanced datasets \\
            MAE & Regression & General prediction accuracy in continuous contexts \\
            MSE & Regression & Penalizes larger errors more severely \\
            \hline
        \end{tabular}
    \end{block}
    
    \textbf{Conclusion:} Selecting the right metric involves understanding context, stakeholder needs, and the nature of the data. It's critical to ensure metrics align closely with your AI solution objectives.

    \textbf{Formula Examples:}
    \begin{equation}
        MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
    \end{equation}
    \begin{equation}
        MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Introduction}
    \begin{block}{Overview}
        When evaluating the performance of AI algorithms, it's crucial to understand that different metrics can provide varying insights. Each metric captures a specific aspect of performance, and selecting the right combination can significantly influence how we interpret results and make decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Key Concepts}
    \begin{itemize}
        \item \textbf{Confusion Matrix}
        \begin{itemize}
            \item True Positives (TP): Correctly predicted positive instances.
            \item True Negatives (TN): Correctly predicted negative instances.
            \item False Positives (FP): Incorrectly predicted positive instances.
            \item False Negatives (FN): Incorrectly predicted negative instances.
        \end{itemize}
        
        \item \textbf{Common Metrics}
        \begin{itemize}
            \item Accuracy:
            \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
            \end{equation}
            
            \item Precision:
            \begin{equation}
            \text{Precision} = \frac{TP}{TP + FP}
            \end{equation}

            \item Recall (Sensitivity):
            \begin{equation}
            \text{Recall} = \frac{TP}{TP + FN}
            \end{equation}

            \item F1-Score:
            \begin{equation}
            \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}

            \item ROC-AUC: A graphical plot illustrating the true positive rate vs. false positive rate at various thresholds.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Insights and Example}
    \begin{block}{Comparative Insights}
        \begin{itemize}
            \item **Accuracy vs. Precision/Recall**: 
            In imbalanced datasets, a high accuracy may be misleading.
            \item **F1-Score vs. ROC-AUC**: 
            The F1-Score is useful when false negatives are critical; ROC-AUC provides a broader perspective.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        Consider a medical diagnosis AI with:
        \begin{itemize}
            \item TP: 80, TN: 50, FP: 5, FN: 15
            \item Accuracy:
            \begin{equation}
            \text{Accuracy} = \frac{80 + 50}{80 + 50 + 5 + 15} \approx 86.67\%
            \end{equation}
            \item Precision:
            \begin{equation}
            \text{Precision} = \frac{80}{80 + 5} \approx 94.12\%
            \end{equation}
            \item Recall:
            \begin{equation}
            \text{Recall} = \frac{80}{80 + 15} \approx 84.21\%
            \end{equation}
            \item F1-Score:
            \begin{equation}
            \text{F1-Score} \approx 88.05
            \end{equation}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparative Analysis of Metrics - Key Takeaways}
    \begin{itemize}
        \item \textbf{Choose Metrics Aligned with Goals}: Tailor evaluation metrics to the specific objectives of your AI task.
        \item \textbf{Unique Insights}: Each metric serves a distinct purpose; use multiple metrics for a comprehensive view of performance.
        \item \textbf{Context Matters}: Consider the application context; critical metrics vary between scenarios.
    \end{itemize}
    
    Understanding the comparative strengths and weaknesses of evaluation metrics is essential for responsibly deploying AI algorithms.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advanced Considerations in AI Algorithm Evaluation}
    \begin{block}{Overview}
        In the evaluation of AI algorithms, relying on a single metric can be misleading. This slide explores the limitations of certain evaluation metrics and the necessity for comprehensive and multi-faceted evaluation strategies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Limitations of Common Metrics}
    \begin{itemize}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item \textit{Description:} Ratio of correctly predicted instances to total instances.
                \item \textit{Limitation:} Misleading for imbalanced datasets.
                \item \textit{Key Point:} Can hide poor performance in minority classes.
            \end{itemize}
        
        \item \textbf{Precision}
            \begin{itemize}
                \item \textit{Description:} Proportion of true positives to total predicted positives.
                \item \textit{Limitation:} Does not reflect overall effectiveness in some contexts.
                \item \textit{Key Point:} Requires careful consideration based on the domain.
            \end{itemize}
        
        \item \textbf{Recall}
            \begin{itemize}
                \item \textit{Description:} Proportion of true positives to total actual positives.
                \item \textit{Limitation:} Can lead to many false positives.
                \item \textit{Key Point:} Needs a balance with precision for practical applications.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Metrics and Evaluation Strategies}
    \begin{itemize}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item \textit{Description:} Harmonic mean of precision and recall.
                \item \textit{Limitation:} May mask underlying performance issues.
                \item \textit{Key Point:} Important to consider stakeholder priorities.
            \end{itemize}
        
        \item \textbf{ROC-AUC}
            \begin{itemize}
                \item \textit{Description:} Measures model's ability to distinguish between classes.
                \item \textit{Limitation:} Misleading in imbalanced datasets.
                \item \textit{Key Point:} Supplement with additional performance metrics.
            \end{itemize}
        
        \item \textbf{Comprehensive Evaluation Strategies}
            \begin{itemize}
                \item \textit{Multi-Metric Approach:} Combine various metrics for a full performance picture.
                \item \textit{Domain-Specific Evaluation:} Customize metrics to fit application needs.
                \item \textit{Cross-Validation:} Utilize k-fold techniques to ensure model robustness.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Applications - Introduction}
    In the evaluation of AI algorithms, critical metrics serve as the foundation for assessing performance, reliability, and effectiveness. Understanding practical applications of these metrics is essential for ensuring that AI systems not only meet technical specifications but also align with real-world needs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Performance Metrics Overview}
    \begin{itemize}
        \item \textbf{Performance Metrics Overview:}
        \begin{itemize}
            \item Performance metrics evaluate how well an AI algorithm performs its intended function. Common metrics include:
            \begin{itemize}
                \item \textbf{Accuracy:} The ratio of correctly predicted instances to the total instances.
                \item \textbf{Precision \& Recall:}
                \begin{itemize}
                    \item Precision measures the correctness of positive predictions.
                    \item Recall assesses how many actual positives were correctly identified.
                \end{itemize}
                \item \textbf{F1 Score:} The harmonic mean of precision and recall, providing a balance between the two.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Healthcare and Finance}
    \begin{itemize}
        \item \textbf{Healthcare Diagnosis:}
        \begin{itemize}
            \item \textbf{Example:} A machine learning model designed to classify medical images for the detection of tumors.
            \item \textbf{Metrics Used:} 
            \begin{itemize}
                \item \textbf{Accuracy} and \textbf{F1 Score} crucial for understanding positive detections.
                \item \textbf{Confusion Matrix} for performance visualization.
            \end{itemize}
            \item \textbf{Importance:} High precision minimizes false positives, while high recall ensures most actual cases are detected.
        \end{itemize}
        
        \item \textbf{Financial Fraud Detection:}
        \begin{itemize}
            \item \textbf{Example:} Algorithms used in banking to detect fraudulent transactions.
            \item \textbf{Metrics Used:}
            \begin{itemize}
                \item \textbf{Precision} to minimize false alerts.
                \item \textbf{AUC-ROC Curve} for assessing trade-offs across thresholds.
            \end{itemize}
            \item \textbf{Importance:} Balancing precision and recall is crucial to minimize loss due to fraud.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Sentiment Analysis and Key Points}
    \begin{itemize}
        \item \textbf{Sentiment Analysis:}
        \begin{itemize}
            \item \textbf{Example:} NLP models analyzing customer feedback on products.
            \item \textbf{Metrics Used:}
            \begin{itemize}
                \item \textbf{Recall} is essential for capturing positive sentiments.
                \item \textbf{Classification Reports} for detailed precision, recall, and F1 scores.
            \end{itemize}
            \item \textbf{Importance:} Effective analysis can inform product improvements and marketing strategies.
        \end{itemize}
        
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item The choice of metrics should align with the specific goals of the application.
            \item Evaluation should consider trade-offs between different metrics (e.g., precision vs. recall).
            \item Real-world implications significantly influence decision-making in business and healthcare.
        \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Metrics and Formulas}
    \begin{block}{Formulas:}
        \begin{equation}
            \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        Where:
        \begin{itemize}
            \item $TP$ = True Positives
            \item $TN$ = True Negatives
            \item $FP$ = False Positives
            \item $FN$ = False Negatives
        \end{itemize}
        
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    These practical applications demonstrate how critical metrics not only guide the development and refinement of AI algorithms but also shape their impact in real-world scenarios. By selecting and analyzing the right metrics, stakeholders can ensure that AI solutions are both effective and beneficial.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Introduction}
    \begin{block}{Recap of the Importance of Selecting Appropriate Metrics}
        Choosing the right metrics for evaluating AI algorithms is essential for several reasons:
    \end{block}
    \begin{itemize}
        \item \textbf{Performance Assessment}: Metrics provide a quantifiable means to assess how well an algorithm performs.
        \item \textbf{Decision-Making}: They guide stakeholders in decision-making processes, influencing the deployment and improvement of models.
        \item \textbf{Comparative Analysis}: Metrics allow comparisons between different models and approaches, facilitating the selection of the best-performing option.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Metrics for Evaluation}
    \begin{block}{Critical Metrics Highlighted}
        Here are some key metrics for evaluating AI algorithms:
    \end{block}
    \begin{itemize}
        \item \textbf{Accuracy}:
        \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Samples}}
        \end{equation}
        
        \item \textbf{Precision} and \textbf{Recall}:
        \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
        \end{equation}
        \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
        \end{equation}
        
        \item \textbf{F1 Score}:
        \begin{equation}
            \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
        \end{equation}
        
        \item \textbf{AUC-ROC}: Evaluates the trade-off between sensitivity and specificity.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Impact and Selection of Metrics}
    \begin{block}{Impact on Decision-Making}
        Metrics directly influence the success of AI deployments:
    \end{block}
    \begin{itemize}
        \item \textbf{Business Alignment}: Metrics should align with business objectives.
        \item \textbf{Model Iteration}: Continuous monitoring enables iterative improvements.
    \end{itemize}
    
    \begin{block}{Selecting Metrics Wisely}
        Emphasize understanding the problem domain:
    \end{block}
    \begin{itemize}
        \item \textbf{Contextual Relevance}: Relate metrics to specific problem or goal.
        \item \textbf{Stakeholder Communication}: Clear communication fosters trust and transparency.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Selecting appropriate metrics is crucial for informed decision-making and improved outcomes in AI.
    \end{block}
\end{frame}


\end{document}