\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 3: Implementing ETL Pipelines]{Week 3: Implementing ETL Pipelines}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to ETL Pipelines}
    \begin{block}{Overview of ETL}
        ETL stands for **Extraction, Transformation, and Loading**. It is a fundamental process used in data warehousing and data integration.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is ETL?}
    \begin{itemize}
        \item **Extraction**: Retrieves data from various sources like databases, spreadsheets, APIs, and cloud storage.
        \item **Transformation**: Cleans and enriches the data, making it suitable for analysis.
        \item **Loading**: Involves loading the transformed data into a target system for reporting and analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of ETL}
    \begin{enumerate}
        \item \textbf{Extraction}
            \begin{itemize}
                \item Retrieves data from various source systems.
                \item \textit{Example}: Extracting sales data from a database.
            \end{itemize}
        \item \textbf{Transformation}
            \begin{itemize}
                \item Cleans and formats data for analysis.
                \item \textit{Common Transformations}:
                \begin{itemize}
                    \item Data Cleaning
                    \item Data Aggregation
                    \item Data Merging
                \end{itemize}
            \end{itemize}
        \item \textbf{Loading}
            \begin{itemize}
                \item Loads transformed data into a target system like a data warehouse.
                \item \textit{Example}: Loading data into Amazon Redshift.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of ETL in Data Processing}
    \begin{itemize}
        \item **Data Integration**: Unifies data from disparate sources for improved decision-making.
        \item **Quality Assurance**: Ensures data accuracy and consistency.
        \item **Performance Enhancement**: Optimizes data retrieval, reducing query times.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    ETL is vital for:
    \begin{itemize}
        \item Transforming raw data into meaningful insights.
        \item Maintaining data integrity and quality.
        \item Automating processes to keep data up to date.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example ETL Pipeline Flow}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{etl_pipeline_flow.png}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet Example}
    Here’s a simple ETL process in Python using Pandas:
    \begin{lstlisting}[language=Python]
import pandas as pd

# Extraction
sales_data = pd.read_csv('sales_data.csv')
customer_data = pd.read_csv('customer_data.csv')

# Transformation
sales_data.drop_duplicates(inplace=True)
merged_data = pd.merge(sales_data, customer_data, on='customer_id')

# Loading
merged_data.to_csv('merged_data.csv', index=False)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    ETL pipelines are essential for effective data management and analytics. Understanding ETL will enable you to develop solutions for informed decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Objectives for Week 3}
    \begin{block}{Overview}
        This week, we will develop foundational skills for building a basic ETL (Extraction, Transformation, Loading) pipeline using Python and Pandas. 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 1}
    By the end of the week, you should be able to:
    \begin{enumerate}
        \item \textbf{Understand the ETL Process:}
        \begin{itemize}
            \item Define ETL and explain its significance.
            \item Identify the three components of ETL:
            \begin{itemize}
                \item \textbf{Extraction:} Retrieve data from sources.
                \item \textbf{Transformation:} Clean and prepare data for analysis.
                \item \textbf{Loading:} Insert data into a target location.
            \end{itemize}
        \end{itemize}

        \item \textbf{Set Up Your Environment:}
        \begin{itemize}
            \item Install Python and libraries (Pandas, NumPy).
            \item Set up IDE (Jupyter Notebook or PyCharm).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 2}
    Continuing from last frame:
    \begin{enumerate}[resume]
        \item \textbf{Develop Basic ETL Pipeline:}
        \begin{itemize}
            \item \textbf{Extraction Phase:}
            \begin{lstlisting}[language=Python]
import pandas as pd
data = pd.read_csv('data.csv')
            \end{lstlisting}
            \item \textbf{Transformation Phase:}
            \begin{lstlisting}[language=Python]
data.dropna(inplace=True)  # Removing rows with missing values
data = data[data['age'] > 18]  # Filtering to include only adults
            \end{lstlisting}
            \item \textbf{Loading Phase:}
            \begin{lstlisting}[language=Python]
data.to_csv('cleaned_data.csv', index=False)
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Learn Best Practices:}
        \begin{itemize}
            \item Importance of error handling and logging.
            \item How to document code effectively.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding ETL Process - Overview}
    \begin{block}{What is ETL?}
        ETL stands for Extract, Transform, Load. It is a critical process in data warehousing used to:
        \begin{itemize}
            \item Pull data from various sources
            \item Transform it into a suitable format
            \item Load it into a target database
        \end{itemize}
        This process is essential for consolidating data from multiple platforms for analysis, reporting, and decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding ETL Process - Components}
    \begin{block}{Components of ETL}
        \begin{enumerate}
            \item \textbf{Extract}: Harvesting data from various sources (e.g., databases, files).
                \begin{itemize}
                    \item \underline{Example}: Extracting sales data from an SQL database and customer info from a CSV file.
                \end{itemize}
            \item \textbf{Transform}: Cleaning and transforming data for analysis.
                \begin{itemize}
                    \item \underline{Example}: Converting date formats, removing duplicates.
                \end{itemize}
            \item \textbf{Load}: Loading the transformed data into the target system.
                \begin{itemize}
                    \item \underline{Example}: Loading data into a PostgreSQL database for reporting.
                \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding ETL Process - Real-World Example}
    \begin{block}{Real-World Example: Retail Company Data Integration}
        \begin{itemize}
            \item \textbf{Extract}: Collecting sales data from multiple stores in different formats.
            \item \textbf{Transform}: 
                \begin{itemize}
                    \item Normalize sales data currencies.
                    \item Clean customer data.
                    \item Create new fields (e.g., "Sales Category").
                \end{itemize}
            \item \textbf{Load}: Loading data into a cloud-based data warehouse for analysis.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Versatility}: Handles various data types.
            \item \textbf{Scalability}: Accommodates increasing data volumes.
            \item \textbf{Hands-On Tools}: Utilizing Python and Pandas for automation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding ETL Process - Code Snippet}
    \begin{block}{Code Example in Python (Pandas)}
        \begin{lstlisting}[language=Python]
import pandas as pd

# Extract
sales_data = pd.read_csv('sales_data.csv')
customer_data = pd.read_sql('SELECT * FROM customers', con=database_connection)

# Transform
sales_data = sales_data.drop_duplicates().drop(columns=['irrelevant_column'])
customer_data['signup_date'] = pd.to_datetime(customer_data['signup_date'])

# Merge data
combined_data = pd.merge(sales_data, customer_data, on='customer_id')

# Load
combined_data.to_sql('combined_sales_data', con=database_connection, if_exists='replace', index=False)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding ETL Process - Conclusion}
    \begin{block}{Conclusion}
        The ETL process is foundational for effective data management, enabling organizations to leverage data for strategic insights and operational efficiency. Understanding each component of ETL is essential for engagement in data analytics or engineering.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools Required}
    \begin{block}{Overview}
        Overview of required tools and software for setting up ETL pipelines.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Python}
    \begin{block}{Description}
        Python is a versatile programming language widely used for data manipulation, analysis, and automation of ETL tasks. Its readability and extensive libraries make it an ideal choice for data engineers and analysts.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Interpreted Language:} Allows for quick testing and iteration.
        \item \textbf{Easy Syntax:} Ideal for beginners and facilitates rapid development.
        \item \textbf{Library Support:} Offers robust libraries such as Pandas, NumPy, and more.
    \end{itemize} 

    \begin{block}{Installation}
        \begin{itemize}
            \item Go to the \textbf{Python Official Website} and download the latest version (ensure 'Add Python to PATH' is checked).
            \item Verify installation by running \texttt{python --version} in the terminal.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Pandas}
    \begin{block}{Description}
        Pandas is a powerful data manipulation and analysis library for Python. It provides data structures and functions needed to clean, transform, and analyze data.
    \end{block}
    
    \begin{itemize}
        \item \textbf{DataFrame Structure:} Handles tabular data efficiently.
        \item \textbf{Data Cleaning:} Functions to handle missing values, duplicates, etc.
        \item \textbf{Integration:} Works with various data formats (CSV, Excel, SQL, etc.).
    \end{itemize}

    \begin{block}{Installation}
        Once Python is installed, run the following command in the terminal: 
        \begin{lstlisting}[language=bash]
pip install pandas
        \end{lstlisting}
        Verify installation by running: 
        \begin{lstlisting}[language=python]
import pandas as pd
print(pd.__version__)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Usage of Pandas in ETL}
    Here’s a simple illustration of how to use Pandas in an ETL pipeline:
    
    \begin{lstlisting}[language=python]
import pandas as pd

# Extract
data = pd.read_csv('data_source.csv')

# Transform
data['date'] = pd.to_datetime(data['date'])  # Changing data type
data.dropna(inplace=True)  # Removing missing values

# Load
data.to_sql('table_name', con=database_connection, if_exists='replace')
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Tools to Consider}
    \begin{itemize}
        \item \textbf{Apache Airflow:} For scheduling and monitoring workflows.
        \item \textbf{SQLAlchemy:} For database connection in Python.
        \item \textbf{Jupyter Notebook:} For interactive data exploration and documentation.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Setting up ETL pipelines requires the right tools. Python and Pandas are central to this process, enabling data extraction, transformation, and loading efficiently.
    \end{block}

    \begin{block}{Next Steps}
        Continue to the next slide for a step-by-step installation and configuration guide of Python and Pandas!
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Installation and Setup}
    \begin{block}{Objective}
        By the end of this slide, you will be able to successfully install Python and the Pandas library to set up your ETL pipeline environment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 1: Installing Python}
    Python is a versatile programming language widely used for data analysis and ETL processes. Follow these steps to install Python:
    \begin{enumerate}
        \item \textbf{Download Python}
        \begin{itemize}
            \item Visit the official Python website: \url{https://www.python.org/downloads/}
            \item Choose the correct version for your operating system (Windows, macOS, or Linux).
        \end{itemize}
        
        \item \textbf{Run the Installer}
        \begin{itemize}
            \item \textbf{Windows}: Double-click the downloaded \texttt{.exe} file. 
            \begin{itemize}
                \item Important: Check the box that says "Add Python to PATH" during installation.
            \end{itemize}
            \item \textbf{macOS}: Open the \texttt{.pkg} file and follow the prompts.
            \item \textbf{Linux}: Install using package management via terminal. For Ubuntu, use:
            \begin{lstlisting}
sudo apt install python3
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Verify Installation}
        \begin{itemize}
            \item Open your terminal or command prompt and type:
            \begin{lstlisting}
python --version
            \end{lstlisting}
            \item This should display the installed version of Python.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 2: Installing Pandas}
    Pandas is a powerful data manipulation and analysis library for Python. Here’s how to install it:
    
    \begin{enumerate}
        \item \textbf{Upgrade Pip}
        \begin{itemize}
            \item Ensure you have the latest version of pip:
            \begin{lstlisting}
python -m pip install --upgrade pip
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Install Pandas}
        \begin{itemize}
            \item In your terminal or command prompt, type:
            \begin{lstlisting}
pip install pandas
            \end{lstlisting}
            \item This command will download and install the Pandas library along with its dependencies.
        \end{itemize}

        \item \textbf{Verify Pandas Installation}
        \begin{itemize}
            \item Open a Python shell by typing \texttt{python} in your terminal.
            \item Import Pandas and check the version:
            \begin{lstlisting}
import pandas as pd
print(pd.__version__)
            \end{lstlisting}
            \item You should see the version number of Pandas, confirming it’s installed correctly.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step 3: Setting Up Your Development Environment}
    To effectively work with Python and Pandas, consider setting up a development environment:
    
    \begin{enumerate}
        \item \textbf{Use an IDE}
        \begin{itemize}
            \item \textbf{Anaconda}: Comes pre-installed with Python, Pandas, and Jupyter Notebook.
            \begin{itemize}
                \item Download from: \url{https://www.anaconda.com/products/distribution#download-section}
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Use Jupyter Notebook}
        \begin{itemize}
            \item After installing Anaconda, launch Jupyter Lab/Notebook to create notebooks for your ETL workflow.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Always ensure Python is added to your system PATH to run it from any terminal.
            \item Regularly update your libraries with pip to access the latest features and security updates.
            \item Jupyter notebooks are an excellent tool for testing ETL scripts in an interactive format.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Python Code Snippet}
        \begin{lstlisting}
import pandas as pd

# Example DataFrame creation
data = {
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35]
}
df = pd.DataFrame(data)

print(df)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating an ETL Pipeline}
    \begin{block}{Introduction to ETL Pipeline}
        ETL stands for Extract, Transform, Load – a process vital for data integration that enables organizations to consolidate data from various sources for analysis.
        In this demonstration, we will build a basic ETL pipeline using Python and the Pandas library.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Components of ETL}
    \begin{enumerate}
        \item \textbf{Extract:} Retrieve data from different sources.
        \item \textbf{Transform:} Clean, format, and enrich the data.
        \item \textbf{Load:} Store the transformed data into a target destination (e.g., databases, data warehouses).
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Why Use Python and Pandas?}
    \begin{itemize}
        \item Python is user-friendly and has vast libraries tailored for data manipulation.
        \item Pandas provides efficient data structures like DataFrames, which make data analysis straightforward.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example ETL Pipeline in Python}
    \begin{block}{Setting Up}
        Ensure you have Pandas installed. You can install it using:
        \begin{lstlisting}
pip install pandas
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example ETL Pipeline Code Snippet}
    \begin{lstlisting}[language=Python]
import pandas as pd

# 1. Extract
def extract(file_path):
    data = pd.read_csv(file_path)
    return data

# 2. Transform
def transform(data):
    # Clean data: Remove rows with missing values
    data = data.dropna()
    
    # Convert date format
    data['date'] = pd.to_datetime(data['date'])
    
    # Add a new column
    data['sales_tax'] = data['amount'] * 0.07  # Assuming a 7% sales tax
    return data

# 3. Load
def load(data, target_path):
    data.to_csv(target_path, index=False)

# Main ETL Process
def main():
    file_path = 'data/input_sales.csv'         # Input file path
    target_path = 'data/transformed_sales.csv' # Transformed output file path
    
    # Execute ETL Process
    data = extract(file_path)
    transformed_data = transform(data)
    load(transformed_data, target_path)
    print("ETL Process Completed Successfully!")

# Run the ETL pipeline
if __name__ == "__main__":
    main()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item ETL is essential for effective data analysis and business intelligence.
        \item Ensure quality data through the transformation phase to improve analytical outcomes.
        \item Python's Pandas library makes ETL tasks accessible and efficient.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Next Steps}
    \begin{block}{Conclusion}
        The ability to create and manage ETL pipelines is a crucial skill for data professionals. 
        By using Python and Pandas, you can automate data processes, improve accuracy, and focus on deriving insights from your data.
    \end{block}
    
    \begin{block}{Next Steps}
        In the upcoming slide, we will delve into different data extraction techniques to broaden our understanding of the sources we can utilize for our ETL pipeline.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Data Extraction Techniques}
    \begin{block}{Overview}
        Data extraction is a critical first step in the ETL (Extract, Transform, Load) pipeline process. It involves retrieving data from various sources to prepare it for transformation and loading into a destination system.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Data Extraction Techniques}
    \begin{enumerate}
        \item \textbf{Database Extraction} 
        \begin{itemize}
            \item \textit{Description}: Pulling data directly from relational databases using SQL.
            \item \textit{Example}:
            \begin{lstlisting}[language=SQL]
            SELECT * FROM customers WHERE purchase_date >= '2023-01-01';
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Web Scraping} 
        \begin{itemize}
            \item \textit{Description}: Automating the retrieval of data from websites.
            \item \textit{Tools}: Beautiful Soup, Scrapy (Python libraries).
            \item \textit{Example}:
            \begin{lstlisting}[language=Python]
            import requests
            from bs4 import BeautifulSoup

            response = requests.get('https://example.com')
            soup = BeautifulSoup(response.text, 'html.parser')
            data = soup.find_all('h2')  # Extracts all headings from the webpage.
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Data Extraction Techniques (Cont'd)}
    \begin{enumerate} \setcounter{enumi}{2}
        \item \textbf{API Extraction}
        \begin{itemize}
            \item \textit{Description}: Using APIs to access structured data from external services.
            \item \textit{Example}:
            \begin{lstlisting}[language=Python]
            import requests

            response = requests.get('https://api.weatherapi.com/v1/current.json?key=YOUR_API_KEY&q=London')
            weather_data = response.json()
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Flat File Extraction} 
        \begin{itemize}
            \item \textit{Description}: Reading data from flat files like CSV, JSON, or XML.
            \item \textit{Example}:
            \begin{lstlisting}[language=Python]
            import pandas as pd

            df = pd.read_csv('data.csv')  # Loads data from a CSV file into a DataFrame.
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Log File Extraction}
        \begin{itemize}
            \item \textit{Description}: Extracting data from log files generated by applications.
            \item \textit{Use Case}: Analyzing user behavior or system performance.
            \item \textit{Example}: Using Python to parse a web server log file.
        \end{itemize}

    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{block}{Important Considerations}
        \begin{itemize}
            \item \textbf{Data Quality}: Ensure the accuracy and completeness of the extracted data.
            \item \textbf{Performance}: Choose efficient extraction techniques for large datasets; consider batch vs. real-time extraction.
            \item \textbf{Compliance}: Be aware of regulations like GDPR or HIPAA when extracting sensitive information.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Practical Considerations}
    \begin{block}{Implementation Tips}
        \begin{itemize}
            \item Consider the volume and velocity of the data being extracted.
            \item Set up error handling and monitoring for integrity in the extraction process.
            \item Document all extraction processes for transparency and reproducibility.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Introduction to Data Transformation}
    Data transformation is a crucial step in the ETL (Extract, Transform, Load) process. This phase involves converting raw data into a format that is more suitable for analysis and decision-making. The transformation can include a variety of processes, such as:
    
    \begin{itemize}
        \item \textbf{Cleaning}: Removing invalid or corrupt data.
        \item \textbf{Normalization}: Standardizing data into a common format.
        \item \textbf{Aggregation}: Summarizing data, such as calculating averages or totals.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Transformation Techniques Using Pandas}
    Pandas is a powerful data manipulation library in Python that simplifies the transformation process. Here are some key techniques:
    
    \begin{enumerate}
        \item \textbf{Data Cleaning}
          \begin{block}{Handling Missing Values}
              \begin{lstlisting}[language=Python]
df.fillna(value=0)  # Replaces NaN values with 0
df.dropna()         # Removes rows with any NaN values
              \end{lstlisting}
          \end{block}
          
        \item \textbf{Data Type Conversion}
          \begin{block}{Change Data Types for Better Analysis}
              \begin{lstlisting}[language=Python]
df['date_column'] = pd.to_datetime(df['date_column'])  # Convert to datetime
df['numeric_column'] = df['numeric_column'].astype(float)  # Convert to float
              \end{lstlisting}
          \end{block}
          
        \item \textbf{Filtering Data}
          \begin{block}{Extract Specific Rows}
              \begin{lstlisting}[language=Python]
filtered_df = df[df['column_name'] > 100]  # Filter rows where column_name > 100
              \end{lstlisting}
          \end{block}
          
        \item \textbf{Creating New Columns}
          \begin{block}{Deriving New Variables}
              \begin{lstlisting}[language=Python]
df['new_column'] = df['column1'] + df['column2']  # Creates new column as sum of two existing columns
              \end{lstlisting}
          \end{block}
          
        \item \textbf{Aggregation Functions}
          \begin{block}{Summarizing Data for Analysis}
              \begin{lstlisting}[language=Python]
aggregated_df = df.groupby('category_column').agg({'value_column': 'sum'})  # Summing values
              \end{lstlisting}
          \end{block}
          
        \item \textbf{Joining DataFrames}
          \begin{block}{Merging Different Datasets}
              \begin{lstlisting}[language=Python]
merged_df = pd.merge(df1, df2, on='key_column')  # Merge based on common key
              \end{lstlisting}
          \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example Transformation Workflow}
    \begin{enumerate}
        \item \textbf{Start with Raw Data}: 
          Imagine you have a sales dataset containing multiple columns including sales amount, date, and product category.

        \item \textbf{Clean the Data}: 
          Remove any erroneous entries or outliers. Use \texttt{df.dropna()} to discard rows with missing information.

        \item \textbf{Transform the Data}: 
          Convert the 'date' column into a datetime format for better filtering later, and create a new column for 'sales tax' as \(0.1 \times \text{sales\_amount}\).

        \item \textbf{Summarize}: 
          Group the data by 'product category' and use the \texttt{sum} function to calculate total sales per category.

        \item \textbf{Output the Transformed Dataframe}: 
          Save or load the clean and transformed data to a CSV or a database for further analysis.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{Importance of Transformation}: Proper transformation ensures data integrity and enhances analysis quality.
        \item \textbf{Use of Pandas}: Leverage Pandas for efficient data manipulation; it offers powerful tools for cleaning, modifying, and analyzing data.
        \item \textbf{Workflow Structure}: Always visualize the data workflow from raw to transformed to avoid potential pitfalls in analysis.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loading Data into Destination - Overview}
    \begin{block}{Overview}
        Loading data into a destination involves moving the processed (transformed) data into a final storage system, such as a data warehouse or database. 
        This step ensures that the data is available for querying, reporting, and analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loading Data into Destination - Key Concepts}
    \begin{itemize}
        \item \textbf{Destination Types}:
            \begin{itemize}
                \item \textbf{Data Warehouses}: Optimized for analytical querying and reporting. \\
                      Examples: Amazon Redshift, Google BigQuery.
                \item \textbf{Databases}: More flexible for transactional applications. \\
                      Examples: MySQL, PostgreSQL.
            \end{itemize}
        
        \item \textbf{Loading Methods}:
            \begin{itemize}
                \item \textbf{Batch Loading}: Data is transferred in bulk at scheduled intervals.
                \item \textbf{Real-time Loading (Streaming)}: Data is loaded continuously as it becomes available.
            \end{itemize}

        \item \textbf{Loading Techniques}:
            \begin{itemize}
                \item SQL INSERT Statements
                \item Bulk Load Utilities (e.g., COPY in PostgreSQL)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loading Data into Destination - Steps}
    \begin{enumerate}
        \item \textbf{Establish Connection}:
            \begin{lstlisting}[language=Python]
from sqlalchemy import create_engine
engine = create_engine('postgresql://username:password@localhost/mydatabase')
            \end{lstlisting}

        \item \textbf{Prepare DataFrame}:
            \begin{lstlisting}[language=Python]
import pandas as pd
df = pd.DataFrame({
    'column1': [1, 2, 3],
    'column2': ['A', 'B', 'C']
})
            \end{lstlisting}

        \item \textbf{Load Data}:
            \begin{lstlisting}[language=Python]
df.to_sql('my_table', con=engine, if_exists='append', index=False)
            \end{lstlisting}
            This appends the DataFrame \texttt{df} to the table \texttt{my\_table} in PostgreSQL.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loading Data into Destination - Best Practices}
    \begin{itemize}
        \item \textbf{Monitor Load Performance}: Track load times and success rates to identify bottlenecks.
        \item \textbf{Transaction Management}: Implement error handling and rollback mechanisms to maintain data integrity.
        \item \textbf{Document Schema Changes}: Keep track of any changes in the destination schema affecting the loading process.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loading Data into Destination - Conclusion}
    Loading transformed data into the destination is a crucial phase of the ETL pipeline. 
    Understanding the destination types and their loading methods can significantly improve data accessibility and usability across analytics processes.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Error Handling and Debugging}
    \begin{block}{Overview}
        Mechanisms for managing errors within the ETL pipeline and debugging techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Error Handling in ETL Pipelines}
    \begin{itemize}
        \item Error handling in ETL refers to strategies to anticipate, detect, and manage errors during data processing.
        \item Errors can arise from:
        \begin{itemize}
            \item Data quality issues
            \item Connectivity problems
            \item Transformation logic failures
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Mechanisms for Error Handling}
    \begin{enumerate}
        \item \textbf{Validation Checks:}
        \begin{itemize}
            \item Implement validation rules to check data integrity.
            \item \textit{Example:} Verify email formats before loading data.
        \end{itemize}
        
        \item \textbf{Error Log Creation:}
        \begin{itemize}
            \item Maintain detailed error logs including timestamps and error types.
            \item \textit{Example:} `ERROR [2023-10-02 10:25]: Invalid data type in column 'Age' for record ID 12345`.
        \end{itemize}

        \item \textbf{Notification Systems:}
        \begin{itemize}
            \item Automated alerts to inform developers of critical errors.
            \item \textit{Example:} Notify via email or Slack on ETL job failures.
        \end{itemize}

        \item \textbf{Retry Logic:}
        \begin{itemize}
            \item Mechanism for transient failures to reattempt tasks.
            \item \textit{Example:} Automatically retry three times after a network timeout.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Debugging Techniques}
    \begin{enumerate}
        \item \textbf{Step-by-Step Execution:}
        \begin{itemize}
            \item Incremental execution to isolate error sources.
            \item \textit{Example:} Test extraction step before transformation.
        \end{itemize}
        
        \item \textbf{Data Profiling:}
        \begin{itemize}
            \item Analyze input data for anomalies and quality issues.
            \item \textit{Example:} Use Pandas in Python to check for null values.
        \end{itemize}

        \item \textbf{Version Control:}
        \begin{itemize}
            \item Track changes in ETL scripts to facilitate rollback.
            \item \textit{Example:} Revert to a previous commit in Git if a bug is introduced.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item Validate data before processing.
            \item Maintain comprehensive logs for troubleshooting.
            \item Implement robust notifications for quick error responses.
            \item Utilize incremental execution and version control as debugging methods.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By integrating these error handling mechanisms and debugging techniques into ETL workflows, the reliability and efficiency of data processing can be significantly enhanced, ensuring accurate data delivery to stakeholders.
\end{frame}

\begin{frame}
    \frametitle{Testing the ETL Pipeline}
    \begin{block}{Overview}
        Methods for testing the ETL pipeline to ensure it functions as expected.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Importance of ETL Testing}
    
    Testing an ETL (Extract, Transform, Load) pipeline is crucial to ensure that the data processes work as intended. The primary goals are to:
    \begin{itemize}
        \item Identify issues
        \item Validate data integrity
        \item Verify accurate and complete data delivery into target systems
    \end{itemize}
    
    \begin{block}{Key Reasons for Testing}
        \begin{itemize}
            \item \textbf{Data Integrity:} Confirm accurate extraction, transformation, and loading without loss.
            \item \textbf{Performance Verification:} Ensure the process runs efficiently.
            \item \textbf{Error Detection:} Identify and rectify issues early to prevent downstream problems.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods for Testing ETL Pipelines}
    
    \begin{enumerate}
        \item \textbf{Unit Testing:}
        \begin{itemize}
            \item Focuses on individual components.
            \item Tests specific functions or scripts.
            \item \textit{Example:} Testing transformation logic for date formats.
        \end{itemize}
        
        \item \textbf{Integration Testing:}
        \begin{itemize}
            \item Validates interactions between components.
            \item Ensures correct processing from one stage to another.
            \item \textit{Example:} Verify that transformed data loads into the target database.
        \end{itemize}
        
        \item \textbf{End-to-End Testing:}
        \begin{itemize}
            \item Tests the entire pipeline workflow.
            \item Validates complete operation from start to finish.
            \item \textit{Example:} Simulating a complete run and verifying output.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Methods for Testing ETL Pipelines (continued)}
    
    \begin{enumerate}
        \setcounter{enumi}{3}
        
        \item \textbf{Data Quality Testing:}
        \begin{itemize}
            \item Validates data quality and accuracy.
            \item Checks for duplicates, null values, and business rule adherence.
            \item \textit{Example:} Ensure customer IDs are unique.
        \end{itemize}
        
        \item \textbf{Performance Testing:}
        \begin{itemize}
            \item Assesses pipeline performance under varying data loads.
            \item Evaluates speed, resource utilization, and scalability.
            \item \textit{Example:} Measuring performance with different data volumes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Example Test Scenario}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Establish a comprehensive test set covering all aspects of the ETL pipeline.
            \item Automate testing routines for efficiency.
            \item Regularly perform tests, especially after updates.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Test Scenario}
    \begin{lstlisting}[language=Python]
def transform_date_format(date_string):
    from datetime import datetime
    return datetime.strptime(date_string, '%m-%d-%Y').strftime('%Y-%m-%d')

def test_transform_date_format():
    assert transform_date_format('12-31-2023') == '2023-12-31', "Test Failed"
    assert transform_date_format('01-01-2023') == '2023-01-01', "Test Failed"
    
test_transform_date_format()  # Should pass silently if all assertions are true
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    
    Testing is vital for ensuring a smooth-running ETL pipeline. A systematic testing strategy is crucial for delivering high-quality data solutions. Effective testing can save time, costs, and headaches in the future.
\end{frame}

\begin{frame}
    \frametitle{Best Practices for ETL Pipelines}
    \begin{block}{Introduction to ETL}
        ETL (Extract, Transform, Load) pipelines are essential processes in data warehousing that allow organizations to consolidate data from multiple sources. Designing an efficient ETL pipeline is crucial for ensuring data integrity, performance, and ease of use.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for ETL Pipelines - Part 1}
    \begin{enumerate}
        \item \textbf{Define Clear Objectives}
        \begin{itemize}
            \item Start with a clear understanding of data needs and business requirements.
            \item Example: If the goal is to analyze sales data quarterly, design the ETL to refresh daily while maintaining monthly aggregates.
        \end{itemize}

        \item \textbf{Use Incremental Loads}
        \begin{itemize}
            \item Load only changed or new records instead of the entire dataset.
            \item Example: Implementing change data capture (CDC) methods helps minimize processing time and system load.
        \end{itemize}

        \item \textbf{Data Quality Checks}
        \begin{itemize}
            \item Validate and clean data during the transformation phase.
            \item Techniques include duplicate detection and format validation (e.g., ensuring email addresses match regex patterns).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for ETL Pipelines - Part 2}
    \begin{enumerate}[resume]
        \item \textbf{Logging and Monitoring}
        \begin{itemize}
            \item Incorporate logging mechanisms to capture errors and monitor performance continuously.
            \item Set up alerts for failures or performance bottlenecks.
        \end{itemize}

        \item \textbf{Optimize Performance}
        \begin{itemize}
            \item Use efficient algorithms and consider parallel processing.
            \item Example Code Snippet (SQL-based):
            \begin{lstlisting}
            INSERT INTO final_table 
            SELECT * 
            FROM staging_table 
            WHERE condition = TRUE;
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Maintain Documentation}
        \begin{itemize}
            \item Create documentation to describe data sources, transformation rules, and load processes.
            \item Develop a data dictionary to ensure clarity.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Best Practices for ETL Pipelines - Part 3}
    \begin{enumerate}[resume]
        \item \textbf{Version Control}
        \begin{itemize}
            \item Utilize version control for ETL code to track changes and maintain history.
            \item Example: Use Git to manage scripts, making collaboration easier.
        \end{itemize}

        \item \textbf{Robust Testing Strategies}
        \begin{itemize}
            \item Conduct thorough testing across various scenarios before deploying.
            \item Techniques include unit testing, integration testing, and performance testing.
        \end{itemize}
    \end{enumerate}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Efficient ETL processes enhance data availability and consistency.
            \item Regularly revisit ETL design to adapt to evolving business needs.
            \item Collaboration across teams ensures effective ETL implementation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    By following these best practices when designing and implementing ETL pipelines, organizations can ensure efficient data processing, improve data quality, and meet business objectives effectively.

    \begin{block}{Next Steps}
        Explore ethical considerations, relevant regulations, and frameworks like GDPR and HIPAA to ensure compliance in data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - Introduction}
    \begin{itemize}
        \item Ethical frameworks are vital for responsible data processing.
        \item Key regulations include:
        \begin{itemize}
            \item General Data Protection Regulation (GDPR)
            \item Health Insurance Portability and Accountability Act (HIPAA)
        \end{itemize}
        \item These frameworks ensure respect for individual rights and societal norms in ETL pipelines.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - GDPR}
    \begin{block}{Overview}
        GDPR is a comprehensive regulation protecting personal data within the EU, effective since May 25, 2018.
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Key Provisions:}
        \begin{itemize}
            \item \textbf{Consent:} Explicit consent required before data collection.
            \item \textbf{Right to Access:} Individuals can access their data.
            \item \textbf{Data Minimization:} Collect only necessary data.
        \end{itemize}

        \item \textbf{Penalties for Non-compliance:}
        \begin{itemize}
            \item Fines up to €20 million or 4\% of global turnover, whichever is higher.
        \end{itemize}

        \item \textbf{ETL Example:} 
        \begin{itemize}
            \item Anonymize data and secure user consent in ETL processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Processing - HIPAA}
    \begin{block}{Overview}
        HIPAA provides privacy standards for protecting patients' medical records and health information in the U.S.
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Key Provisions:}
        \begin{itemize}
            \item \textbf{Privacy Rule:} Standards for protecting medical records and personal health information.
            \item \textbf{Security Rule:} Safeguards for the confidentiality and security of electronic PHI.
        \end{itemize}

        \item \textbf{Penalties for Non-compliance:}
        \begin{itemize}
            \item Fines range from $100 to $50,000 depending on negligence.
            \item Annual caps are also applicable for certain violations.
        \end{itemize}

        \item \textbf{ETL Example:}
        \begin{itemize}
            \item Implement access controls and encryption for healthcare data in ETL processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Overview of ETL Pipelines}
    \begin{block}{Definition}
        ETL stands for Extract, Transform, Load. This process is essential in data warehousing and analytics, allowing organizations to consolidate data from multiple sources for analysis.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Extract:} Pulling data from various sources (e.g., databases, APIs, files).
        \item \textbf{Transform:} Cleaning and normalizing data to meet business needs.
        \item \textbf{Load:} Loading transformed data into a target database or data warehouse.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Detailed Steps}
    \begin{enumerate}
        \item \textbf{Extract}
        \begin{itemize}
            \item Example: Extracting user data from a CRM system and sales transactions from a financial database.
        \end{itemize}
        
        \item \textbf{Transform}
        \begin{itemize}
            \item Example: Converting date formats from "MM/DD/YYYY" to "YYYY-MM-DD" or calculating total sales per customer.
        \end{itemize}
        
        \item \textbf{Load}
        \begin{itemize}
            \item Example: Loading processed sales and user data into a centralized data warehouse for business intelligence tools.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Best Practices}
    \begin{itemize}
        \item \textbf{Data Quality:} The success hinges on data quality.
        \item \textbf{Automation:} Enhance efficiency using tools like Apache NiFi, Talend, or Python scripts.
        \item \textbf{Error Handling:} Implement error handling and logging mechanisms.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Mastery of ETL pipelines enables organizations to make data-driven decisions and derive meaningful insights. Always consider automation and data quality best practices.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Takeaways - Practical Code Snippet}
    Here’s a simple Python code snippet using the Pandas library to illustrate the transformation phase:
    
    \begin{lstlisting}[language=Python]
import pandas as pd

# Extract
data = pd.read_csv('sales_data.csv')

# Transform
data['Order_Date'] = pd.to_datetime(data['Order_Date']).dt.strftime('%Y-%m-%d')
data['Total_Sales'] = data['Quantity'] * data['Price']

# Load (Example loading into a SQL database)
from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:')
data.to_sql('sales_summary', engine, index=False)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session}
    Open floor for questions and clarifications regarding the topics discussed.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    \begin{itemize}
        \item The Q\&A session is essential for clarifying concepts and fostering discussions about ETL (Extract, Transform, Load) pipelines.
        \item Engaging with peers enhances understanding and aids in troubleshooting common challenges.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts to Clarify}
    \begin{enumerate}
        \item \textbf{ETL Pipeline Components}:
        \begin{itemize}
            \item \textit{Extract}: Gathering data from sources (e.g., databases, APIs).
            \item \textit{Transform}: Cleaning and processing data (e.g., normalization, aggregation).
            \item \textit{Load}: Inserting data into a destination, such as a data warehouse.
        \end{itemize}
        
        \item \textbf{Importance of ETL}:
        \begin{itemize}
            \item Enables informed decisions based on accurate data.
            \item Integrates data from disparate sources for analysis.
        \end{itemize}

        \item \textbf{Common Challenges}:
        \begin{itemize}
            \item Data quality issues: Managing missing or inconsistent data.
            \item Performance bottlenecks: Optimizing extraction and loading processes.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Questions to Encourage Discussion}
    \begin{itemize}
        \item Can anyone share their experience or challenges faced during the transformation phase?
        \item How do we ensure data quality during ETL processes? What techniques have been effective?
        \item What tools have you used for building ETL pipelines? Discuss their strengths and weaknesses.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples to Illustrate Concepts}
    \begin{itemize}
        \item \textbf{Real-World Example of ETL}:
        \begin{itemize}
            \item A retail company extracts sales data from POS systems, transforms it by adding customer demographics and trends, and loads it into a data warehouse for analysis.
        \end{itemize}
        \item \textbf{ETL Tools}:
        \begin{itemize}
            \item \textit{Apache Nifi}: Streamlines data flows with visual programming.
            \item \textit{Talend}: Various connectors for different data sources.
            \item \textit{Apache Airflow}: Manages complex workflows in ETL.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engage with the Audience}
    \begin{itemize}
        \item Share any ETL projects you are working on or planning to start.
        \item What aspects of ETL are you most interested in learning about or need more practice with?
    \end{itemize}
    \begin{block}{Conclusion}
        Actively participating in this Q\&A will sharpen your understanding of ETL pipelines and enhance collaboration skills in data operations.
    \end{block}
\end{frame}


\end{document}