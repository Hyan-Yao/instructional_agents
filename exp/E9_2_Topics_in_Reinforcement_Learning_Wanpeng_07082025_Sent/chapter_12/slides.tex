\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Title Page Information
\title[Week 12: Deep Reinforcement Learning]{Week 12: Deep Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Deep Reinforcement Learning - Overview}
    \begin{itemize}
        \item \textbf{What is Deep Reinforcement Learning (DRL)?}
        \begin{itemize}
            \item DRL combines \textbf{Reinforcement Learning (RL)} and \textbf{Deep Learning (DL)}.
            \item Involves an agent making decisions by interacting with an environment.
        \end{itemize}
        \item \textbf{Key Concepts:}
        \begin{itemize}
            \item \textbf{Agent}: Decision-maker (e.g., robot).
            \item \textbf{Environment}: Context or setting (e.g., maze).
            \item \textbf{State (s)}: Current situation representation.
            \item \textbf{Action (a)}: Choices available to the agent.
            \item \textbf{Reward (r)}: Feedback based on action taken.
            \item \textbf{Policy (π)}: Strategy for decision-making.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Deep Reinforcement Learning}
    \begin{enumerate}
        \item \textbf{Advanced Decision-Making:}
        \begin{itemize}
            \item Enables complex decisions in high-dimensional input environments (e.g., images, real-time data).
        \end{itemize}
        \item \textbf{Robustness:}
        \begin{itemize}
            \item Adaptability to varying circumstances and learning from both successes and failures.
        \end{itemize}
        \item \textbf{Applications:}
        \begin{itemize}
            \item \textbf{Gaming:} AlphaGo, Dota 2 bot.
            \item \textbf{Robotics:} Learning tasks like navigation.
            \item \textbf{Finance:} Algorithmic trading strategies.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example and Key Points}
    \begin{block}{Example: Training an AI to Play Video Games}
        \begin{itemize}
            \item Environment: Game world.
            \item Actions: Move left, jump, shoot.
            \item Rewards: Points for completing tasks.
            \item Result: Improved policy over iterations to maximize rewards.
        \end{itemize}
    \end{block}
    
    \begin{itemize}
        \item DRL merges \textbf{decision-making} of RL with \textbf{feature extraction} of DL.
        \item Adaptability and efficiency make DRL crucial in AI advancements.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Overview}
    \begin{block}{Overview}
        The primary learning objectives for this week's exploration of Deep Reinforcement Learning (DRL) are designed to provide you with a comprehensive understanding of key concepts, techniques, and applications. By the end of this week, you should be able to:
    \end{block}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Understanding Fundamentals}
    \begin{enumerate}
        \item \textbf{Understand the Fundamentals of Deep Reinforcement Learning}
            \begin{itemize}
                \item \textbf{Concepts}: Review the key components of reinforcement learning, including agents, environments, states, and actions.
                \item \textbf{Example}: An agent playing a game (like chess) makes decisions to maximize its score, based on the state of the game board.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Key Concepts}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Differentiate between Reinforcement Learning and Other Learning Paradigms}
            \begin{itemize}
                \item \textbf{Contrast with Supervised and Unsupervised Learning}: While supervised learning relies on labeled data, and unsupervised learning finds patterns in unlabeled data, reinforcement learning learns through interactions with the environment.
                \item \textbf{Key Point}: RL is particularly effective in scenarios where decision-making is sequential and the outcomes are partly random.
            \end{itemize}
        
        \item \textbf{Explore Algorithms Used in Deep Reinforcement Learning}
            \begin{itemize}
                \item \textbf{Main Algorithms}: Understand popular DRL algorithms such as:
                    \begin{itemize}
                        \item Q-Learning
                        \item Deep Q-Networks (DQN)
                        \item Policy Gradient Methods
                        \item Actor-Critic Methods
                    \end{itemize}
                \item \textbf{Illustration}: Diagram showing the flow of information in a Q-Learning algorithm where the agent updates the value of actions based on received rewards.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Practical Skills}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Implement Basic DRL Algorithms}
            \begin{itemize}
                \item \textbf{Practical Skills}: Gain hands-on experience by coding your own simple implementation of a DRL algorithm, such as DQN, using Python and libraries like TensorFlow or PyTorch.
                \item \textbf{Code Snippet}:
                \begin{lstlisting}[language=Python]
import numpy as np
import random

# Sample Q-Learning function
def select_action(state):
    if random.uniform(0, 1) < epsilon:
        return random.choice(range(action_size))  # Explore
    else:
        return np.argmax(Q[state])  # Exploit
                \end{lstlisting}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Performance and Ethics}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Analyze and Optimize Performance of DRL Models}
            \begin{itemize}
                \item \textbf{Techniques}: Learn to evaluate the performance of DRL models using metrics such as average reward and convergence speed.
                \item \textbf{Example}: Compare model performance over episodes and adjust hyperparameters to improve results.
            \end{itemize}

        \item \textbf{Identify Real-World Applications of Deep Reinforcement Learning}
            \begin{itemize}
                \item \textbf{Application Areas}: Explore the significant impacts of DRL in various domains such as robotics, game playing (e.g., AlphaGo), personalized recommendations, and resource management.
                \item \textbf{Case Study}: Review a case study where DRL optimizes traffic light control to reduce congestion in urban areas.
            \end{itemize}

        \item \textbf{Discuss Ethical Implications and Challenges}
            \begin{itemize}
                \item \textbf{Ethical Considerations}: Engage in conversations surrounding the ethical implications of deploying DRL systems in sensitive areas, such as autonomous vehicles and healthcare.
                \item \textbf{Challenge Awareness}: Be aware of challenges like exploration-exploitation trade-offs, sample efficiency, and interpretability of deep learning models.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Course Learning Objectives - Conclusion}
    \begin{block}{Conclusion}
        These objectives will guide your learning process as we delve deeper into Deep Reinforcement Learning throughout this week. By achieving these goals, you will acquire both theoretical knowledge and practical skills that are essential for advancing in the field of Artificial Intelligence.
    \end{block}
    
    \begin{block}{Note}
        Be prepared to participate in discussions and hands-on activities that will further reinforce these objectives as we explore the fascinating world of DRL!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Foundational Knowledge - Part 1}
    \textbf{Key Concepts in Reinforcement Learning}

    \begin{enumerate}
        \item \textbf{Markov Decision Processes (MDPs)}
        \item \textbf{Q-Learning}
        \item \textbf{Agents}
        \item \textbf{Environments}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Foundational Knowledge - Part 2}
    \textbf{1. Markov Decision Processes (MDPs)}

    \begin{block}{Definition}
        An MDP provides a mathematical framework for modeling decision-making, where outcomes are partly under the control of a decision-maker (the agent) and partly random.
    \end{block}

    \textbf{Components}:
    \begin{itemize}
        \item \textbf{State (S)}: Represents the environment's condition. (e.g., position of a robot in a grid)
        \item \textbf{Action (A)}: Choices available to the agent. (e.g., move left, right, up, down)
        \item \textbf{Transition Probability (P)}: Likelihood of moving from one state to another given a specific action.
        \item \textbf{Reward (R)}: Immediate feedback received from the environment after taking an action. (e.g., +10 for reaching a goal)
        \item \textbf{Discount Factor ($\gamma$)}: Value (0 $\leq$ $\gamma$ < 1) that determines the importance of future rewards versus immediate rewards.
    \end{itemize}

    \textbf{Illustration}:
    \begin{center}
        Current State ($S_t$) -- Action ($A_t$) $\rightarrow$ Next State ($S_{t+1}$)\\
        \hspace{1cm} | \hspace{1cm} |\\
        \hspace{1cm} Reward ($R_t$) \hspace{1cm} $P(S_{t+1}|S_t, A_t)$
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Foundational Knowledge - Part 3}
    \textbf{2. Q-Learning}

    \begin{block}{Definition}
        A model-free reinforcement learning algorithm that learns the value of an action in a specific state.
    \end{block}
    
    \textbf{Q-Value ($Q(s, a)$)}:
    Represents the expected utility of taking action 'a' in state 's', including the expected future rewards.

    \textbf{Update Rule}:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max(Q(s', a')) - Q(s, a) \right]
    \end{equation}
    Where:
    \begin{itemize}
        \item $\alpha$: Learning rate (0 < $\alpha$ ≤ 1)
        \item $R$: Reward received after taking action 'a' in state 's'
        \item $\max(Q(s', a'))$: Highest Q-value for the next state 's'
    \end{itemize}

    \textbf{Key Points to Emphasize}:
    \begin{itemize}
        \item MDPs form the foundation of reinforcement learning by formalizing the agent-environment interaction.
        \item Q-learning allows agents to learn optimal policies without needing a model of the environment.
        \item Agents and environments' components are crucial for developing effective reinforcement learning systems.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration of Deep Learning and Reinforcement Learning - Overview}
    \begin{block}{Deep Reinforcement Learning (DRL)}
        Deep Reinforcement Learning combines Deep Learning (DL) and Reinforcement Learning (RL) to enable agents to learn complex tasks from high-dimensional sensory inputs (e.g., images, audio).
    \end{block}
    \begin{itemize}
        \item Significant advancements in fields such as:
        \begin{itemize}
            \item Robotics
            \item Video games
            \item Autonomous systems
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration of Deep Learning and Reinforcement Learning - Key Concepts}
    \begin{enumerate}
        \item \textbf{Deep Learning (DL)}:
        \begin{itemize}
            \item Uses neural networks with multiple layers to model complex patterns.
            \item Common architectures: 
            \begin{itemize}
                \item Convolutional Neural Networks (CNNs) for images
                \item Recurrent Neural Networks (RNNs) for sequential data
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Reinforcement Learning (RL)}:
        \begin{itemize}
            \item An agent learns through interactions to maximize cumulative rewards.
            \item Key elements include:
            \begin{itemize}
                \item Agent: Learner or decision maker
                \item Environment: The agent’s interaction context
                \item Actions (A): Choices available to the agent
                \item State (S): Current situation of the environment
                \item Reward (R): Feedback based on agent’s actions
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Integration of Deep Learning and Reinforcement Learning - Convergence}
    \begin{itemize}
        \item \textbf{Function Approximation}: DRL uses neural networks to approximate value functions or policies for complex environments.
        
        \item \textbf{Policy Representation}: Neural networks can represent both the policy and value function, allowing agents to learn directly from raw pixel inputs.
        
        \item \textbf{Example: Playing Atari Games}:
        \begin{itemize}
            \item An agent powered by a deep neural network learns to play games (like "Breakout") directly from pixel data.
            \item Input: Raw pixel frames; Output: Probability distribution over actions.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Deep Q-Networks (DQN) - Introduction}
  \begin{block}{Introduction to Deep Q-Networks}
    Deep Q-Networks (DQNs) merge traditional Q-learning with deep learning, enabling efficient handling of complex environments in Deep Reinforcement Learning (DRL).
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Deep Q-Networks (DQN) - Functionality}
  \begin{block}{How DQNs Function in Deep RL Frameworks}
    \begin{enumerate}
      \item \textbf{Q-Learning Recap}:
        \begin{itemize}
          \item Agent learns by updating a Q-value table.
          \item Updates follow the Bellman equation:
            \begin{equation}
              Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
            \end{equation}
          \item Variables: \( s \), \( a \), \( r \), \( \alpha \), \( \gamma \).
        \end{itemize}
  
      \item \textbf{Challenges with Q-learning}:
        \begin{itemize}
          \item Q-tables become large for continuous/state spaces.
          \item Difficulty in generalizing similar states.
        \end{itemize}

      \item \textbf{Deep Learning Integration}:
        \begin{itemize}
          \item DQNs use neural networks to approximate Q-values:
            \begin{equation}
              Q(s, a) \approx \text{NeuralNetwork}(s)
            \end{equation}
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Deep Q-Networks (DQN) - Training and Applications}
  \begin{block}{Training Algorithm}
    \begin{itemize}
      \item \textbf{Experience Replay}: 
        \begin{itemize}
          \item Stores experiences in a replay buffer.
          \item Random sampling breaks correlations and stabilizes training.
        \end{itemize}
        
      \item \textbf{Target Network}:
        \begin{itemize}
          \item Utilizes a separate target network updated less frequently.
          \item Stabilizes learning updates.
        \end{itemize}
    \end{itemize}
  \end{block}

  \begin{block}{Example Scenario}
    DQNs applied to a game (e.g., Atari's Breakout):
    \begin{itemize}
      \item Input: Pixel values of the game screen.
      \item Output: Array of Q-values for potential actions.
      \item Updates Q-values through experience replay.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Deep Q-Networks (DQN) - Illustrative Code Snippet}
  \begin{lstlisting}[language=Python]
import random
import numpy as np
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95    # Discount rate
        self.epsilon = 1.0    # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        # Initiate Q-network and target network
        self.model = self._build_model()
        self.target_model = self._build_model()

    def _build_model(self):
        # Create a neural network to predict Q-values
        pass  # Implementation of the neural network architecture
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Improvements over Q-Learning - Introduction}
    \begin{itemize}
        \item \textbf{Q-Learning}: A model-free reinforcement learning algorithm learning action values in states.
        \item Uses a Q-table to store state-action pair values, improving estimates over time through exploration-exploitation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Improvements over Q-Learning - Limitations and DQNs}
    \begin{block}{Limitations of Traditional Q-Learning}
        \begin{itemize}
            \item \textbf{State-Action Space}: Struggles with high-dimensional environments—relies on discrete Q-table.
            \item \textbf{Sample Efficiency}: Requires large amounts of training data to converge effectively.
        \end{itemize}
    \end{block}

    \begin{block}{Introduction to Deep Q-Networks (DQN)}
        \begin{itemize}
            \item Integrates deep learning with Q-learning, using neural networks to approximate Q-values.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Improvements over Q-Learning - Key Enhancements}
    \begin{enumerate}
        \item \textbf{Function Approximation}:
            \begin{itemize}
                \item DQN uses a neural network instead of a Q-table to map states to Q-values.
            \end{itemize}

        \item \textbf{Experience Replay}:
            \begin{itemize}
                \item Stores experiences in a replay buffer for mini-batch training.
                \item Improves stability by breaking the correlation of consecutive samples.
                \item \textbf{Loss Function}:
                \begin{equation}
                    L(\theta) = \mathbb{E}_{(s, a, r, s') \sim \text{replay}} \left[ (r + \gamma \max_{a'} Q(s', a'; \theta^{-}) - Q(s, a; \theta))^2 \right]
                \end{equation}
            \end{itemize}

        \item \textbf{Target Network}:
            \begin{itemize}
                \item Provides stable Q-value targets, updating periodically from the online network.
            \end{itemize}

        \item \textbf{Better Exploration Strategies}:
            \begin{itemize}
                \item Utilizes techniques like $\epsilon$-greedy and Boltzmann exploration for effective exploration-exploitation balance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods - Overview}
    \begin{itemize}
        \item **Definition**: Optimize the policy directly in deep reinforcement learning.
        \item Unlike value-based methods, they parameterize the policy and adjust parameters to maximize expected rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods - Key Concepts}
    \begin{itemize}
        \item **Policy** ($\pi$): A mapping from states ($S$) to actions ($A$). Provides probabilities of taking actions given states.
        \item **Objective Function**:
          \begin{equation}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ G(\tau) \right]
          \end{equation}
          where \(G(\tau)\) is the return from trajectory $\tau$, and $\theta$ are the policy parameters.
        \item **Gradient Estimate**:
          \begin{equation}
            \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla \log \pi_\theta(a|s) G(\tau) \right]
          \end{equation}
          Adjusts the policy toward actions that yield higher rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods - Advantages and Algorithms}
    \begin{itemize}
        \item **Advantages**:
            \begin{itemize}
                \item Continuous action spaces: Useful in environments where discrete Q-values are impractical.
                \item Stability: More stable learning, particularly in complex environments.
                \item Direct control: Ability to model stochastic policies.
            \end{itemize}
        \item **Common Algorithms**:
            \begin{itemize}
                \item **REINFORCE Algorithm**: A basic implementation using the policy gradient theorem and Monte Carlo methods.
                \item **Proximal Policy Optimization (PPO)**: Advanced algorithm that uses clipped objective functions for stable updates.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Gradient Methods - Implementation Example}
    \begin{lstlisting}[language=Python]
import numpy as np

# Simple implementation of REINFORCE
def update_policy(states, actions, rewards, gamma=0.99):
    G = 0
    policy_loss = []
    for t in reversed(range(len(rewards))):
        G = rewards[t] + gamma * G  # Discount future rewards
        policy_loss.append(-np.log(policy_function(states[t], actions[t])) * G)
    return np.mean(policy_loss)
# Note: `policy_function` should define and parameterize your policy.
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion & Next Steps}
    \begin{itemize}
        \item Policy gradient methods provide a powerful alternative to traditional value-based methods.
        \item Direct optimization of the policy simplifies modeling complex behavior.
        \item Next topic: Actor-Critic Methods, combining policy-based and value-based techniques.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Introduction}
    \begin{block}{Overview}
        Actor-Critic methods are algorithms in Deep Reinforcement Learning (DRL) that combine value-based and policy-based approaches.
        This hybrid strategy involves two key components: the \textbf{actor} (policy) and the \textbf{critic} (value evaluation).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - Key Concepts}
    \begin{itemize}
        \item \textbf{Actor}:
        \begin{itemize}
            \item Responsible for selecting actions based on a given policy.
            \item Interacts with the environment to make decisions.
            \item Policy ($\pi$) can be stochastic or deterministic.
        \end{itemize}
        
        \item \textbf{Critic}:
        \begin{itemize}
            \item Evaluates actions by computing the value function.
            \item The value function ($V(s)$ for state or $Q(s, a)$ for action) guides policy improvement.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actor-Critic Methods - How It Works}
    \begin{enumerate}
        \item The actor chooses an action $a_t = \text{Actor}(s_t)$.
        \item The action leads to a new state $s_{t+1}$ and reward $r_t$.
        \item The critic evaluates the action with the Temporal Difference (TD) method:
        \begin{equation}
            \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
        \end{equation}
        \item The actor updates its policy:
        \begin{equation}
            \nabla J(\theta) \approx \nabla \ln \pi(a_t | s_t; \theta) \cdot \delta_t
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Deep Reinforcement Learning}
  \begin{block}{Overview of Common Challenges}
    Deep Reinforcement Learning (Deep RL) uses neural networks to approximate policies and value functions. While powerful, it faces significant challenges:
  \end{block}
  \begin{itemize}
    \item Instability
    \item Overfitting
    \item Sample Efficiency
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Instability}
  \begin{block}{Explanation}
    Instability arises from the interaction between the learning algorithm and the environment. Small changes in the policy can lead to large changes in the performance of the agent.
  \end{block}
  \begin{itemize}
    \item \textbf{Value Function Approximation:} Inaccuracies in value estimates can cause oscillations in learning.
    \item \textbf{Policy Updates:} Frequent updates can destabilize the learning process if not done carefully.
  \end{itemize}
  \begin{block}{Example}
    Imagine training a robot to navigate a maze. Rapid changes in decision-making may result in oscillation between ineffective strategies.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Overfitting and Sample Efficiency}
  \begin{block}{Overfitting}
    Overfitting occurs when a model captures noise rather than the underlying patterns, leading to poor generalization.
  \end{block}
  \begin{itemize}
    \item Impacts generalization to new states.
    \item Increases variance in value function outputs.
  \end{itemize}
  \begin{block}{Example}
    A Deep RL agent trained on specific video game strategies may fail in varied scenarios if it merely memorizes specific solutions.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Sample Efficiency}
  \begin{block}{Explanation}
    Deep RL often requires a large number of samples (interactions with the environment) to learn effectively.
  \end{block}
  \begin{itemize}
    \item \textbf{Issues:}
      \begin{itemize}
        \item High computational cost and resource utilization.
        \item Balancing exploration of new states vs. exploitation of known strategies.
      \end{itemize}
  \end{itemize}
  \begin{block}{Example}
    Training a driving agent may incur costs with each failed attempt, thus requiring efficient learning to reduce trials.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Strategies and Conclusion}
  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item \textbf{Instability:} Stabilized using experience replay and target networks.
      \item \textbf{Overfitting:} Mitigated through regularization techniques and better state representation.
      \item \textbf{Sample Efficiency:} Improved with transfer learning and hierarchical reinforcement learning.
    \end{itemize}
  \end{block}
  \begin{block}{Conclusion}
    Addressing these challenges is crucial for enhancing the effectiveness of Deep RL algorithms. Keep in mind these challenges as we move forward to applications of Deep RL.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Note}
  \begin{block}{Performance Monitoring}
    Always consider logging performance metrics and visualizing loss curves to identify overfitting and instability early. This practice can significantly enhance debugging and improve learning processes.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Deep Reinforcement Learning - Overview}
  \begin{block}{Overview}
    Deep Reinforcement Learning (Deep RL) combines reinforcement learning (RL) with deep learning techniques. It enables machines to learn decision-making in complex environments.
  \end{block}
  \begin{itemize}
    \item Robotics
    \item Gaming
    \item Optimization
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Deep Reinforcement Learning - Robotics}
  \begin{block}{Concept}
    In robotics, Deep RL enables robots to learn tasks through trial-and-error interactions, mimicking human learning.
  \end{block}
  \begin{itemize}
    \item \textbf{Example:} \textit{Robotic Manipulation}
      \begin{itemize}
        \item Robots learn to pick and place objects.
        \item Positive rewards for success, negative for failures.
      \end{itemize}
    \item \textbf{Key Point:}
      \begin{itemize}
        \item Real-time learning and adaptation to unstructured environments.
        \item Applications: Industrial automation, autonomous vehicles, personal assistants.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Deep Reinforcement Learning - Gaming & Optimization}
  \begin{block}{Gaming}
    \begin{itemize}
      \item \textbf{Concept:} Deep RL agents learn to play games at superhuman levels.
      \item \textbf{Example:} \textit{AlphaGo}
        \begin{itemize}
          \item Defeated world champions by learning unknown strategies.
        \end{itemize}
      \item \textbf{Key Point:}
        \begin{itemize}
          \item Complex strategy learning and impactful game design.
        \end{itemize}
    \end{itemize}
  \end{block}
  
  \begin{block}{Optimization}
    \begin{itemize}
      \item \textbf{Concept:} Solves complex optimization problems.
      \item \textbf{Example:} \textit{Supply Chain Optimization}
        \begin{itemize}
          \item Optimizes resource allocation and logistics.
        \end{itemize}
      \item \textbf{Key Point:}
        \begin{itemize}
          \item Adapts to changing conditions; applicable in finance, healthcare, telecommunications.
        \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Deep Reinforcement Learning - Conclusion & Key Takeaways}
  \begin{block}{Conclusion}
    The diverse applications of Deep RL demonstrate its transformative potential across various fields. It helps solve real-world challenges by adapting, learning, and optimizing complex systems.
  \end{block}
  \begin{block}{Key Takeaways}
    \begin{itemize}
      \item Deep RL advances robotics, gaming strategies, and optimization.
      \item Concepts of trial-and-error learning highlight its innovative nature.
    \end{itemize}
  \end{block}
  \begin{block}{Further Reading}
    \begin{itemize}
      \item \href{https://arxiv.org/abs/2003.03371}{Deep Reinforcement Learning: An Overview}
      \item \href{https://deepmind.com/research/case-studies/alphago-the-story-so-far}{DeepMind’s Research on Game Playing AI}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Industry Case Studies - Introduction}
    \begin{block}{Introduction to Deep Reinforcement Learning (DRL)}
        Deep Reinforcement Learning combines deep learning and reinforcement learning to create powerful models capable of making decisions and optimizing actions in complex environments. 
        This approach is well-suited for real-world applications because of its ability to learn from high-dimensional sensory inputs like images or complex data structures.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Industry Case Studies - Overview}
    \begin{block}{Key Case Studies in Various Industries}
        \begin{enumerate}
            \item \textbf{Gaming Industry: AlphaGo}
            \item \textbf{Robotics: OpenAI’s Dota 2 Bot}
            \item \textbf{Finance: Algorithmic Trading}
            \item \textbf{Healthcare: Personalized Medicine}
            \item \textbf{Transportation: Autonomous Vehicles}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: AlphaGo}
    \begin{itemize}
        \item \textbf{Overview:} Developed by DeepMind, AlphaGo defeated world champion Go player Lee Sedol in 2016.
        \item \textbf{Methodology:} 
            \begin{itemize}
                \item Used a combination of supervised learning from human games.
                \item Reinforcement learning through self-play to improve performance.
            \end{itemize}
        \item \textbf{Key Insight:} 
            AlphaGo showcased DRL's capability to manage vast state spaces and tackle complex decision-making challenges.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: OpenAI’s Dota 2 Bot}
    \begin{itemize}
        \item \textbf{Overview:} OpenAI trained a bot for Dota 2, demonstrating coordination and strategic planning.
        \item \textbf{Methodology:}
            \begin{itemize}
                \item Employed Proximal Policy Optimization (PPO).
                \item Trained in a multi-agent environment via interactions.
            \end{itemize}
        \item \textbf{Key Insight:} 
            Illustrates DRL's application in real-time strategy games emphasizing timing and teamwork.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Algorithmic Trading}
    \begin{itemize}
        \item \textbf{Overview:} Firms like JPMorgan Chase utilize DRL for automating trading strategies.
        \item \textbf{Methodology:}
            \begin{itemize}
                \item Historical data used for training agents to predict market trends.
                \item Decisions driven by maximizing returns and minimizing risks.
            \end{itemize}
        \item \textbf{Key Insight:} 
            DRL aids in navigating the stochastic environment of financial markets.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Personalized Medicine}
    \begin{itemize}
        \item \textbf{Overview:} Hospitals are employing DRL for personalized treatment in healthcare.
        \item \textbf{Methodology:}
            \begin{itemize}
                \item Algorithms analyze patient history datasets for tailored therapies.
            \end{itemize}
        \item \textbf{Key Insight:} 
            Significant opportunities to enhance patient outcomes by optimizing treatment strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Autonomous Vehicles}
    \begin{itemize}
        \item \textbf{Overview:} Companies like Tesla and Waymo implement DRL in self-driving cars.
        \item \textbf{Methodology:}
            \begin{itemize}
                \item Vehicles learn to navigate through simulations and real-life scenarios.
                \item Reinforcement learning adjusts driving policies for varying road conditions.
            \end{itemize}
        \item \textbf{Key Insight:} 
            Demonstrates DRL’s ability to handle dynamic environments while ensuring safety.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}
    \begin{itemize}
        \item Deep Reinforcement Learning excels in areas requiring complex decision-making.
        \item Case studies across diverse industries show its versatility.
        \item Learning from environmental interactions allows DRL agents to adapt in real-time.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engaging with DRL}
    \begin{itemize}
        \item Explore tools and frameworks for implementing DRL (e.g., TensorFlow, PyTorch).
        \item Consider ethical implications and safety measures when deploying AI systems.
    \end{itemize}
    By examining these case studies, we gain insights into the implications and possibilities of deep reinforcement learning across various sectors.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Research Frontiers in Deep Reinforcement Learning}
  \begin{block}{Overview}
    Deep Reinforcement Learning (Deep RL) combines reinforcement learning (RL) principles with deep learning. This approach addresses complex decision-making tasks across various applications. Understanding current trends and future directions is essential as the field evolves rapidly.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Current Trends in Deep RL - Part 1}
  \begin{enumerate}
    \item \textbf{Hierarchical Reinforcement Learning (HRL)}
      \begin{itemize}
        \item \textbf{Concept:} Decomposes complex tasks into simpler sub-tasks for modular learning.
        \item \textbf{Example:} Navigating a maze requires both high-level planning and low-level movement.
        \item \textbf{Key Point:} HRL enhances efficiency by reusing learned skills across tasks.
      \end{itemize}
      
    \item \textbf{Multi-Agent Reinforcement Learning (MARL)}
      \begin{itemize}
        \item \textbf{Concept:} Involves multiple agents learning and interacting in the same environment.
        \item \textbf{Example:} Autonomous vehicles cooperating to safely navigate city streets.
        \item \textbf{Key Point:} Focus on communication, coordination, and competition for optimal group performance.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Current Trends in Deep RL - Part 2}
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Exploration vs. Exploitation}
      \begin{itemize}
        \item \textbf{Concept:} Balancing exploration of new strategies and exploiting known ones to discover optimal policies.
        \item \textbf{Example:} An agent in a game might explore new moves or use successful tactics.
        \item \textbf{Key Point:} Techniques like Upper Confidence Bound (UCB) and Thompson Sampling enhance exploration strategies.
      \end{itemize}

    \item \textbf{Transfer Learning in RL}
      \begin{itemize}
        \item \textbf{Concept:} Leverages knowledge from one task to improve learning in another related task.
        \item \textbf{Example:} An RL model can apply learned skills from one video game to another.
        \item \textbf{Key Point:} Speeds up the training process and reduces data requirements.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Future Directions in Deep RL}
  \begin{enumerate}
    \item \textbf{Sample Efficiency Improvement}
      \begin{itemize}
        \item \textbf{Trend:} Reducing data needed for training RL algorithms is vital.
        \item \textbf{Potential Solutions:} Combine RL with imitation learning and few-shot learning.
      \end{itemize}

    \item \textbf{Safety and Robustness}
      \begin{itemize}
        \item \textbf{Trend:} Ensuring RL agents operate safely in real-world applications.
        \item \textbf{Focus Areas:} Prevent catastrophic failures in complex environments.
      \end{itemize}

    \item \textbf{Explainability and Interpretability}
      \begin{itemize}
        \item \textbf{Trend:} Providing insights into the decision-making processes of RL agents.
        \item \textbf{Importance:} Building trust in high-stakes applications like healthcare.
      \end{itemize}

    \item \textbf{Integration with Other AI Fields}
      \begin{itemize}
        \item \textbf{Trend:} Merging Deep RL with NLP and computer vision for integrated systems.
        \item \textbf{Example:} Conversational agents that learn optimal interactions over time.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Takeaways}
  \begin{itemize}
    \item Deep RL is rapidly evolving with a focus on efficiency, safety, and interpretability.
    \item Understanding emerging trends such as HRL, MARL, and transfer learning is crucial.
    \item Preparing for future directions ensures research contributions are relevant and impactful.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Overview - Goals}
    \begin{block}{Understanding the Project Goals}
        In this collaborative project, students will explore the applications and methodologies of Deep Reinforcement Learning (DRL). The primary objectives are:
    \end{block}
    \begin{itemize}
        \item \textbf{Development}: Design and implement a DRL agent capable of solving a specified problem using a simulation or game environment.
        \item \textbf{Collaboration}: Engage with peers to brainstorm ideas, share insights, and provide feedback on project development.
        \item \textbf{Documentation}: Maintain comprehensive project documentation detailing your methodology, experiments, and results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Overview - Structure}
    \begin{block}{Project Structure}
        The project will consist of the following key components:
    \end{block}
    \begin{enumerate}
        \item \textbf{Problem Definition (Week 1)}: Clearly define the problem you are addressing.
        \item \textbf{Agent Design (Weeks 2-4)}: Choose and implement a DRL algorithm.
            \begin{lstlisting}[language=Python]
            import gym
            from stable_baselines3 import PPO  # Example: Using Proximal Policy Optimization
            
            env = gym.make("CartPole-v1")
            model = PPO("MlpPolicy", env, verbose=1)
            model.learn(total_timesteps=10000)
            \end{lstlisting}
        \item \textbf{Training and Evaluation (Weeks 5-8)}: Train the agent, define metrics, and visualize performance.
        \item \textbf{Results Analysis (Weeks 9-11)}: Analyze training results and visualize data using graphs and charts.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Project Overview - Deliverables}
    \begin{block}{Deliverables}
        Ensure your project includes the following:
    \end{block}
    \begin{itemize}
        \item \textbf{Codebase}: A well-structured codebase with clear comments and documentation.
        \item \textbf{Report}: A detailed project report (5-10 pages) with:
        \begin{itemize}
            \item \textbf{Introduction}: Overview of the problem.
            \item \textbf{Methodology}: Description of algorithms, environments, and training details.
            \item \textbf{Results}: Findings, graphs, and discussion.
        \end{itemize}
        \item \textbf{Presentation}: A 10-minute presentation summarizing key learning outcomes, challenges faced, and future work suggestions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feedback and Evaluation Methods - Overview}
  Feedback and evaluation are essential in assessing student comprehension and project outcomes in Deep Reinforcement Learning (DRL). 

  \begin{block}{Key Aspects}
    \begin{itemize}
      \item Importance of effective assessment strategies
      \item Mechanisms for understanding and applying DRL concepts
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feedback and Evaluation in DRL}
  
  \underline{Understanding Feedback and Evaluation in DRL}
  
  \begin{enumerate}
    \item \textbf{Formative Assessment}
      \begin{itemize}
        \item Continuous real-time feedback
        \item Examples: quizzes, coding challenges, peer reviews
      \end{itemize}
      
    \item \textbf{Summative Assessment}
      \begin{itemize}
        \item Evaluation at the end of a unit
        \item Examples: presentations, finalized projects
      \end{itemize}
      
    \item \textbf{Project-Specific Feedback}
      \begin{itemize}
        \item Criteria include clarity, innovation, and effectiveness
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Assessment Strategies}

  \begin{itemize}
    \item \textbf{Rubric-Based Assessment}
      \begin{itemize}
        \item Detailed criteria for success such as:
          \begin{itemize}
            \item Understanding of DRL frameworks (e.g., TensorFlow, PyTorch)
            \item Application of theoretical concepts
            \item Code quality and documentation
          \end{itemize}
      \end{itemize}

    \item \textbf{Code Review Sessions}
      \begin{itemize}
        \item Peer reviews for constructive feedback on code presentation and design
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Examples and Key Points}

  \underline{Example Quiz Question}
  
  \begin{itemize}
    \item \textit{Explain the role of the reward function in a reinforcement learning environment. Provide an example of how it influences agent behavior.}
  \end{itemize}
  
  \underline{Project Evaluation Framework}
  
  \begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
      Criteria & Excellent (5) & Good (4) & Fair (3) & Needs Improvement (2) & Poor (1) \\ \hline
      Methodology & Clear and innovative use of DRL techniques & Good use with minor errors & Average understanding & Major errors & Lacks understanding \\ \hline
      Results \& Interpretation & Clear analysis of results & Mostly clear interpretation & Basic analysis & Incomplete analysis & No analysis \\ \hline
      Presentation \& Clarity & Well-organized and engaging & Mostly organized & Somewhat clear & Disorganized & Confusing \\ \hline
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Action Items}
  
  \underline{Key Points to Emphasize}
  
  \begin{itemize}
    \item \textbf{Iterative Learning}
      \begin{itemize}
        \item Feedback as a tool for development
      \end{itemize}
    \item \textbf{Collaboration}
      \begin{itemize}
        \item Peer feedback enhances problem-solving creativity
      \end{itemize}
    \item \textbf{Real-World Applications}
      \begin{itemize}
        \item Evaluating projects against real-world scenarios
      \end{itemize}
  \end{itemize}

  \underline{Action Items}
  
  \begin{itemize}
    \item Prepare your project for peer review.
    \item Familiarize yourself with the evaluation rubric.
    \item Engage with peers for feedback during project development.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Understanding Reinforcement Learning (RL)}:
        \begin{itemize}
            \item \textbf{Definition}: RL is a machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards.
            \item \textbf{Components}:
            \begin{itemize}
                \item \textbf{Agent}: The learner or decision maker.
                \item \textbf{Environment}: The setting through which the agent interacts.
                \item \textbf{Actions (A)}: Choices the agent can make.
                \item \textbf{States (S)}: All possible situations the agent can find itself in.
                \item \textbf{Rewards (R)}: Feedback received from the environment based on actions taken.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Deep Learning Integration}:
        \begin{itemize}
            \item \textbf{Deep Q-Networks (DQN)}: Combines Q-learning with deep neural networks to approximate the Q-value function.
            \item \textbf{Policy Gradients}: Directly parameterize and optimize the policy function, enhancing exploration.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Q\&A - Key Takeaways Continued}
    \begin{enumerate}
        \setcounter{enumi}{3} % Continue numbering
        \item \textbf{Common Algorithms}:
        \begin{itemize}
            \item \textbf{DQN}: Uses experience replay and target networks to stabilize learning.
            \item \textbf{Actor-Critic}: Combines value-based and policy-based approaches for efficient updates.
        \end{itemize}
        
        \item \textbf{Practical Applications}:
        \begin{itemize}
            \item \textbf{Gaming}: AlphaGo and training agents to play video games (e.g., Atari).
            \item \textbf{Robotics}: Navigating environments and learning tasks through trial and error.
            \item \textbf{Finance}: Algorithmic trading strategies based on market signals.
        \end{itemize}
        
        \item \textbf{Future Directions}:
        \begin{itemize}
            \item \textbf{Sample Efficiency}: Improving how quickly an agent learns from less data.
            \item \textbf{Transfer Learning}: Allowing agents to apply knowledge from one task to another.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engaging Q\&A Session}
    \begin{block}{Encourage Questions}
        Invite students to ask about concepts that were unclear or intriguing. Some example questions could include:
        \begin{itemize}
            \item How does the choice of discount factor [$\gamma$] affect learning?
            \item What are the advantages of policy gradient methods over value-based methods?
        \end{itemize}
    \end{block}
    
    \begin{block}{Discussion}
        Promote a collaborative discussion on potential future applications of deep reinforcement learning or the ethical considerations of deploying such technologies.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Further Readings and Resources - Introduction}
  \begin{block}{Introduction to Further Reading}
    Deep Reinforcement Learning (DRL) is a rapidly evolving field that combines reinforcement learning (RL) and deep learning. 
    To deepen your understanding and explore advanced topics, the following resources are recommended:
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Further Readings and Resources - Books}
  \begin{block}{Books}
    \begin{enumerate}
      \item \textbf{"Deep Reinforcement Learning Hands-On" by Maxim Lapan}
        \begin{itemize}
          \item Practical hands-on projects using Python and PyTorch.
          \item Covers foundational RL concepts and advanced DRL architectures.
          \item \textbf{Key Example:} Implementing various algorithms like DQN and PPO.
        \end{itemize}
        
      \item \textbf{"Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto}
        \begin{itemize}
          \item Thorough coverage of theoretical foundations and practical examples.
          \item \textbf{Key Topics:} Exploration vs. Exploitation, Markov Decision Processes, Temporal-Difference Learning.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Further Readings and Resources - Research Papers}
  \begin{block}{Research Papers}
    \begin{enumerate}
      \item \textbf{"Playing Atari with Deep Reinforcement Learning" by Mnih et al. (2013)}
        \begin{itemize}
          \item Introduced the DQN algorithm, training neural networks to play Atari games from pixel inputs.
          \item \textbf{Key Contribution:} Combining RL with deep learning for high-dimensional sensory data.
        \end{itemize}

      \item \textbf{"Continuous Control with Deep Reinforcement Learning" by Lillicrap et al. (2015)}
        \begin{itemize}
          \item Presents the Deep Deterministic Policy Gradient (DDPG) algorithm for continuous action spaces.
          \item \textbf{Key Insight:} Application of DRL in robotic control tasks.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Further Readings and Resources - Online Courses}
  \begin{block}{Online Courses}
    \begin{enumerate}
      \item \textbf{Coursera: "Deep Learning Specialization" by Andrew Ng}
        \begin{itemize}
          \item Covers fundamental deep learning concepts aiding RL integration.
          \item \textbf{Emphasized Module:} Last course on Sequence Models in Neural Networks.
        \end{itemize}

      \item \textbf{Udacity: "Deep Reinforcement Learning Nanodegree"}
        \begin{itemize}
          \item Comprehensive curriculum with projects using TensorFlow and PyTorch.
          \item \textbf{Project Example:} Creating an agent for Unity's Banana Collector or CartPole.
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Further Readings and Resources - Online Blogs and Discussion}
  \begin{block}{Online Blogs and Tutorials}
    \begin{enumerate}
      \item \textbf{Towards Data Science}
        \begin{itemize}
          \item Articles on practical DRL implementations and trends.
          \item Search for "Deep Reinforcement Learning" topics for insights.
        \end{itemize}

      \item \textbf{OpenAI Blog}
        \begin{itemize}
          \item Insights into AI research and DRL advancements.
          \item \textbf{Highlighted Articles:} Model interpretability and safety in RL discussions.
        \end{itemize}
    \end{enumerate}
  \end{block}

  \begin{block}{Conclusion}
    Encourages self-study using these resources to stay current in DRL techniques.
  \end{block}
\end{frame}


\end{document}