\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning - Neural Networks]{Week 7: Supervised Learning - Neural Networks}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks}
    \begin{block}{What are Neural Networks?}
        Neural networks are a subset of machine learning algorithms inspired by the structure and function of the human brain. They consist of interconnected nodes (or neurons) that process data in a layered architecture.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of Neural Networks}
    \begin{itemize}
        \item \textbf{Neurons:} Fundamental units that receive inputs, process them, and produce outputs.
        \item \textbf{Layers:}
        \begin{itemize}
            \item \textbf{Input Layer:} Receives the initial data.
            \item \textbf{Hidden Layers:} Perform computations and feature extraction.
            \item \textbf{Output Layer:} Produces the final output/results.
        \end{itemize}
        \item \textbf{Weights:} Parameters that adjust as the model learns, determining the influence of each input on the output.
        \item \textbf{Biases:} Additional parameters that allow the model to fit the data better.
        \item \textbf{Activation Functions:} Mathematical functions that determine whether a neuron should be activated based on the input it receives (e.g., ReLU, Sigmoid).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Machine Learning}
    \begin{itemize}
        \item \textbf{Feature Learning:} Automatically learns features from raw data, reducing the need for manual feature extraction.
        \item \textbf{Complex Pattern Recognition:} Effective in recognizing intricate patterns, ideal for tasks like image classification and natural language processing.
        \item \textbf{Scalability:} Well-suited for large datasets due to their ability to learn hierarchical representations.
    \end{itemize}
    \begin{block}{Example Use Case: Image Classification}
        In image recognition tasks, neural networks transform pixel data through multiple layers into high-level representations that can accurately classify images (e.g., identifying different breeds of dogs).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematics and Code Example}
    \begin{block}{Neuron Calculation}
        Each neuron computes the weighted sum of inputs:
        \begin{equation}
            z = \sum_{i=1}^{n} (w_i \cdot x_i) + b
        \end{equation}
        where \( z \) is the neuron's output before applying the activation function, \( w \) are the weights, \( x \) are the inputs, and \( b \) is the bias.
    \end{block}
    \begin{lstlisting}[language=Python]
import numpy as np

# Sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Example input, weights, and bias
inputs = np.array([0.5, 0.2])
weights = np.array([0.4, 0.6])
bias = 0.1

# Compute neuron output
z = np.dot(weights, inputs) + bias
output = sigmoid(z)
print(output)  # Output will be between 0 and 1
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Neural Networks - Part 1}
  \frametitle{Key Concepts in Neural Networks}

  \begin{itemize}
    \item \textbf{Nodes (Neurons)}
    \begin{itemize}
      \item \textbf{Definition:} A fundamental unit that receives input, processes it, and produces output.
      \item \textbf{Functionality:}
      \begin{itemize}
        \item Inputs multiplied by weights and summed.
        \item Result passed through an activation function.
      \end{itemize}
    \end{itemize}
    
    \item \textbf{Layers}
    \begin{itemize}
      \item \textbf{Types:}
      \begin{itemize}
        \item Input Layer: Receives raw input features.
        \item Hidden Layers: Intermediate processing layers.
        \item Output Layer: Produces the final result.
      \end{itemize}
      \item \textbf{Example:} Neural network for image recognition with 1 Input Layer, 2 Hidden Layers, and 1 Output Layer.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Neural Networks - Part 2}
  \frametitle{Key Concepts in Neural Networks (Continued)}

  \begin{itemize}
    \item \textbf{Weights}
    \begin{itemize}
      \item \textbf{Definition:} Parameters that transform input data within nodes.
      \item \textbf{Role:} Adjusted as the network learns from the data.
      \item \textbf{Mathematical Representation:}
      \begin{equation}
      z = w_1x_1 + w_2x_2 + ... + w_nx_n + b
      \end{equation}
      where \( w \) are weights, \( x \) are input features, and \( b \) is the bias.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Key Concepts in Neural Networks - Part 3}
  \frametitle{Key Concepts in Neural Networks (Final Part)}

  \begin{itemize}
    \item \textbf{Activation Functions}
    \begin{itemize}
      \item \textbf{Purpose:} Introduce non-linearity to the model.
      \item \textbf{Common Types:}
      \begin{itemize}
        \item \textbf{Sigmoid:} Outputs values between 0 and 1.
        \begin{equation}
        \sigma(z) = \frac{1}{1 + e^{-z}}
        \end{equation}
        \item \textbf{ReLU:} Outputs 0 for negative input, linear for positive.
        \begin{equation}
        f(z) = \max(0, z)
        \end{equation}
        \item \textbf{Softmax:} Converts outputs into probabilities.
        \begin{equation}
        P(y_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
        \end{equation}
      \end{itemize}
    \end{itemize}
  \end{itemize}

  \begin{block}{Key Points to Emphasize}
    \begin{itemize}
      \item Neurons mimic biological neurons; process inputs through weighted connections.
      \item Layer structure determines model complexity and learning ability.
      \item Weights are critical, adjusted during training to minimize prediction errors.
      \item Choice of activation function significantly impacts performance.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Structure of Neural Networks}
  \begin{block}{Overview of Neural Network Architectures}
    Neural networks are structured in different ways to solve various types of problems. The three primary architectures we will focus on are:
    \begin{enumerate}
      \item Feedforward Neural Networks (FNN)
      \item Convolutional Neural Networks (CNN)
      \item Recurrent Neural Networks (RNN)
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{1. Feedforward Neural Networks (FNN)}
  \begin{block}{Explanation}
    FNNs are the simplest type of artificial neural network. Information moves in one direction—from the input layer, through hidden layers, to the output layer. There are no cycles or loops, so the output depends solely on the current input and the associated weights.
  \end{block}
  
  \begin{block}{Example}
    Used in classification tasks, such as predicting whether an email is spam or not.
  \end{block}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item Composed of an input layer, one or more hidden layers, and an output layer.
      \item Activation functions (e.g., ReLU, Sigmoid) introduce non-linearity into the model.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{2. Convolutional Neural Networks (CNN)}
  \begin{block}{Explanation}
    CNNs are specifically designed for processing structured grid data such as images. They utilize convolutional layers that apply filters (kernels) to input data to capture spatial hierarchical features.
  \end{block}

  \begin{block}{Example}
    Commonly used in image recognition tasks, such as identifying objects in photos (e.g., distinguishing cats from dogs).
  \end{block}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item Features layers like convolutional layers, pooling layers, and fully connected layers.
      \item Pooling layers help in down-sampling the feature maps, reducing the computational load.
    \end{itemize}
  \end{block}

  \begin{block}{Visualization}
    Convolution operation:
    \begin{equation}
      (I * K)(x,y) = \sum_{m}\sum_{n}I(m,n)K(x-m,y-n)
    \end{equation}
    Where \(I\) is the input image, \(K\) is the kernel, and \((x,y)\) are the coordinates of the output feature map.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{3. Recurrent Neural Networks (RNN)}
  \begin{block}{Explanation}
    RNNs are designed for sequential data processing. They maintain a hidden state that is updated at each time step, allowing them to capture temporal dependencies. Unlike FNNs, RNNs can utilize information from previous inputs, making them suitable for tasks where context matters.
  \end{block}
  
  \begin{block}{Example}
    Ideal for natural language processing tasks, such as language translation and sentiment analysis.
  \end{block}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item Features loops, allowing information to persist, which is crucial for understanding context in sequences.
      \item Could suffer from vanishing gradient problems, but Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures address these issues.
    \end{itemize}
  \end{block}

  \begin{block}{Mathematical Representation}
    The recurrent relationship can be expressed as:
    \begin{equation}
      h_t = f(W_hh_{t-1} + W_xx_t + b)
    \end{equation}
    Where \(h_t\) is the hidden state at time \(t\), \(W_h\) and \(W_x\) are weight matrices, \(x_t\) is the input at time \(t\), and \(b\) is a bias term.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Understanding the structure of neural networks is fundamental to their application in machine learning. Each architecture—FNNs, CNNs, and RNNs—serves distinct purposes and is optimized for different types of data. By leveraging these architectures appropriately, we can build powerful models for a variety of tasks, from image processing to natural language understanding. Keep these key points in mind as we delve deeper into the world of deep learning in the upcoming slides!
\end{frame}

\begin{frame}
    \frametitle{Deep Learning - Introduction}
    \begin{block}{Introduction to Deep Learning}
        Deep Learning is a subfield of machine learning that is based on artificial neural networks with representation learning. It enables computers to perform tasks directly from images, text, or sound.
    \end{block}
    
    \begin{block}{Neural Networks}
        Inspired by the structure of the human brain, neural networks consist of layers of interconnected nodes (neurons) that process data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Deep Learning and Neural Networks}
    \begin{itemize}
        \item \textbf{Neural Networks}:
        \begin{itemize}
            \item Foundation of Deep Learning.
            \item Comprises an input layer, one or more hidden layers, and an output layer.
            \item Complexity increases with the number of layers and types of neurons.
        \end{itemize}
        \item \textbf{Depth}:
        \begin{itemize}
            \item Refers to the number of layers in a neural network.
            \item More layers capture more complex features and representations from the data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts and Examples}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Architecture}: Different neural network architectures are suited for various tasks.
            \item \textbf{Learning Process}: Networks learn by adjusting weights based on errors, more effective with deeper networks.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Examples}
        \begin{enumerate}
            \item \textbf{Image Classification}: 
            \begin{itemize}
                \item With CNNs, deeper architectures identify basic shapes in early layers and combine them for complex pattern recognition.
            \end{itemize}
            \item \textbf{Text Processing}: 
            \begin{itemize}
                \item RNNs can predict sequences, capturing context in deeper networks.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Conclusion and Key Points}
    \begin{itemize}
        \item \textbf{Scalability}: Deep Learning excels with large datasets.
        \item \textbf{Computation}: GPUs make it feasible to train deeper architectures quickly.
        \item \textbf{Transfer Learning}: Using pre-trained models for new tasks allows leveraging existing deep models effectively.
    \end{itemize}
    
    \begin{block}{Conclusion}
        Deep Learning is transforming problem-solving in various fields, making it essential to understand its reliance on the depth and complexity of neural networks for building robust AI models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula and Code Snippet}
    \begin{block}{Formula}
        The forward pass of a neural network can be summarized as:
        \begin{equation}
            y = f(W \cdot x + b)
        \end{equation}
        where:
        \begin{itemize}
            \item \( y \) = output
            \item \( W \) = weights
            \item \( x \) = input
            \item \( b \) = bias
            \item \( f \) = activation function
        \end{itemize}
    \end{block}
    
    \begin{block}{Python Example}
        \begin{lstlisting}[language=Python]
import tensorflow as tf
from tensorflow.keras import layers, models

# Example of a simple deep learning model using TensorFlow
model = models.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=(input_dim,))) # First hidden layer
model.add(layers.Dense(64, activation='relu')) # Second hidden layer
model.add(layers.Dense(num_classes, activation='softmax')) # Output layer
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - Overview}
    \begin{block}{Description}
        Understanding various activation functions such as Sigmoid, Tanh, and ReLU and their roles in training.
    \end{block}
    
    \begin{enumerate}
        \item What are Activation Functions?
        \item Common Activation Functions
        \item Example Illustrations
        \item Key Points to Emphasize
        \item Summary
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Activation Functions - What are They?}
    \begin{block}{Definition}
        Activation functions determine how the weighted sum of inputs is transformed into an output in a neuron.
    \end{block}
    
    \begin{itemize}
        \item Introduce non-linearity into the model.
        \item Enable neural networks to learn complex patterns.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Activation Functions}
    \begin{itemize}
        \item \textbf{Sigmoid Function}
        \begin{itemize}
            \item \textbf{Formula:} $$ f(x) = \frac{1}{1 + e^{-x}} $$
            \item \textbf{Range:} (0, 1)
            \item \textbf{Characteristics:}
            \begin{itemize}
                \item Smooth gradient.
                \item Outputs as probabilities.
            \end{itemize}
            \item \textbf{Drawbacks:} Vanishing gradients for large values.
        \end{itemize}
        
        \item \textbf{Hyperbolic Tangent (Tanh) Function}
        \begin{itemize}
            \item \textbf{Formula:} $$ f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
            \item \textbf{Range:} (-1, 1)
            \item \textbf{Characteristics:}
            \begin{itemize}
                \item Zero-centered outputs.
            \end{itemize}
            \item \textbf{Drawbacks:} Suffers from vanishing gradients.
        \end{itemize}
        
        \item \textbf{Rectified Linear Unit (ReLU)}
        \begin{itemize}
            \item \textbf{Formula:} $$ f(x) = \max(0, x) $$
            \item \textbf{Range:} [0, $\infty$)
            \item \textbf{Characteristics:}
            \begin{itemize}
                \item Computationally efficient.
                \item Mitigates vanishing gradient problem.
            \end{itemize}
            \item \textbf{Drawbacks:} Can suffer from "dying ReLU."
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{itemize}
        \item Activation functions are vital for neurons to learn and model complex relationships.
        \item The choice of activation function affects model performance.
        \item Different use cases: 
        \begin{itemize}
            \item Sigmoid for binary classification.
            \item Tanh for hidden layers.
            \item ReLU for general use.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Summary}
        Activation functions introduce non-linear transformations, enhancing neural networks' learning abilities, improving outcomes in tasks like classification and regression.
    \end{block}

    \begin{block}{Next Steps}
        Prepare to delve into the \textbf{Forward Propagation} process to explore how outputs are generated.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward Propagation - Overview}
  \begin{block}{Definition}
    Forward propagation is a crucial process in neural networks where input data is passed through multiple layers to generate outputs. It predicts outcomes based on learned weights and biases using activation functions.
  \end{block}
  
  \begin{itemize}
    \item Essential for making predictions.
    \item Precedes the backpropagation phase.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward Propagation - Step-by-Step Process}
  \begin{enumerate}
    \item \textbf{Input Layer}:
      \begin{itemize}
        \item Input feature vector is fed into the network (e.g., height, weight, age).
      \end{itemize}
    
    \item \textbf{Weighted Sums}:
      \begin{equation}
        z = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
      \end{equation}
      \begin{itemize}
        \item \( w \) are weights, \( x \) are inputs, \( b \) is the bias.
      \end{itemize}
    
    \item \textbf{Activation Function}:
      \begin{itemize}
        \item Applies non-linearity (e.g., Sigmoid, Tanh, ReLU).
      \end{itemize}
    
    \item \textbf{Hidden Layers}:
      \begin{itemize}
        \item Outputs from one layer input to the next; process repeats.
      \end{itemize}
    
    \item \textbf{Output Layer}:
      \begin{itemize}
        \item Outputs results; activation function varies by task (e.g., softmax).
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Forward Propagation - Key Points and Example Code}
  \begin{block}{Key Points}
    \begin{itemize}
      \item Forward propagation flows in one direction (input to output).
      \item Weights and biases determine network performance.
      \item Activation functions enable learning of complex patterns.
    \end{itemize}
  \end{block}

  \begin{block}{Example Code Snippet (Python)}
    \begin{lstlisting}[language=Python]
import numpy as np

# Example weights and biases
weights = np.array([[0.2, 0.8], [0.5, 0.3]])
biases = np.array([0.1, 0.2])
inputs = np.array([1.0, 0.5])

# Forward Propagation Calculation
z = np.dot(weights, inputs) + biases
activation_output = 1 / (1 + np.exp(-z))  # Sigmoid activation
print("Output of Forward Propagation:", activation_output)
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Loss Functions}
  \begin{block}{Overview}
    In supervised learning, loss functions measure how well a neural network's predictions match the actual outcomes. They quantify the error between predicted and true values, guiding optimization during training. Two popular loss functions are:
    \begin{itemize}
      \item Mean Squared Error (MSE)
      \item Cross-Entropy Loss
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Mean Squared Error (MSE)}
  \begin{block}{Definition}
    Mean Squared Error is primarily used in regression tasks. It computes the average of the squares of the errors, which is the average squared difference between estimated values and the actual values.
  \end{block}

  \begin{block}{Formula}
    \begin{equation}
      \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
    \end{equation}
    where:
    \begin{itemize}
      \item \( y_i \) = true value
      \item \( \hat{y}_i \) = predicted value
      \item \( N \) = number of observations
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example of MSE}
  Consider a model's predictions for three data points:
  \begin{itemize}
    \item True values: \( [3, -0.5, 2] \)
    \item Predicted values: \( [2.5, 0.0, 2] \)
  \end{itemize}
  \begin{block}{Calculation}
    \begin{equation}
      \text{MSE} = \frac{1}{3} \left((3 - 2.5)^2 + (-0.5 - 0.0)^2 + (2 - 2)^2\right) = \frac{1}{3} \left(0.25 + 0.25 + 0\right) = \frac{0.5}{3} \approx 0.167
    \end{equation}
  \end{block}

  \begin{block}{Key Points}
    \begin{itemize}
      \item **Sensitivity**: MSE heavily penalizes larger errors due to squaring, making it sensitive to outliers.
      \item **Continuous Outputs**: Best suited for problems predicting continuous values.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Cross-Entropy Loss}
  \begin{block}{Definition}
    Cross-Entropy Loss is primarily used for classification tasks, measuring the dissimilarity between the predicted and true probability distributions.
  \end{block}

  \begin{block}{Formula}
    For binary classification:
    \begin{equation}
      \text{BCE} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)
    \end{equation}
    For multi-class classification:
    \begin{equation}
      \text{CCE} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
    \end{equation}
    where:
    \begin{itemize}
      \item \( C \) = number of classes
      \item \( y_i \) = true label
      \item \( \hat{y}_i \) = predicted probability for class \( i \)
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example of Cross-Entropy Loss}
  Consider a binary classification scenario:
  \begin{itemize}
    \item True class: 1
    \item Predicted probability: 0.9
  \end{itemize}
  \begin{block}{Calculation}
    \begin{equation}
      \text{BCE} = - (1 \times \log(0.9) + 0 \times \log(0.1)) = -\log(0.9) \approx 0.105
    \end{equation}
  \end{block}

  \begin{block}{Key Points}
    \begin{itemize}
      \item **Interpretability**: Outputs represent probabilities—suitable for assessing multi-class predictions.
      \item **Sensitivity**: Minimizes the divergence between true labels and predicted probabilities.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Loss functions play a critical role in training neural networks by quantifying prediction errors. Using MSE for regression and Cross-Entropy for classification allows models to learn effectively and improve their prediction accuracy.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Next Steps}
  In the upcoming slide, we will discuss \textbf{Backpropagation}, the mechanism through which neural networks learn from their errors as measured by these loss functions.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Backpropagation - Overview}
  \begin{block}{Explanation of Backpropagation}
      Backpropagation is a fundamental algorithm used for training neural networks, enabling the model to learn from its errors and adjust weights to minimize the loss function.
  \end{block}
  
  \begin{itemize}
      \item Utilizes the chain rule from calculus to compute gradients.
      \item Minimizes the loss function through weight adjustments.
      \item Essential for effective training of neural networks.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Backpropagation - Steps}
  \begin{enumerate}
      \item \textbf{Forward Pass}: Calculate predictions from input data.
      \item \textbf{Compute Loss}: Compare predicted and actual outputs.
      \item \textbf{Backward Pass}: Compute gradients for each weight using:
      \begin{equation}
      \frac{\partial L}{\partial w} = \frac{\partial L}{\partial Y} \cdot \frac{\partial Y}{\partial w}
      \end{equation}
      \item \textbf{Update Weights}: Adjust weights to minimize loss:
      \begin{equation}
      w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}
      \end{equation}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Backpropagation - Importance}
  \begin{itemize}
      \item \textbf{Efficiency}: Reduces computational complexity, enabling training of deep networks.
      \item \textbf{Flexibility}: Applicable to various network architectures (e.g., feedforward, CNN).
      \item \textbf{Improvement}: Systematically updates weights to enhance model performance.
  \end{itemize}
  
  \begin{block}{Key Points}
      \begin{itemize}
          \item Relies on gradients and chain rule.
          \item Two main phases: forward and backward passes.
          \item Understanding loss functions is crucial for its effectiveness.
      \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Training Neural Networks - Overview}
    \begin{block}{Overview of the Training Process}
        Training a neural network involves converting raw input data into meaningful predictions through supervised learning with labeled training data.
    \end{block}
    
    \begin{block}{Key Steps in Training Neural Networks}
        \begin{enumerate}
            \item \textbf{Initialization}: Set network weights and biases randomly.
            \item \textbf{Forward Pass}: Feed input data and compute outputs using activation functions.
            \item \textbf{Loss Calculation}: Evaluate the output against the target using a loss function.
            \item \textbf{Backpropagation}: Compute gradients of the loss using the chain rule.
            \item \textbf{Weight Updates}: Update weights using an optimization algorithm.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Algorithms}
    \begin{block}{Introduction to Optimization Algorithms}
        Optimization algorithms adjust weights to minimize loss. Among these, \textbf{Stochastic Gradient Descent (SGD)} is the most prevalent.
    \end{block}
    
    \begin{block}{Stochastic Gradient Descent (SGD)}
        \begin{itemize}
            \item \textbf{Concept}: Uses a mini-batch of the dataset for weight updates.
            \item \textbf{Update Rule}:
            \begin{equation}
                w = w - \eta \nabla L(w)
            \end{equation}
            \begin{itemize}
                \item \(w\): weight vector
                \item \(\eta\): learning rate
                \item \(\nabla L(w)\): gradient of the loss
            \end{itemize}
            \item \textbf{Advantages}:
              \begin{itemize}
                \item Faster convergence
                \item Better generalization due to noise
              \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Optimization Techniques}
    \begin{block}{Advanced Techniques}
        \begin{enumerate}
            \item \textbf{Momentum}: Accelerates SGD by incorporating previous gradients.
            \item \textbf{Adaptive Methods}: E.g., Adam and RMSprop adapt the learning rate based on past gradients.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The training process is iterative, involving multiple steps.
            \item SGD is foundational for effective training with mini-batches.
            \item Proper tuning of hyperparameters, such as the learning rate, is crucial for performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hyperparameter Tuning}
    \begin{block}{Importance of Hyperparameters in Neural Networks}
        Hyperparameters are settings that govern the training process and the structure of neural networks. 
        \begin{itemize}
            \item They differ from parameters (weights and biases) which are learned during training.
            \item Controlled before training, they impact the learning process significantly.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Why Hyperparameters Matter}
    \begin{itemize}
        \item \textbf{Model Performance:} 
        The right combination of hyperparameters can improve accuracy, speed up convergence, and prevent overfitting.
        
        \item \textbf{Complexity Management:} 
        Help balance model complexity (e.g., number of layers/nodes) and generalization on unseen data.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Common Hyperparameters to Tune}
    \begin{enumerate}
        \item \textbf{Learning Rate ($\alpha$):}
            \begin{itemize}
                \item Controls weight adjustment during training.
                \item Too large: Divergence; Too small: Slow convergence.
                \item \textit{Example:} Start with $\alpha = 0.01$.
            \end{itemize}
        \item \textbf{Number of Layers and Nodes:}
            \begin{itemize}
                \item More layers/nodes capture complexity but may lead to overfitting.
                \item \textit{Illustration:} A shallow network vs. a deep network's performance.
            \end{itemize}
        \item \textbf{Batch Size:}
            \begin{itemize}
                \item Influences learning speed and model quality.
                \item \textit{Example:} Mini-batch sizes of 32 vs. 256.
            \end{itemize}
        \item \textbf{Activation Functions:}
            \begin{itemize}
                \item Functions like ReLU, Sigmoid that affect neuron output.
            \end{itemize}
        \item \textbf{Dropout Rate:}
            \begin{itemize}
                \item Specifies the proportion of ignored neurons during training.
                \item \textit{Example:} 0.5 dropout rate means 50\% of neurons are ignored.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Strategies for Hyperparameter Tuning}
    \begin{enumerate}
        \item \textbf{Grid Search:}
            \begin{itemize}
                \item Exhaustively search through a defined parameter grid.
                \item \textit{Advantage:} Simple and thorough; \textit{Disadvantage:} Computationally expensive.
            \end{itemize}
        \item \textbf{Random Search:}
            \begin{itemize}
                \item Randomly sample hyperparameters from distributions.
                \item \textit{Advantage:} More efficient; often finds good parameters faster.
            \end{itemize}
        \item \textbf{Bayesian Optimization:}
            \begin{itemize}
                \item Uses a probabilistic model to predict best hyperparameters.
                \item \textit{Advantage:} Optimizes discovery while reducing evaluations.
            \end{itemize}
        \item \textbf{Cross-Validation:}
            \begin{itemize}
                \item Assess model performance by dividing data into subsets for training and validation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet: Hyperparameter Tuning with Grid Search}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

# Define a function to create the model
def create_model(learning_rate):
    model = Sequential()
    model.add(Dense(12, input_dim=8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Grid search parameters
param_grid = {'learning_rate': [0.001, 0.01, 0.1]}
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=-1)

# Fit model with grid search
grid_result = grid.fit(X_train, y_train)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Hyperparameter tuning is critical for achieving optimal model performance.
            \item Different tuning strategies have pros and cons; choices depend on resources and time.
            \item Continuous monitoring and adjustments are necessary as model performance metrics change.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Regularization Techniques}
  \begin{block}{Introduction to Regularization in Neural Networks}
    Regularization is a set of techniques used to reduce overfitting in machine learning models, particularly in neural networks. 
    Overfitting occurs when the model learns noise in the training data rather than the underlying patterns, leading to poor generalization on unseen data. 
    Regularization helps to ensure that the model remains simple and generalizes well.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Key Regularization Techniques}
  \begin{enumerate}
    \item \textbf{Dropout}
      \begin{itemize}
        \item \textbf{Concept:} Randomly "drop out" a proportion of neurons in the network (typically 20-50%) during training to improve robustness.
        \item \textbf{Implementation:}
        \begin{lstlisting}[language=python]
from keras.models import Sequential
from keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate
        \end{lstlisting}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Key Regularization Techniques (cont'd)}
  \begin{enumerate}
    \setcounter{enumi}{1} % Continue numbering from previous frame
    \item \textbf{L2 Regularization (Weight Decay)}
      \begin{itemize}
        \item \textbf{Concept:} Adds a penalty to the loss function based on the sum of the squares of the weights to discourage overly complex models.
        \item \textbf{Formulation:}
          \begin{equation}
            L = L_{original} + \lambda \sum_{i} w_i^2
          \end{equation}
        \item \textbf{Implementation:}
        \begin{lstlisting}[language=python]
from keras.regularizers import l2

model.add(Dense(128, kernel_regularizer=l2(0.01), activation='relu'))
        \end{lstlisting}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Avoid Overfitting:} Regularization techniques help balance bias and variance.
    \item \textbf{Performance Boost:} Dropout and L2 regularization improve performance on unseen data.
    \item \textbf{Hyperparameter Tuning:} Optimize regularization parameters as part of hyperparameter tuning.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  Implementing regularization techniques like dropout and L2 regularization is essential in building robust neural networks that generalize well. 
  Mastering these techniques will enhance your ability to design effective models in supervised learning tasks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation - Introduction}
    \begin{block}{Overview}
        When building neural networks, it's crucial to assess their performance accurately. Evaluating a model helps determine how well it generalizes to unseen data. The following are some standard metrics used to evaluate neural networks:
    \end{block}
    \begin{itemize}
        \item \textbf{Accuracy}
        \item \textbf{Precision}
        \item \textbf{F1 Score}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation - Key Concepts}
    \begin{enumerate}
        \item \textbf{Accuracy}
            \begin{itemize}
                \item \textbf{Definition}: The proportion of true results (both true positives and true negatives) among the total number of cases examined.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
                \end{equation}
            \end{itemize}
        \item \textbf{Precision}
            \begin{itemize}
                \item \textbf{Definition}: The ratio of correctly predicted positive observations to the total predicted positives.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
                \end{equation}
            \end{itemize}
        \item \textbf{F1 Score}
            \begin{itemize}
                \item \textbf{Definition}: The harmonic mean of precision and recall.
                \item \textbf{Formula}:
                \begin{equation}
                    \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                \end{equation}
                \item \textbf{Recall} can be defined as:
                \begin{equation}
                    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
                \end{equation}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation - Key Points and Summary}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Accuracy is best used with balanced datasets; it may be misleading in cases of class imbalance.
            \item Precision is critical when the cost of false positives is high.
            \item F1 Score is valuable when seeking a balance between precision and recall.
        \end{itemize}
    \end{block}
    \begin{block}{Summary}
        Model evaluation is crucial for understanding the performance of neural networks. Using metrics like Accuracy, Precision, and F1 Score provides insights into how well the model will perform in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Implementation - Part 1}
    \begin{block}{Introduction to Neural Networks}
        Neural Networks are a subset of machine learning models inspired by the human brain. They consist of interconnected units (neurons) that process data in layers.
    \end{block}

    \begin{itemize}
        \item \textbf{Key Components}:
        \begin{itemize}
            \item \textbf{Input Layer}: Receives input data.
            \item \textbf{Hidden Layers}: Perform computations and feature extraction.
            \item \textbf{Output Layer}: Produces the final output (e.g., classification).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Implementation - Part 2}
    \textbf{Steps in Implementation}:
    
    \begin{enumerate}
        \item \textbf{Install Required Libraries}
        \begin{lstlisting}[language=bash]
pip install tensorflow
        \end{lstlisting}

        \item \textbf{Import Libraries}
        \begin{lstlisting}[language=python]
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
        \end{lstlisting}

        \item \textbf{Load and Preprocess Data}
        \begin{lstlisting}[language=python]
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train = X_train.reshape((60000, 28 * 28)).astype('float32') / 255
X_test = X_test.reshape((10000, 28 * 28)).astype('float32') / 255
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Practical Implementation - Part 3}
    \textbf{Constructing the Neural Network}:

    \begin{enumerate}
        \setcounter{enumi}{3} % continue from previous frame
        \item \textbf{Build the Neural Network Model}
        \begin{lstlisting}[language=python]
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(28 * 28,)),  # Hidden layer
    layers.Dense(10, activation='softmax')  # Output layer
])
        \end{lstlisting}

        \item \textbf{Compile the Model}
        \begin{lstlisting}[language=python]
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
        \end{lstlisting}

        \item \textbf{Train the Model}
        \begin{lstlisting}[language=python]
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)
        \end{lstlisting}

        \item \textbf{Evaluate the Model}
        \begin{lstlisting}[language=python]
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_accuracy:.4f}')
        \end{lstlisting}
    \end{enumerate}
    
    \textbf{Key Points to Emphasize}:
    \begin{itemize}
        \item Requires careful tuning for optimal performance.
        \item Activation functions like ReLU and Softmax introduce non-linearity.
        \item Training involves adjusting weights through backpropagation.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies: Real-World Applications of Neural Networks}
    \begin{block}{Introduction}
        Neural networks are powerful computational models designed to recognize patterns in data, mimicking human brain function. They are widely used due to their ability to learn from large datasets.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Areas of Application}
    \begin{enumerate}
        \item \textbf{Image Recognition}
            \begin{itemize}
                \item Definition: Identifying and classifying objects in images.
                \item Application Examples:
                    \begin{itemize}
                        \item \textbf{Facial Recognition:} Systems like Apple’s Face ID and Facebook tagging use convolutional neural networks (CNNs).
                        \item \textbf{Medical Imaging:} Assists radiologists in detecting anomalies in MRI scans, enhancing diagnostic accuracy.
                    \end{itemize}
                \item Key Point: CNNs excel in efficient pixel data processing.
            \end{itemize}

        \item \textbf{Speech Recognition}
            \begin{itemize}
                \item Definition: Technology allowing computers to understand human speech.
                \item Application Examples:
                    \begin{itemize}
                        \item \textbf{Virtual Assistants:} Amazon Alexa and Google Assistant use recurrent neural networks (RNNs) for speech recognition.
                        \item \textbf{Transcription Services:} Neural networks enhance automated transcription accuracy using large voice data.
                    \end{itemize}
                \item Key Point: RNNs and Long Short-Term Memory (LSTM) models capture temporal language dependencies.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Natural Language Processing (NLP) and Success Stories}
    \begin{itemize}
        \item \textbf{Natural Language Processing (NLP)}
            \begin{itemize}
                \item Definition: Interaction between computers and humans using natural language.
                \item Application Examples:
                    \begin{itemize}
                        \item \textbf{Chatbots:} Neural network-based systems for customer service.
                        \item \textbf{Translation Services:} Google Translate uses neural networks for improved real-time translations.
                    \end{itemize}
                \item Key Point: Transformer models like BERT and GPT revolutionize text context understanding.
            \end{itemize}

        \item \textbf{Notable Success Stories:}
            \begin{itemize}
                \item \textbf{ImageNet Challenge:} CNNs significantly reduced error rates in image classification.
                \item \textbf{DeepMind's AlphaGo:} Showcased deep learning in complex games, blending reinforcement and supervised learning.
            \end{itemize}
    \end{itemize}

    \begin{block}{Conclusion}
        Neural networks have transformed industries, enabling machines to perform human-like tasks, illustrating the versatility of supervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges and Limitations of Neural Networks}
    \begin{block}{Introduction to Challenges}
        Neural networks have revolutionized many fields, but they are not without their challenges and limitations. Understanding these challenges is crucial for effectively applying neural networks in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Requirements}
    \begin{itemize}
        \item \textbf{Large Datasets}: 
        \begin{itemize}
            \item Neural networks often require large volumes of data to train effectively due to numerous parameters (weights and biases).
            \item \textit{Example}: A convolutional neural network (CNN) may need thousands of labeled images to generalize well.
        \end{itemize}
        
        \item \textbf{Data Quality}: 
        \begin{itemize}
            \item The quality of data is as crucial as quantity. Poor quality data can lead to subpar model performance.
            \item \textit{Illustration}: Training a facial recognition model with varied lighting, angles, or obstructions can severely limit effectiveness.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpretability Issues and Other Challenges}
    \begin{itemize}
        \item \textbf{Interpretability Issues}:
        \begin{itemize}
            \item Neural networks are often seen as "black boxes" with complex and non-transparent decision-making processes.
            \item \textit{Key Point}: Understanding the reasoning behind predictions can be problematic in high-stakes fields like healthcare or finance.
        \end{itemize}
        
        \item \textbf{Need for Explainability}:
        \begin{itemize}
            \item Stakeholders often require explanations, particularly in critical applications, offering transparency in decision-making.
            \item \textit{Example}: A neural network predicting a disease must provide insights into influencing factors for trust.
        \end{itemize}

        \item \textbf{Other Challenges}:
        \begin{itemize}
            \item \textbf{Training Time and Resources}: Training can be resource-intensive, requiring powerful hardware.
            \item \textbf{Hyperparameter Tuning}: Finding optimal combinations can be complex and require expertise.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Neural networks thrive on large, high-quality datasets.
            \item The complexity of neural networks raises issues of interpretability.
            \item Significant computational resources are required, and effective hyperparameter tuning is essential.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        While neural networks offer unprecedented performance, addressing their challenges is crucial for building reliable and interpretable models to effectively solve real-world problems.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Future Directions - Key Takeaways}
  
  \begin{block}{Neural Networks Basics}
    \begin{itemize}
      \item Computational models inspired by the human brain.
      \item Designed for pattern recognition, including tasks such as classification and regression.
      \item Composed of:
      \begin{itemize}
        \item Input layers (feature input)
        \item Hidden layers (processing)
        \item Output layers (final classifications or predictions)
      \end{itemize}
    \end{itemize}
  \end{block}
  
  \begin{block}{Training Process}
    \begin{itemize}
      \item Learn using backpropagation to adjust weights based on prediction errors.
      \item Objective: minimize the loss function, quantifying difference between predicted and actual values.
      \item Common Loss Functions:
      \begin{itemize}
        \item Mean Squared Error (MSE) for regression tasks:
          \begin{equation}
          \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
          \end{equation}
        \item Cross-Entropy Loss for classification tasks:
          \begin{equation}
          \text{Loss} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
          \end{equation}
      \end{itemize}
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Future Directions - Challenges and Directions}
  
  \begin{block}{Challenges Identified}
    \begin{itemize}
      \item Data Requirements: Large sets of high-quality labeled data are crucial.
      \item Interpretability: Understanding decisions made by neural networks is complex.
    \end{itemize}
  \end{block}
  
  \begin{block}{Future Directions}
    \begin{itemize}
      \item Explainable AI (XAI): Develop methods like Layer-wise Relevance Propagation (LRP) for better model interpretability.
      \item Efficiency and Scalability: Create efficient architectures such as Transformers and enhance hardware optimizations.
      \item Transfer Learning: Use pre-trained models to accelerate learning in domain-specific tasks.
      \item Ethics and Responsible AI: Establish guidelines addressing privacy, bias, and transparency.
      \item Integration with Other Technologies: Combine with reinforcement learning, edge computing, and IoT for advanced applications.
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}

  As neural network technologies evolve, they will significantly enhance various fields such as healthcare, finance, and autonomous systems. Recognizing their complexities and challenges positions us to optimize future developments, ensuring advancements that are both powerful and equitable.

\end{frame}


\end{document}