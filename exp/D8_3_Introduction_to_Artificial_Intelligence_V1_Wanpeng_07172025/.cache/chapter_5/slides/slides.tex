\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5]{Week 5: Data Handling \& Management}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Handling \& Management}
    \begin{block}{Understanding Data Handling \& Management}
        Data handling and management are crucial processes in any Artificial Intelligence (AI) project. Effective management of datasets can significantly influence the performance and reliability of AI systems. 
    \end{block}
    \begin{block}{Steps Involved}
        \begin{itemize}
            \item **Collecting** datasets
            \item **Cleaning** datasets
            \item **Preprocessing** datasets
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Collecting Data}
    Collecting data involves gathering raw information from various sources. This data can be structured (like databases) or unstructured (like text documents, images, or videos). 

    \begin{block}{Key Points}
        \begin{itemize}
            \item Use diverse sources to ensure comprehensive datasets (e.g., surveys, APIs, public databases).
            \item Quality of collected data directly impacts model performance.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Collecting sales data from a retail store's transactions, customer feedback, and web interaction logs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Cleaning Data}
    Cleaning data means identifying and correcting errors or inconsistencies in the dataset. This step is vital as it eliminates noise that can distort analysis or training outcomes.

    \begin{block}{Key Points}
        \begin{itemize}
            \item Handle missing values (e.g., imputation or removal).
            \item Correct erroneous entries (like typos in categorical data).
            \item Remove duplicates to avoid biased results.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        If a dataset shows "5000" as an entry in a field meant for temperature (assuming valid range is in °C), it might need revisiting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Preprocessing Data}
    Preprocessing prepares the cleaned data for analysis or modeling. This may involve normalization, encoding categorical variables, and splitting datasets into training and testing sets.

    \begin{block}{Key Points}
        \begin{itemize}
            \item Normalization ensures that different scales do not distort model training.
            \item Categorical variables need encoding to convert them into a numerical format for algorithms.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Turning categorical age groups (e.g., "Young," "Middle-aged," "Senior") into corresponding numerical codes (0, 1, 2) for model input.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance and Conclusion}
    Effective data handling helps in:
    \begin{itemize}
        \item Improving model accuracy and performance.
        \item Minimizing bias and ensuring fairness in AI outputs.
        \item Streamlining the workflow, making it reproducible and transparent.
    \end{itemize}

    \begin{block}{Conclusion}
        A good foundational understanding of data handling and management is essential for any AI practitioner. Mastery of these steps leads to robust and reliable AI solutions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python Code Snippet for Preprocessing}
    \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
import pandas as pd

# Load your dataset
data = pd.read_csv('data.csv')

# Fill missing values with the mean (for numerical columns)
data['age'].fillna(data['age'].mean(), inplace=True)

# Encode categorical variables
data['gender'] = data['gender'].map({'Male': 0, 'Female': 1})
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 1}
    \begin{block}{Learning Objectives for Data Handling \& Management in AI Projects}
        \begin{enumerate}
            \item \textbf{Understand the Importance of Data Handling:}
            \begin{itemize}
                \item Grasp the foundational role of effective data handling in the success of AI projects.
                \item Recognize that high-quality data is critical for training accurate models and achieving reliable outcomes.
            \end{itemize}
            \textbf{Key Point:} Poor data quality compromises model performance.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Explore Data Collection Techniques:}
        \begin{itemize}
            \item \textbf{Surveys:} Gathering information through questionnaires, both online and offline.
            \item \textbf{Web Scraping:} Extracting data from websites using tools and programming languages (e.g., Python packages like BeautifulSoup).
            \item \textbf{Open Datasets:} Utilizing publicly available datasets from platforms such as Kaggle, UCI Machine Learning Repository, etc.
        \end{itemize}
        \textbf{Illustration:} Flowchart showing data collection methods and their sources.
        
        \item \textbf{Learn Data Preprocessing Steps:}
        \begin{itemize}
            \item \textbf{Data Cleaning:} Remove duplicates, handle missing values, and correct inconsistencies.
            \item \textbf{Data Transformation:} Normalize or standardize data to bring different features onto the same scale.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives - Part 3}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Assess Ethical Considerations in Data Handling:}
        \begin{itemize}
            \item Recognize the ethical implications associated with data usage, including:
            \begin{itemize}
                \item Privacy concerns related to personal data.
                \item Bias in data collection and its impact on AI model fairness.
            \end{itemize}
            \textbf{Discussion Point:} Consider the potential consequences of using biased datasets in training models.
        \end{itemize}
        
        \item \textbf{Implement Data Management Best Practices:}
        \begin{itemize}
            \item Explore frameworks for data management, focusing on:
            \begin{itemize}
                \item \textbf{Data Versioning:} Track changes over time to ensure reproducibility.
                \item \textbf{Documentation:} Maintain clear records of data sources, preprocessing steps, and transformations applied.
            \end{itemize}
            \textbf{Best Practice:} Use tools like Git for version control of datasets.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Data Collection Techniques - Overview}
    % Introduction to data collection techniques
    Data collection is a vital step in the data handling and management process. The method chosen influences the quality, reliability, and relevance of the data obtained, impacting outcomes in analysis or AI projects. 
    \begin{itemize}
        \item Three primary techniques: 
        \begin{itemize}
            \item Surveys
            \item Web Scraping
            \item Open Datasets
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Data Collection Techniques - Surveys}
    % Overview of Surveys as a data collection technique
    \begin{block}{Surveys}
        Surveys involve collecting data from a predefined group of respondents using structured questionnaires.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Purpose}: Ideal for gathering qualitative and quantitative data, gaining insights into opinions, behaviors, or demographics.
        \item \textbf{Types}:
        \begin{itemize}
            \item Online Surveys (e.g., Google Forms, SurveyMonkey)
            \item Telephone Surveys
            \item Face-to-Face Interviews
        \end{itemize}
        \item \textbf{Example}: A company wants customer feedback on a new product; they deploy an online survey with questions about customer satisfaction and product features.
    \end{itemize}
    
    \textbf{Key Points}:
    \begin{itemize}
        \item Can yield rich data but depends on the design of the questionnaire.
        \item Response bias can occur based on question framing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Web Scraping}
    % Overview of Web Scraping as a data collection technique
    \begin{block}{Web Scraping}
        Web scraping is the automated extraction of data from websites using scripts and software tools.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Purpose}: Useful for collecting large volumes of data from freely available online sources, especially where traditional data collection methods are not feasible.
        \item \textbf{Implementation}: Requires knowledge of programming languages (e.g., Python).
    \end{itemize}
    
    \textbf{Example Code}:
    \begin{lstlisting}[language=Python]
import requests
from bs4 import BeautifulSoup

URL = "https://example.com"
page = requests.get(URL)
soup = BeautifulSoup(page.content, "html.parser")

# Extracting titles from articles
titles = soup.find_all('h2')
for title in titles:
    print(title.text)
    \end{lstlisting}
    
    \textbf{Key Points}:
    \begin{itemize}
        \item Enables access to real-time data but must comply with the website's terms of service.
        \item Challenges include dealing with bot detection and unstructured data formats.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality}
    
    \begin{block}{1. Understanding Data Quality}
        Data quality refers to the condition of a set of values of qualitative or quantitative variables.
        High-quality data is critical for effective decision-making, particularly in AI applications, where the integrity and reliability of data directly influence outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Impact on AI Outcomes}
    
    \begin{itemize}
        \item \textbf{Accuracy}: High-quality data leads to accurate models. 
        \begin{itemize}
            \item Flawed data, such as incorrect labels, results in flawed predictions (e.g., spam filters).
        \end{itemize}
        
        \item \textbf{Completeness}: Models require complete datasets to function effectively.
        \begin{itemize}
            \item Missing values can bias results (e.g., missing demographic info in healthcare datasets). 
        \end{itemize}
        
        \item \textbf{Consistency}: Data from different sources must be consistent.
        \begin{itemize}
            \item Discrepancies reduce model trustworthiness (e.g., differing date formats).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Data Quality - Examples and Key Points}

    \begin{block}{3. Examples of Poor Data Quality Effects}
        \begin{enumerate}
            \item E-commerce company using inconsistent user ratings leading to irrelevant product suggestions.
            \item Predictive maintenance using noisy sensor data resulting in false alerts and unnecessary costs.
        \end{enumerate}
    \end{block}

    \begin{block}{4. Key Points to Emphasize}
        \begin{itemize}
            \item Improved data quality correlates with enhanced model performance (accuracy, precision, recall).
            \item Poor data can lead to costly business implications and reputation damage.
            \item Continuous monitoring of data quality is vital throughout the AI system lifecycle.
        \end{itemize}
    \end{block}

    \begin{block}{5. Conclusion}
        Investing in data quality ensures robust and reliable AI models, leading to better business and operational outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Introduction}
  % Introduction to the importance of data cleaning
  Data cleaning is an essential step in the data preprocessing phase that ensures the integrity, accuracy, and usability of your dataset. A clean dataset is crucial for effective analysis and high-performing AI models.
  
  % Importance of data quality
  \begin{block}{Key Point}
    Data quality directly influences AI outcomes and model performance.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Key Steps}
  % Overview of key steps in data cleaning
  \begin{enumerate}
    \item Handling Missing Values
    \item Identifying and Removing Duplicates
    \item Detecting and Handling Outliers
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Handling Missing Values}
  % Methods for handling missing values
  There are several ways to handle missing values:
  
  \begin{itemize}
    \item \textbf{Deletion}: Remove records with missing data.
    \begin{block}{Example}
      If a survey respondent left the age question blank, that entry can be discarded.
    \end{block}
    
    \item \textbf{Imputation}: Fill in missing data using statistical methods.
    \begin{itemize}
      \item \textit{Mean/Median Imputation}: Replace missing values with the mean or median.
      \begin{equation}
        \text{Value}_{\text{new}} = \frac{\sum \text{Value}}{n}
      \end{equation}
      \item \textit{Predictive Imputation}: Use algorithms to predict missing values.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Identifying Duplicates}
  % Identifying and removing duplicates
  Duplicate records can skew your analysis. Use the following methods:
  
  \begin{itemize}
    \item \textbf{Exact Matching}: Identify and remove rows that are identical across all columns.
    \begin{block}{Example}
      Multiple entries for the same transaction can create misleading insights.
    \end{block}
    \item \textbf{Fuzzy Matching}: Use algorithms to identify approximate matches (useful in text data).
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Handling Outliers}
  % Detecting and handling outliers
  Outliers can distort statistical analyses. Methods include:
  
  \begin{itemize}
    \item \textbf{Statistical Methods}:
    \begin{itemize}
      \item \textit{Z-score Method}:
      \begin{equation}
        Z = \frac{(X - \mu)}{\sigma}
      \end{equation}
      A data point is considered an outlier if |Z| > 3.
      
      \item \textit{IQR Method}: Outliers are those beyond 1.5 * IQR.
      \begin{equation}
        \text{Outlier} = Q_1 - 1.5 \times IQR \text{ or } Q_3 + 1.5 \times IQR
      \end{equation}
    \end{itemize}
    \item \textbf{Capping}: Replace extreme outliers with less extreme values.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Data Cleaning Processes - Conclusion}
  % Conclusion emphasizing the importance of data cleaning
  Effective data cleaning processes help ensure that the input data is reliable, improving the reliability of any analysis or AI model output. Remember:
  
  \begin{block}{Final Points}
    \begin{itemize}
      \item Establish a systematic approach to data cleaning.
      \item Methods depend on the nature of the data and analysis goals.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
    \frametitle{Preprocessing Data for AI}
    \begin{block}{Description}
        This slide explains essential techniques for preparing data for Artificial Intelligence (AI) applications, specifically focusing on normalization, scaling, and encoding categorical variables.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts: Normalization}
    \begin{itemize}
        \item \textbf{Definition}: Adjusting values in a dataset to a common scale, typically 0 to 1.
        \item \textbf{Why Normalize?}: Reduces bias towards variables with larger ranges; essential for algorithms like K-Means clustering and Neural Networks.
        \item \textbf{How to Normalize}:
            \begin{itemize}
                \item \textbf{Min-Max Normalization}:
                \begin{equation*}
                x' = \frac{x - \text{min}}{\text{max} - \text{min}}
                \end{equation*}
                \item \textbf{Example}: For ages ranging from 15 to 100:
                \begin{equation*}
                25' = \frac{25 - 15}{100 - 15} = \frac{10}{85} \approx 0.118
                \end{equation*}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts: Scaling}
    \begin{itemize}
        \item \textbf{Definition}: Adjusts the range of individual data features to the same scale without distorting differences in ranges.
        \item \textbf{Types of Scaling}:
            \begin{itemize}
                \item \textbf{Standardization (Z-score Scaling)}:
                \begin{equation*}
                z = \frac{x - \mu}{\sigma}
                \end{equation*}
                where $\mu$ is the mean and $\sigma$ is the standard deviation.
                \item \textbf{Example}: For height with a mean of 175 cm and standard deviation of 10 cm:
                \begin{equation*}
                z = \frac{180 - 175}{10} = 0.5
                \end{equation*}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Concepts: Encoding Categorical Variables}
    \begin{itemize}
        \item \textbf{Definition}: Transforming categorical labels into a numerical format for AI algorithms.
        \item \textbf{Types of Encoding}:
            \begin{itemize}
                \item \textbf{One-Hot Encoding}: Converts categorical values into a binary matrix.
                \item \textbf{Example}:
                \begin{itemize}
                    \item "Red" → [1, 0, 0]
                    \item "Blue" → [0, 1, 0]
                    \item "Green" → [0, 0, 1]
                \end{itemize}
                \item \textbf{Label Encoding}: Assigns each unique category an integer value.
                \item \textbf{Example}:
                \begin{itemize}
                    \item "Red" → 1
                    \item "Blue" → 2
                    \item "Green" → 3
                \end{itemize}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet (Python with Scikit-learn)}
    \begin{lstlisting}[language=Python]
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
import pandas as pd

# Sample data
data = pd.DataFrame({'Feature1': [25, 50, 75], 'Category': ['Red', 'Blue', 'Green']})

# Normalization
scaler = MinMaxScaler()
data['Normalized'] = scaler.fit_transform(data[['Feature1']])

# Standardization
standard_scaler = StandardScaler()
data['Standardized'] = standard_scaler.fit_transform(data[['Feature1']])

# One-Hot Encoding
encoder = OneHotEncoder()
encoded = encoder.fit_transform(data[['Category']]).toarray()

# Display results
print(data)
print(encoded)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Preprocessing is crucial for AI models; proper techniques significantly influence outcomes.
        \item Select appropriate preprocessing techniques based on data context.
        \item Visualize and analyze data to determine the most effective methods.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Handling - Introduction}
    \begin{itemize}
        \item Ethical data handling is paramount in AI.
        \item The standards in managing data influence AI outcomes and user trust.
        \item This slide outlines key ethical standards and best practices.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Handling - Key Concepts}
    \begin{enumerate}
        \item \textbf{Data Privacy}
            \begin{itemize}
                \item Protecting personally identifiable information (PII) from unauthorized access.
                \item Compliance with laws such as GDPR and HIPAA.
            \end{itemize}
        \item \textbf{Data Integrity}
            \begin{itemize}
                \item Maintaining accuracy and consistency of data.
                \item Example: Implementing validation checks during data entry.
            \end{itemize}
        \item \textbf{Bias Mitigation}
            \begin{itemize}
                \item Recognizing and addressing biases in datasets.
                \item Example: Conducting fairness audits for demographic representation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices in Data Handling - Best Practices}
    \begin{itemize}
        \item \textbf{Conduct Regular Audits:} Periodically review data handling processes.
        \item \textbf{Educate and Train Staff:} Provide training on ethical data handling.
        \item \textbf{Implement Robust Security Measures:} Use encryption and access controls to safeguard data.
        \item \textbf{Create Comprehensive Data Policies:} Develop clear data management policies.
    \end{itemize}
    \begin{block}{Conclusion}
        Following these best practices builds trust with users and enhances AI systems' effectiveness.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Data Management in Real World AI}
    \begin{block}{Introduction to Effective Data Management in AI}
        In AI, the quality and management of data are crucial. Effective data management ensures ethical data use and enhances AI model performance. We will analyze a well-known case study to illustrate these concepts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Google Photos}
    \begin{block}{Background}
        Google Photos is a cloud-based photo storage service that uses advanced AI algorithms for image recognition and organization. With billions of photos uploaded, efficient data handling is key to its success.
    \end{block}

    \begin{block}{Key Aspects of Data Management}
        \begin{enumerate}
            \item \textbf{Data Collection:}
            \begin{itemize}
                \item Diversity of Data: Variety from smartphones, cameras, and apps.
                \item User-generated Content: Reflects real-world variations (lighting, angles).
            \end{itemize}
            \item \textbf{Data Cleaning:}
            \begin{itemize}
                \item Removing Irrelevant Data: Filter out duplicates and low-quality images.
                \item Ethical Considerations: Anonymization to protect user identity.
            \end{itemize}
            \item \textbf{Data Annotation:}
            \begin{itemize}
                \item Use of AI for Labeling: Machine learning tags images based on detected objects.
                \item Human Review: Ensures accuracy in predictions.
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Impact on AI Model Success}
    \begin{itemize}
        \item \textbf{Improved Accuracy:} High accuracy in image recognition; identifying thousands of objects with precision.
        \item \textbf{User Experience Enhancement:} Features like “Automatic Album Creation” and “Search by Keywords” improve interaction due to robust data management.
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Quality Over Quantity: Focus on data quality instead of maximizing data volume.
            \item Ethical Responsibility: Complies with regulations and builds user trust.
            \item Iterative Improvement: Continuous data cleaning leads to ongoing improvements in AI models.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Google Photos exemplifies effective data management's role in AI success. Quality, ethical usage, and a combination of automated and human processes advance AI performance.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hands-on Lab: Data Cleaning Exercise}
    \begin{itemize}
        \item Interactive session to practice data cleaning techniques on a provided dataset.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Objectives of the Session}
    \begin{itemize}
        \item \textbf{Understand Data Cleaning:} 
            Learn the importance of data cleaning in ensuring data quality and integrity.
        \item \textbf{Practical Skills:} 
            Apply data cleaning techniques in a hands-on environment.
        \item \textbf{Tool Proficiency:} 
            Gain experience using data manipulation tools or programming languages (e.g., Python with Pandas).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What is Data Cleaning?}
    Data cleaning involves identifying and rectifying errors and inconsistencies in data. 
    \begin{itemize}
        \item Ensures data is accurate, complete, and usable for analysis.
        \item Improves the reliability of AI models.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Steps in Data Cleaning}
    \begin{enumerate}
        \item \textbf{Removing Duplicates:} 
            Remove duplicated records that may skew analysis.
        \item \textbf{Handling Missing Values:} 
            - \textit{Imputation:} Replacing missing values (mean, median).
            - \textit{Removal:} Excluding records with missing values.
        \item \textbf{Standardizing Data:} 
            Ensure consistency in data formats (e.g., YYYY-MM-DD for dates).
        \item \textbf{Outlier Detection:} 
            Identify outliers using methods like Z-scores.
        \item \textbf{Data Type Conversion:} 
            Ensure correct data types for analysis.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Values Example}
    \begin{block}{Python Code Example}
    \begin{lstlisting}[language=Python]
import pandas as pd

# Example of filling missing values with mean
df['column_name'].fillna(df['column_name'].mean(), inplace=True)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Practical Exercise Overview}
    \begin{itemize}
        \item \textbf{Dataset Distribution:} 
            Students will receive a dataset with pre-introduced issues (duplicates, missing values, etc.).
        \item \textbf{Tools to Use:} 
            Pandas library (Python), spreadsheets, or other data cleaning software.
        \item \textbf{Guided Tasks:} 
            \begin{enumerate}
                \item Inspect the dataset for duplicates and missing values.
                \item Apply appropriate cleaning techniques (removal, imputation, standardization).
                \item Share results and reflect on choices made.
            \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Data cleaning is critical for the success of data-based projects and AI models.
        \item Effectively cleaned data leads to better insights and improved decision-making.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    This hands-on lab provides an opportunity to directly engage with the materials and concepts introduced in previous chapters. 
    \begin{itemize}
        \item Practicing data cleaning techniques enhances understanding of real-world data handling.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Questions to Consider During Exercise}
    \begin{itemize}
        \item What challenges did you encounter while cleaning the dataset?
        \item How did your approach to data cleaning affect the analysis outcomes?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Reflection and Discussion on Data Handling \& Management Challenges in AI Projects}
    % Introduction to data handling in AI
    \begin{block}{Introduction}
        Data handling and management are crucial before training AI models. Effective data preparation ensures model accuracy and impacts fairness, reliability, and outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Management}
    % List of common challenges
    \begin{enumerate}
        \item \textbf{Data Quality Issues:}
            \begin{itemize}
                \item Definition: Missing, incorrect, or inconsistent data distorts analysis.
                \item Example: Negative or unrealistic ages in a dataset require correction.
            \end{itemize}
        \item \textbf{Data Volume:}
            \begin{itemize}
                \item Definition: Large data sizes can overwhelm conventional techniques.
                \item Example: Terabytes of data in image classification need distributed computing.
            \end{itemize}
        \item \textbf{Data Variety:}
            \begin{itemize}
                \item Definition: Data comes in different formats (structured, semi-structured, unstructured).
                \item Example: Social media data involving text, images, and videos needs diverse processing.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Challenges in Data Management (Continued)}
    % Continued list of challenges
    \begin{enumerate}
        \setcounter{enumi}{3} % To continue the numbering from the previous frame
        \item \textbf{Data Privacy and Ethical Considerations:}
            \begin{itemize}
                \item Definition: Handling sensitive information must comply with regulations like GDPR.
                \item Example: Anonymizing Personally Identifiable Information (PII) before training.
            \end{itemize}
        \item \textbf{Data Accessibility:}
            \begin{itemize}
                \item Definition: Relevant team members must access and utilize data efficiently.
                \item Example: Data siloing can slow down project timelines.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Prompts}
    % Discussion prompts for the audience
    \begin{itemize}
        \item \textbf{Identifying Obstacles:} What specific data quality issues have you encountered?
        \item \textbf{Sharing Solutions:} How did you address issues surrounding data volume?
        \item \textbf{Ethical Considerations:} Reflect on a time when you faced ethical dilemmas in data usage. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    % Key points and conclusion
    \begin{itemize}
        \item Data is foundational in AI projects; poor data quality leads to unreliable results.
        \item Addressing data challenges requires technical skills, team collaboration, and ethical awareness.
        \item Continuous monitoring and evaluation of data processes is necessary for effective management.
    \end{itemize}
    \begin{block}{Conclusion}
        Engaging in this reflection and discussion enhances understanding of real-world data challenges and sharpens problem-solving skills in AI projects.
    \end{block}
\end{frame}


\end{document}