\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Week 1: Introduction to Reinforcement Learning}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Overview}
    
    \begin{block}{Reinforcement Learning (RL)}
        Reinforcement Learning is an essential area of artificial intelligence (AI) that focuses on how agents should take actions in an environment to maximize cumulative rewards. 
    \end{block}
    
    \begin{itemize}
        \item RL differs from supervised learning as it learns through \textbf{trial and error}, interacting with the environment.
        \item The goal is to discover the best strategies over time.
    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Significance}
    
    \begin{block}{Significance in AI}
        RL plays a crucial role in various fields:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Decision Making}:
            \begin{itemize}
                \item Essential for autonomous systems with multiple steps and uncertainties.
                \item \textit{Example}: Robotics navigating through terrains.
            \end{itemize}
        \item \textbf{Game Playing}:
            \begin{itemize}
                \item Achievements in complex games like Chess and Go.
                \item \textit{Example}: AlphaGo, using RL to defeat human champions.
            \end{itemize}
        \item \textbf{Real-World Applications}:
            \begin{itemize}
                \item Healthcare, Finance, Advertisement.
            \end{itemize}
    \end{enumerate}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Key Concepts}

    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Agent}: The learner or decision-maker interacting with the environment.
            \item \textbf{Environment}: The context which provides feedback based on the agent's actions.
            \item \textbf{Actions}: Choices made by the agent to influence the state's outcome.
            \item \textbf{Rewards}: Feedback that guides the agent towards achieving desired outcomes.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Learning Process}
    
    \begin{block}{Basic Learning Steps in RL}
        The agent follows these steps:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Interaction}: Observe the state of the environment.
        \item \textbf{Action Selection}: Choose an action based on a policy.
        \item \textbf{Feedback}: Receive a new state and a reward from the environment.
        \item \textbf{Policy Update}: Update the strategy based on received rewards.
    \end{enumerate}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Example}
    
    \begin{block}{Example of RL in Practice}
        Consider a maze with the following elements:
    \end{block}

    \begin{itemize}
        \item \textbf{State}: Current location within the maze.
        \item \textbf{Actions}: Move up, down, left, or right.
        \item \textbf{Reward}: 
            \begin{itemize}
                \item High positive reward for reaching the exit.
                \item Negative reward for running into a wall.
            \end{itemize}
    \end{itemize}

    The agent learns the best path through repeated attempts, balancing rewards and penalties.

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Conclusion}

    \begin{block}{Conclusion}
        Reinforcement Learning is a powerful paradigm in AI that enables machines to develop optimal behaviors through experiences. 
    \end{block}
    
    \begin{itemize}
        \item Addresses complex decision-making problems.
        \item Covers diverse real-world applications.
    \end{itemize}

    As we progress, we will delve into the intricacies of RL and its applications.

\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Reinforcement Learning - Key Points to Remember}

    \begin{block}{Key Points}
        \begin{itemize}
            \item RL is distinct from other machine learning paradigms due to its trial-and-error approach.
            \item Understanding the reinforcement framework is crucial for developing effective AI agents.
            \item Real-world scenarios illustrate the practical importance of RL in dynamic environments.
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Reinforcement Learning?}
    \begin{block}{Definition}
        Reinforcement Learning (RL) is a subset of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
    \end{block}
    
    \begin{block}{Importance}
        RL is crucial for solving sequential decision-making problems such as robotics, gaming, and autonomous driving.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Reinforcement Learning}
    
    \begin{itemize}
        \item \textbf{Agent}: The learner or decision-maker.
        \item \textbf{Environment}: The context or space in which the agent operates.
        \item \textbf{State}: Current situation representation within the environment.
        \item \textbf{Action}: Choices made by the agent to change the state.
        \item \textbf{Reward}: Feedback from the environment after taking an action.
        \item \textbf{Policy}: Strategy that defines actions for each state.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Reinforcement Learning}
    
    \begin{itemize}
        \item \textbf{Sequential Decision-Making}: Ideal for problems requiring sequence-based decisions affecting future outcomes.
        \item \textbf{Real-Time Learning}: Adapts in real-time through trial and error without pre-annotated data.
        \item \textbf{Complex Problem Solving}: Handles environments with complex or unknown rules effectively.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example: Robot in a Maze}
    
    \begin{itemize}
        \item \textbf{Agent}: The robot itself.
        \item \textbf{Environment}: The maze layout.
        \item \textbf{State}: Current position of the robot in the maze.
        \item \textbf{Actions}: Move forward, turn left, turn right.
        \item \textbf{Reward}: 
        \begin{itemize}
            \item +1 for reaching the exit
            \item -1 for hitting walls
        \end{itemize}
    \end{itemize}
    
    By exploring the maze and receiving feedback, the robot learns the most effective paths.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Formulas}
    
    \begin{itemize}
        \item RL uses trial-and-error, suitable for uncertain environments.
        \item Goal: Find a policy that maximizes rewards over time.
    \end{itemize}
    
    \begin{block}{Value Function}
        The value function \( V(s) \) reflects the expected cumulative reward:
        \begin{equation}
            V(s) = \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots | S_t = s]
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminologies in RL - Introduction}
    \begin{block}{Overview}
        Reinforcement Learning (RL) involves various critical components that work together in the decision-making process. 
        In this slide, we will break down and clearly define the key terminologies essential for understanding RL.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminologies in RL - Definitions}
    \begin{enumerate}
        \item \textbf{Agent}  
            \begin{itemize}
                \item \textbf{Definition:} The learner or decision-maker in the RL framework.
                \item \textbf{Examples:} A robot navigating a maze or a software program playing chess.
            \end{itemize}

        \item \textbf{Environment}  
            \begin{itemize}
                \item \textbf{Definition:} The external context with which the agent interacts.
                \item \textbf{Examples:} The maze for the robot or the chessboard for the chess program.
            \end{itemize}
        
        \item \textbf{State}  
            \begin{itemize}
                \item \textbf{Definition:} A specific situation of the environment.
                \item \textbf{Example:} The robot’s current position in the maze.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Terminologies in RL - Actions, Rewards, and Policies}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Action}  
            \begin{itemize}
                \item \textbf{Definition:} The possible moves an agent can make.
                \item \textbf{Example:} The robot can move left, right, forward, or backward in the maze.
            \end{itemize}

        \item \textbf{Reward}  
            \begin{itemize}
                \item \textbf{Definition:} A feedback signal received after taking an action.
                \item \textbf{Example:} +10 for reaching the maze exit, -1 for hitting a wall.
            \end{itemize}

        \item \textbf{Policy}  
            \begin{itemize}
                \item \textbf{Definition:} A strategy that defines actions for each state.
                \item \textbf{Example:} Move forward if in state X, turn left if in state Y.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Visual Aids}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Each component plays a crucial role in the reinforcement learning process.
            \item Understanding agent-environment interactions through states, actions, rewards, and policies is fundamental for mastering RL.
            \item The agent aims to maximize cumulative rewards over time by learning the best policy through trial and error.
        \end{itemize}
    \end{block}
    
    \begin{block}{Visual Aid}
        Consider illustrating the relationship among the components: 
        an agent interacts with the environment by observing its state, selects an action, receives a reward, and updates its policy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents in RL - Overview}
    In Reinforcement Learning (RL), an **agent** refers to the learner or decision-maker that interacts with an environment to achieve a goal. The agent learns through trial and error, receiving feedback in the form of rewards or penalties based on its actions. 
    \begin{block}{Key Role}
        Understanding the role of agents is crucial in the context of RL since their behavior influences the overall learning dynamics and outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents in RL - Key Concepts}
    \begin{enumerate}
        \item \textbf{Agent Definition}:
        \begin{itemize}
            \item An agent maximizes cumulative rewards by taking actions in an environment.
            \item Agents operate based on **policies** that dictate actions in different states.
        \end{itemize}
        
        \item \textbf{Types of Agents}:
        \begin{itemize}
            \item **Random Agent**: Acts randomly to serve as a baseline.
            \item **Greedy Agent**: Chooses the best action based on past experience.
            \item **Exploratory Agent**: Balances exploration of new actions and exploitation of known rewards (e.g., using $\epsilon$-greedy strategies).
            \item **Learning Agent**: Adapts strategies using algorithms like Q-learning or policy gradients.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents in RL - Learning Process}
    \begin{block}{How Agents Learn}
        - **Trial and Error**: Agents try various actions and learn from outcomes.
        - **Feedback Loop**:
        \begin{itemize}
            \item \textbf{State}: Current situation of the agent in the environment.
            \item \textbf{Action}: Choice made by the agent among available options.
            \item \textbf{Reward}: Feedback indicating the quality of an action in achieving the goal.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example Scenario}
        Consider a robot (agent) in a maze:
        \begin{itemize}
            \item **States**: Positions in the maze.
            \item **Actions**: Move left, right, up, or down.
            \item **Rewards**:
            \begin{itemize}
                \item Positive for reaching the exit.
                \item Negative for hitting walls or obstacles.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Agents in RL - Summary and Formula}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Agents are central to RL systems; understanding their behavior is fundamental.
            \item The learning process involves balancing exploration and exploitation.
            \item The design and strategy of agents significantly affect learning efficiency and effectiveness.
        \end{itemize}
    \end{block}
    
    \begin{block}{Cumulative Reward Formula}
        The cumulative reward can be represented as:
        \begin{equation}
            G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots + \gamma^{n}R_{t+n}
        \end{equation}
        where:
        \begin{itemize}
            \item \( G_t \): Cumulative reward at time \( t \)
            \item \( R \): Received rewards
            \item \( \gamma \) (0 ≤ \( \gamma \) < 1): Discount factor affecting the importance of future rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Environments - Part 1}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{What is a State?}
            \begin{itemize}
                \item A \textbf{state} in reinforcement learning (RL) is a specific representation of the environment at a given point in time.
                \item It encapsulates all relevant information needed for the agent's decision-making.
                \item States may include raw observations (e.g., camera images) or structured data (e.g., chessboard position).
            \end{itemize}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Environments - Part 2}
    \begin{block}{Environment Overview}
        \begin{itemize}
            \item An \textbf{environment} is the external system that the agent interacts with.
            \item It includes states, possible actions, and rewards.
            \item The environment provides various states based on the actions taken by the agent.
        \end{itemize}
    \end{block}
    
    \begin{block}{Influence of States on Agent Behavior}
        \begin{itemize}
            \item The agent perceives the current state to decide on actions based on its policy (strategy).
            \item Different states can lead to different actions, necessitating adaptation of agent behavior.
            \item Example: In chess, the state changes drastically with each move, requiring reevaluation of strategy.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{States and Environments - Part 3}
    \begin{block}{Examples}
        \begin{enumerate}
            \item \textbf{Example 1: Game Environment}
            \begin{itemize}
                \item The state is the arrangement of pieces on a game board.
                \item Each move results in a new state, changing decision-making.
            \end{itemize}
            
            \item \textbf{Example 2: Autonomous Driving}
            \begin{itemize}
                \item The agent (self-driving car) observes states like traffic lights, other vehicles, and road conditions.
                \item The car makes decisions (accelerate, brake, turn) based on these states.
            \end{itemize}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding the relationship between states and environments is fundamental in RL, guiding agents toward optimal behavior and strategic decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards - Understanding Actions}
    \begin{block}{Definition}
        In reinforcement learning (RL), an \textbf{action} is a decision made by an agent within the environment. The choice of action influences the future state and outcomes of the agent's experience.
    \end{block}
    \begin{itemize}
        \item \textbf{Types of Actions:}
        \begin{itemize}
            \item \textbf{Discrete Actions:} Limited set of possible actions (e.g., move left, move right).
            \item \textbf{Continuous Actions:} Infinite set of possible actions (e.g., steering angle in a car).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards - Examples}
    \begin{itemize}
        \item \textbf{Game Environment:} In a chess game, an agent can choose actions such as moving a pawn, capturing a piece, or castling.
        \item \textbf{Robot Navigation:} A robot can choose to move forward, turn left, or stop.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards - Understanding Rewards}
    \begin{block}{Definition}
        A \textbf{reward} is a numerical value provided to the agent after performing an action in a given state. Rewards guide learning by evaluating the effectiveness of the actions.
    \end{block}
    \begin{itemize}
        \item \textbf{Reward Structure:}
        \begin{itemize}
            \item \textbf{Positive Reward:} Indicates a desirable outcome (e.g., winning a game).
            \item \textbf{Negative Reward (Penalty):} Indicates an undesirable outcome (e.g., falling off a cliff).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards - Example and Interplay}
    \begin{itemize}
        \item \textbf{Example:}
        \begin{itemize}
            \item \textbf{Maze:} If the agent successfully finds the exit, it receives a positive reward (+10). If it hits a wall, it receives a negative reward (-5).
        \end{itemize}
        \item \textbf{The Interplay Between Actions and Rewards:}
        \begin{itemize}
            \item Actions lead to changes in state and subsequently receive rewards, forming the basis of learning:
            \begin{itemize}
                \item \textbf{Trial and Error:} The agent explores various actions to maximize cumulative rewards over time.
                \item \textbf{Learning Process:} Over many iterations, the agent learns which actions yield the highest rewards in specific states.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Actions and Rewards - Key Equation and Points}
    \begin{block}{Key Equation}
        The agent's objective is to maximize the expected cumulative reward, often formulated as:
        \begin{equation}
            R = r(t) + \gamma r(t+1) + \gamma^2 r(t+2) + \ldots
        \end{equation}
        where \( R \) is the total expected reward, \( \gamma \) is the discount factor (0 ≤ \( \gamma \) < 1), and \( r(t) \) is the reward received at time \( t \).
    \end{block}
    \begin{itemize}
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item Actions directly impact the agent's learning process in RL.
            \item Rewards evaluate actions and reinforce learning through feedback, guiding agents toward optimal strategies.
            \item Understanding the dynamics between actions and rewards is essential for designing effective RL systems.
            \item By mastering actions and rewards, students gain better insight into policies in reinforcement learning.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in RL - Definition}
    % Definition of policies in reinforcement learning
    \begin{block}{Definition of Policies}
        In Reinforcement Learning (RL), a \textbf{policy} is a strategy employed by an agent that defines the actions it will take based on the current state of the environment. 
        It acts as a blueprint for decision-making, guiding the agent on what to do in different situations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in RL - Key Concepts}
    % Key concepts related to policies in RL
    \begin{itemize}
        \item \textbf{Types of Policies:}
        \begin{enumerate}
            \item \textbf{Deterministic Policy}
            \begin{itemize}
                \item Maps each state to a specific action.
                \item Example: If the agent is in state $s_1$, it always takes action $a_1$.
                \item Mathematically: $\pi(s) = a$, where $a$ is a specific action for state $s$.
            \end{itemize}
            \item \textbf{Stochastic Policy}
            \begin{itemize}
                \item Selects an action based on a probability distribution.
                \item Introduces variability in the agent's behavior.
                \item Mathematically: $\pi(a|s) = P(A=a | S=s)$, indicating the probability of taking action $a$ when in state $s$.
            \end{itemize}
        \end{enumerate}
        
        \item \textbf{Role of Policies:}
        \begin{itemize}
            \item Define how the agent behaves in various states.
            \item Adapt and evolve based on received rewards to maximize cumulative rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in RL - Examples and Summary}
    % Examples and summary of policies in RL
    \begin{itemize}
        \item \textbf{Examples:}
        \begin{enumerate}
            \item \textbf{Game Playing:}
            \begin{itemize}
                \item Deterministic: In chess, a fixed move for a specific board configuration.
                \item Stochastic: In a video game, choosing actions based on probabilities.
            \end{itemize}
            \item \textbf{Robot Navigation:}
            \begin{itemize}
                \item Deterministic: Always turning left at intersections.
                \item Stochastic: Choosing left or right with certain probabilities.
            \end{itemize}
        \end{enumerate}
        
        \item \textbf{Key Points to Emphasize:}
        \begin{itemize}
            \item Policies are fundamental to decision-making.
            \item The choice between deterministic and stochastic depends on the learning environment.
            \item Agent performance relies on the effectiveness of its policy.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Summary}
        A policy in reinforcement learning is crucial for guiding an agent's actions, impacting how agents interact with their environments and make decisions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of the Reinforcement Learning Process}
    Reinforcement Learning (RL) is a subset of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards. The RL process involves several key components:
    
    \begin{itemize}
        \item \textbf{Agent}: The learner or decision-maker.
        \item \textbf{Environment}: The external system the agent interacts with.
        \item \textbf{State (s)}: Representation of the current situation.
        \item \textbf{Action (a)}: An operation taken by the agent.
        \item \textbf{Reward (r)}: Feedback from the environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{The RL Process}
    The RL process can be broken down into a series of steps:
    
    \begin{enumerate}
        \item \textbf{Observation}: The agent observes the current state of the environment (s).
        \item \textbf{Decision Making}: The agent selects an action (a) based on its policy.
        \item \textbf{Action}: The agent performs the selected action (a).
        \item \textbf{Reward}: The agent receives a reward (r) based on the action taken.
        \item \textbf{Update Policy}: The agent updates its policy using the reward received.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Exploration vs. Exploitation}
    A critical aspect of RL is balancing exploration and exploitation:
    
    \begin{itemize}
        \item \textbf{Exploration}: Trying out new actions to discover effects and rewards.
        \item \textbf{Exploitation}: Leveraging known information to maximize short-term rewards.
    \end{itemize}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item The RL process is iterative and dynamic.
            \item A balance between exploration and exploitation is vital.
            \item Understanding this balance is crucial for effective learning algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formula Representation}
    The agent's goal can be framed as maximizing future rewards, typically expressed through the concept of the value function:
    
    \begin{equation}
        V(s) = \max_{a} \left( r(s, a) + \gamma V(s') \right)
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \( V(s) \) is the value of the state.
        \item \( r(s, a) \) is the immediate reward after taking action \( a \) in state \( s \).
        \item \( \gamma \) is the discount factor (0 ≤ \( \gamma \) < 1).
        \item \( s' \) is the next state resulting from the action.
    \end{itemize}
    
    Understanding the RL process and the interplay between exploration and exploitation is fundamental to various applications, such as gaming and robotics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Reinforcement Learning}
    \begin{block}{Introduction}
        Reinforcement Learning (RL) is a powerful machine learning paradigm that enables agents to learn optimal behaviors through interactions with their environment. It has applications in diverse domains, including gaming, robotics, and finance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Gaming}
    \begin{itemize}
        \item \textbf{Example: AlphaGo}
        \begin{itemize}
            \item \textbf{Description:} Developed by DeepMind, AlphaGo is an RL-based program that achieved unprecedented success in defeating world champions in the game of Go.
            \item \textbf{Mechanism:} It combines deep Neural Networks and Monte Carlo Tree Search to evaluate board positions and optimize its strategy through self-play.
            \item \textbf{Key Point:} AlphaGo demonstrated RL's capacity to master complex strategy games beyond traditional computational limits.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications - Robotics and Finance}
    \begin{itemize}
        \item \textbf{Robotics}
        \begin{itemize}
            \item \textbf{Example: Robot Manipulation}
            \begin{itemize}
                \item \textbf{Description:} RL is used to teach robots how to manipulate objects, navigate spaces, and perform tasks autonomously.
                \item \textbf{Application:} Robots learn to pick up and place objects, refining their strategies through feedback from success or failure.
                \item \textbf{Key Point:} RL enables robots to adapt to dynamic environments by continuously learning.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Finance}
        \begin{itemize}
            \item \textbf{Example: Algorithmic Trading}
            \begin{itemize}
                \item \textbf{Description:} RL algorithms develop trading strategies that maximize returns by learning from market trends and historical data.
                \item \textbf{Mechanism:} Trading agents receive rewards for profitable trades and penalties for losses, optimizing buy/sell decisions over time.
                \item \textbf{Key Point:} RL can surpass traditional trading models by adjusting dynamically to market conditions.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Conclusion}
    \begin{block}{Summary}
        RL enhances capabilities in various fields:
        \begin{itemize}
            \item \textbf{Gaming}: Achievements like AlphaGo redefine competitive strategy.
            \item \textbf{Robotics}: Offers adaptive solutions for task execution through experimentation.
            \item \textbf{Finance}: Powers intelligent trading strategies improving investment outcomes.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        As RL continues to advance, understanding its applications will empower learners to innovate and implement RL solutions effectively in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - Part 1}
    \begin{enumerate}
        \item \textbf{Understanding Reinforcement Learning (RL)}
        \begin{itemize}
            \item \textbf{Definition}: RL is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward.
            \item \textbf{Key Components}:
            \begin{itemize}
                \item \textbf{Agent}: The learner or decision maker.
                \item \textbf{Environment}: Everything the agent interacts with.
                \item \textbf{Action (A)}: Choices made by the agent.
                \item \textbf{State (S)}: The current situation of the agent in the environment.
                \item \textbf{Reward (R)}: Feedback from the environment after the agent takes an action.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Core Concepts of RL}
        \begin{itemize}
            \item \textbf{Exploration vs. Exploitation}: Balancing the need to explore new actions versus leveraging known actions that yield high rewards.
            \item \textbf{Markov Decision Process (MDP)}: A framework for modeling decision-making where outcomes are partly random and partly controlled by a decision maker.
            \item \textbf{Value Functions}: Estimates the expected return (total future reward) from any state or state-action pair.
        \end{itemize}

        \item \textbf{Real-World Applications Covered}
        \begin{itemize}
            \item \textbf{Gaming}: RL in DeepMind’s AlphaGo, which learned to play Go at a superhuman level.
            \item \textbf{Robotics}: Teaching robots tasks like grasping objects or walking.
            \item \textbf{Finance}: Adaptable algorithmic trading strategies based on learned actions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Learning Objectives}
    By the end of this week, students should be able to:
    \begin{enumerate}
        \item \textbf{Define Key Terms in Reinforcement Learning}
            \begin{itemize}
                \item Understand terminology such as agent, environment, actions, states, rewards, and value functions.
            \end{itemize}
        \item \textbf{Illustrate the Exploration-Exploitation Dilemma}
            \begin{itemize}
                \item Discuss examples of when an agent should explore versus exploit to maximize rewards.
            \end{itemize}
        \item \textbf{Apply the Concepts of MDP}
            \begin{itemize}
                \item Model simple decision-making scenarios as Markov Decision Processes.
            \end{itemize}
        \item \textbf{Recognize the Various Applications of RL}
            \begin{itemize}
                \item Identify and explain at least three different applications of reinforcement learning in real-world scenarios.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Formulation}
    The formula for calculating the expected reward for a state-action pair is represented as:
    \begin{equation}
        Q(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')
    \end{equation}
    Where:
    \begin{itemize}
        \item $Q(s, a)$: Expected utility of taking action $a$ in state $s$.
        \item $R(s, a)$: Reward received after executing action $a$ in state $s$.
        \item $\gamma$: Discount factor prioritizing immediate rewards over future rewards.
        \item $P(s'|s, a)$: Transition probability to the next state $s'$ from state $s$ given action $a$.
    \end{itemize}
\end{frame}


\end{document}