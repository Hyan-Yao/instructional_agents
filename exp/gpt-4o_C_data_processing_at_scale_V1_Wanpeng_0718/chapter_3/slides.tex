\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Chapter 3: Setting Up a Data Processing Environment}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{Introduction to Data Processing Environment}
    \begin{block}{Importance of a Data Processing Environment}
        Setting up an effective data processing environment is crucial for managing, processing, and analyzing data efficiently. 
        A well-structured environment ensures data collection, storage, and access conducive to analysis, enabling insights for informed decisions.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Benefits of a Data Processing Environment}
    \begin{itemize}
        \item \textbf{Scalability}: Handle growing amounts of data without compromising performance.
        \item \textbf{Performance}: Optimizes processing speed and efficiency through distributed computing.
        \item \textbf{Cost-Effectiveness}: Reduces operational costs using open-source frameworks.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Overview of Spark and Hadoop}
    \begin{block}{Apache Spark}
        \begin{itemize}
            \item \textbf{Description}: Unified analytics engine for large-scale data processing with in-memory capabilities.
            \item \textbf{Core Features}:
            \begin{itemize}
                \item \textbf{Speed}: In-memory computation allowing fast processing.
                \item \textbf{Ease of Use}: Rich APIs in Python, Java, Scala.
                \item \textbf{Versatility}: Supports batch, streaming, machine learning, and graph processing.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Apache Spark Use Case}
    \begin{block}{Example Use Case}
        Real-time analytics for online retail, allowing dynamic pricing adjustments based on customer activity.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Overview of Hadoop}
    \begin{block}{Apache Hadoop}
        \begin{itemize}
            \item \textbf{Description}: Framework for distributed processing of large data sets across clusters.
            \item \textbf{Core Components}:
            \begin{itemize}
                \item \textbf{HDFS}: Distributed file storage system running on commodity hardware.
                \item \textbf{MapReduce}: Programming model for parallel processing of data across a cluster.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Hadoop Use Case}
    \begin{block}{Example Use Case}
        Analyzing large-scale datasets in healthcare to identify trends in patient data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item The choice of processing environment impacts the efficiency of data workflows.
        \item Both Spark and Hadoop offer unique advantages for specific data processing tasks.
        \item Understanding their strengths and weaknesses is essential for an optimal data strategy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Code Snippet (Spark)}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder \
    .appName("Example App") \
    .getOrCreate()

# Load data
data = spark.read.csv("datafile.csv")

# Show the first few records
data.show()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    By setting up a tailored data processing environment using tools like Spark and Hadoop, organizations can significantly enhance their data handling capabilities, leading to better analytics and data-driven decision-making.
\end{frame}

\begin{frame}
    \frametitle{Next Steps}
    In the following slide, we will outline the course learning objectives, including the installation and configuration of Spark and Hadoop.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Course Learning Objectives}
    This chapter aims to equip students with the necessary skills to effectively set up a data processing environment using Apache Spark and Hadoop. By the end of this chapter, students should be able to:
    \begin{enumerate}
        \item Understand the importance of a data processing environment.
        \item Install Apache Hadoop.
        \item Configure Hadoop components.
        \item Install Apache Spark.
        \item Configure Spark with Hadoop.
        \item Run sample applications.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understand the Importance}
    \begin{itemize}
        \item Grasp the significance of installing and configuring data processing tools to handle big data efficiently.
        \item Recognize how these environments facilitate data analysis, storage, and management.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Install Apache Hadoop}
    Gain hands-on experience with downloading and installing the Hadoop framework on local or cluster environments. Follow step-by-step guidance for proper installation.
    
    \begin{block}{Example Command}
    \begin{lstlisting}
    wget https://downloads.apache.org/hadoop/common/hadoop-x.y.z/hadoop-x.y.z.tar.gz
    tar -xzvf hadoop-x.y.z.tar.gz
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Configure Hadoop Components}
    Learn to modify configuration files to optimize performance, security, and storage options.

    \begin{itemize}
        \item Key configurations: core-site.xml, hdfs-site.xml, mapred-site.xml
    \end{itemize}
    
    \begin{block}{Key Configuration Parameters}
        \begin{itemize}
            \item \texttt{fs.defaultFS} (set the default file system)
            \item \texttt{dfs.replication} (specifying the number of data block replicas)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Install Apache Spark}
    Acquire knowledge on downloading and installing Spark, taking into account its dependencies such as Java and Scala. 

    \begin{block}{Example Command for Spark Installation}
    \begin{lstlisting}
    wget https://downloads.apache.org/spark/spark-x.y.z/spark-x.y.z-bin-hadoopx.y.tgz
    tar -xzvf spark-x.y.z-bin-hadoopx.y.tgz
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Configure Spark with Hadoop}
    Discover how to configure Spark to use Hadoop as its underlying storage.

    \begin{itemize}
        \item Set environment variables and update configuration files for seamless communication.
    \end{itemize}

    \begin{block}{Key Environment Variable}
        \begin{itemize}
            \item \texttt{SPARK\_HOME} (pointing to Spark installation directory)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Run Sample Applications}
    Execute simple Spark applications to validate successful installation and configuration. Test functionalities to ensure that Spark can read from and write to HDFS.

    \begin{block}{Sample Spark Command}
    \begin{lstlisting}
    spark-submit --master local[2] /path/to/your/wordcount.py
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Understanding the prerequisites for a successful setup of Spark and Hadoop.
        \item Importance of configuration settings in optimizing performance for data processing tasks.
        \item Hands-on experience is critical; practice installations and configurations.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    By mastering the objectives outlined, students will be well-prepared to utilize Spark and Hadoop for their data processing needs, laying the foundation for further exploration of big data technologies in subsequent chapters.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Spark and Hadoop}
    \begin{block}{Introduction}
        Apache Spark and Hadoop are widely used frameworks for big data processing, designed to handle large datasets with distributed computing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Hadoop}
    \begin{block}{Description}
        Hadoop is an open-source framework for distributed storage and processing of large datasets across clusters using simple programming models.
    \end{block}

    \begin{block}{Key Components}
        \begin{enumerate}
            \item \textbf{Hadoop Distributed File System (HDFS)}:
                \begin{itemize}
                    \item A distributed file system for high throughput access to data.
                    \item \textit{Example:} Storing large customer transaction datasets across multiple nodes.
                \end{itemize}
            \item \textbf{MapReduce}:
                \begin{itemize}
                    \item A programming model for processing large data sets in parallel.
                    \item \textit{Process:}
                    \begin{itemize}
                        \item \textbf{Map Phase}: Processes input data into key-value pairs.
                        \item \textbf{Reduce Phase}: Aggregates results from the map phase.
                    \end{itemize}
                    \item \textit{Example:} Word count application to count occurrences of words in large texts.
                \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Key Functionalities of Hadoop}
        \begin{itemize}
            \item Scalability: Scales to store and process petabytes of data.
            \item Fault Tolerance: Automatic data replication avoids data loss.
            \item Batch Processing: Best for processing large data volumes in batch.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Apache Spark}
    \begin{block}{Description}
        Spark is a fast, general-purpose cluster computing system designed for speed and ease of use, supporting data parallelism and fault tolerance.
    \end{block}

    \begin{block}{Key Components}
        \begin{enumerate}
            \item \textbf{Spark Core}:
                \begin{itemize}
                    \item The foundation providing essential functionalities and APIs.
                \end{itemize}
            \item \textbf{Spark SQL}:
                \begin{itemize}
                    \item Executes SQL queries, integrating with existing data sources.
                \end{itemize}
            \item \textbf{Spark Streaming}:
                \begin{itemize}
                    \item Processes live data streams, integrating with various data sources in real-time.
                \end{itemize}
            \item \textit{Example:} Analyzing real-time sensor data from IoT devices to predict machine failure.
        \end{enumerate}
    \end{block}

    \begin{block}{Key Functionalities of Spark}
        \begin{itemize}
            \item In-Memory Processing: Significantly speeds up data processing tasks.
            \item Unified Platform: Supports various workloads including batch, interactive, and streaming.
            \item Compatibility: Works with multiple data sources like HDFS, Apache Cassandra, and Amazon S3.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison: Spark vs. Hadoop}
    \begin{tabular}{|l|l|l|}
        \hline
        \textbf{Aspect} & \textbf{Hadoop} & \textbf{Spark} \\
        \hline
        Processing Model & Batch processing (MapReduce) & In-memory processing \\
        \hline
        Speed & Slower due to disk I/O & Faster due to in-memory computing \\
        \hline
        Ease of Use & More complex, requires Java/MapReduce coding & Simpler APIs (e.g., Python, Scala) \\
        \hline
        Real-time Processing & Limited (requires additional tools) & Capable (Spark Streaming) \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Both Spark and Hadoop have distinct advantages and use cases:
        \begin{itemize}
            \item Hadoop excels in batch processing and storage.
            \item Spark shines in speed and versatility.
        \end{itemize}
        Understanding their strengths and weaknesses is crucial for efficient data processing environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Requirements for Setup - Hardware Requirements}
    \begin{block}{Minimum Configuration}
        \begin{itemize}
            \item \textbf{Processor:} Dual-core CPU (Intel i5 or equivalent)
            \item \textbf{RAM:} At least 8 GB (16 GB recommended for better performance)
            \item \textbf{Storage:} Minimum of 100 GB free disk space (SSD recommended for faster data access)
        \end{itemize}
    \end{block}
    
    \begin{block}{Recommended Configuration}
        \begin{itemize}
            \item \textbf{Processor:} Quad-core CPU (Intel i7 or equivalent)
            \item \textbf{RAM:} 16-32 GB RAM for efficient data processing
            \item \textbf{Storage:} 500 GB or more (consider using a distributed file system)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Requirements for Setup - Software Requirements}
    \begin{block}{Operating Systems}
        \begin{itemize}
            \item \textbf{Linux:} (Ubuntu, CentOS, etc.) - Preferred for Hadoop and Spark deployment
            \item \textbf{Windows:} (10 or higher) - Can be used for development; some limitations in production
        \end{itemize}
    \end{block}

    \begin{block}{Java Development Kit (JDK)}
        \begin{itemize}
            \item \textbf{Version:} JDK 8 or later
            \item \textbf{Setup:} Ensure JAVA\_HOME environment variable is set to your JDK installation path
        \end{itemize}
        \begin{lstlisting}
export JAVA_HOME=/path/to/jdk
export PATH=$JAVA_HOME/bin:$PATH
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Requirements for Setup - Libraries and Configuration}
    \begin{block}{Necessary Libraries}
        \begin{itemize}
            \item \textbf{Hadoop Libraries:}
            \begin{itemize}
                \item Apache Commons, SLF4J, Log4j need to be included in Hadoop's lib directory.
            \end{itemize}
            \item \textbf{Apache Spark Libraries:}
            \begin{itemize}
                \item Ensure native Hadoop libraries are present in Spark for HDFS compatibility.
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Configuration Files}
        \begin{itemize}
            \item Modify configuration files such as \texttt{core-site.xml}, \texttt{hdfs-site.xml}, and \texttt{spark-defaults.conf} to set up environment specific configurations.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Ensure compatibility and correct installation of all required software components.
            \item Adequate RAM and storage are crucial for effective data processing.
            \item Correctly setting environment variables is essential for smooth operation.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Overview}
    \begin{block}{Overview}
        Apache Hadoop is a widely used framework for distributed storage and processing of large datasets. 
        The installation process involves downloading, configuring, and setting up various components.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Prerequisites}
    \begin{block}{Step 1: Prerequisites}
        \begin{itemize}
            \item \textbf{Operating System}: Compatible OS (Linux or MacOS). Windows users may use a VM or WSL.
            \item \textbf{Java}: Ensure Java 8 or higher is installed. Verify with:
                \begin{lstlisting}
                java -version
                \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Download and Extract}
    \begin{block}{Step 2: Download Hadoop}
        \begin{enumerate}
            \item Visit the \href{https://hadoop.apache.org/releases.html}{official Apache Hadoop website}.
            \item Choose the latest stable release (e.g., Hadoop 3.x.x).
            \item Download the tar.gz package:
                \begin{lstlisting}
                wget https://downloads.apache.org/hadoop/common/hadoop-3.x.x/hadoop-3.x.x.tar.gz
                \end{lstlisting}
        \end{enumerate}
    \end{block}
    
    \begin{block}{Step 3: Extract the Downloaded File}
        Extract the downloaded file using:
        \begin{lstlisting}
        tar -xzvf hadoop-3.x.x.tar.gz
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Environment Variables}
    \begin{block}{Step 4: Configure Environment Variables}
        Add the following to your \texttt{~/.bashrc} (Linux) or \texttt{~/.bash_profile} (MacOS):
        \begin{lstlisting}
        # Hadoop Environment Variables
        export HADOOP_HOME=/path/to/hadoop-3.x.x
        export JAVA_HOME=/path/to/java
        export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
        \end{lstlisting}
        Replace paths with the actual paths on your system. Run:
        \begin{lstlisting}
        source ~/.bashrc
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Configuration Files}
    \begin{block}{Step 5: Configuration Files}
        Edit the configuration files in the \texttt{conf} directory:
        \begin{itemize}
            \item \texttt{core-site.xml}:
            \begin{lstlisting}[language=XML]
            <configuration>
                <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://localhost:9000</value>
                </property>
            </configuration>
            \end{lstlisting}

            \item \texttt{hdfs-site.xml}:
            \begin{lstlisting}[language=XML]
            <configuration>
                <property>
                    <name>dfs.replication</name>
                    <value>1</value>
                </property>
            </configuration>
            \end{lstlisting}

            \item \texttt{mapred-site.xml}:
            \begin{lstlisting}[language=XML]
            <configuration>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>
            \end{lstlisting}

            \item \texttt{yarn-site.xml}:
            \begin{lstlisting}[language=XML]
            <configuration>
                <property>
                    <name>yarn.nodemanager.auxservices</name>
                    <value>mapreduce_shuffle</value>
                </property>
            </configuration>
            \end{lstlisting}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Hadoop - Final Steps}
    \begin{block}{Step 6: Format the HDFS}
        Before starting Hadoop, format the HDFS:
        \begin{lstlisting}
        hdfs namenode -format
        \end{lstlisting}
    \end{block}

    \begin{block}{Step 7: Start Hadoop}
        Finally, start the Hadoop services:
        \begin{lstlisting}
        start-dfs.sh
        start-yarn.sh
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item Ensure Java is installed and accessible.
            \item Set up environment variables correctly to avoid path issues.
            \item Configuration files are essential for Hadoop operation.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Following these steps will enable you to successfully install Apache Hadoop and begin working with big data effectively!
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Spark - Overview}
    \begin{block}{What is Apache Spark?}
        Apache Spark is a fast, open-source data processing engine built around speed, ease of use, and sophisticated analytics.
    \end{block}
    \begin{block}{Objectives}
        This slide will guide you through:
        \begin{itemize}
            \item Installing Apache Spark
            \item Integrating it with Hadoop
            \item Setting up the necessary environment configurations
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Spark - Step-by-Step Process}
    \begin{enumerate}
        \item \textbf{Download Apache Spark}
        \begin{itemize}
            \item Visit the \texttt{Apache Spark website} and download the latest version.
            \item Choose a pre-built version for Hadoop (e.g., Spark 3.3.1 with Hadoop 3.2+).
        \end{itemize}
        
        \item \textbf{Extract the Downloaded File}
        \begin{itemize}
            \item Navigate to your download directory and extract it:
            \begin{lstlisting}
tar -xvzf spark-3.3.1-bin-hadoop3.2.tgz
            \end{lstlisting}
            \item Move to desired installation directory (e.g., /usr/local/spark).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Spark - Configuration and Integration}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Set Environment Variables}
        \begin{itemize}
            \item Add to your \texttt{.bashrc} or \texttt{.bash\_profile}:
            \begin{lstlisting}
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin:$PATH
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Integrate Spark with Hadoop}
        \begin{itemize}
            \item Ensure your Hadoop installation is configured.
            \item Copy Hadoop configuration files to Spark configuration:
            \begin{lstlisting}
cp $HADOOP_HOME/etc/hadoop/* $SPARK_HOME/conf/
            \end{lstlisting}
        \end{itemize}

        \item \textbf{Start Spark}
        \begin{itemize}
            \item Navigate to Spark installation directory:
            \begin{lstlisting}
cd $SPARK_HOME
            \end{lstlisting}
            \item Start the Spark shell:
            \begin{lstlisting}
./bin/spark-shell
            \end{lstlisting}
            \item Validate if Spark is running successfully.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Installing Apache Spark - Key Takeaways and Example}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Ensure compatibility between Spark and Hadoop versions.
            \item Set environment variables correctly to prevent errors.
            \item Spark can read from multiple data sources.
        \end{itemize}
    \end{block}

    \begin{block}{Example Code Snippet}
        Here is an example of initializing a DataFrame in Spark:
        \begin{lstlisting}[language=Scala]
val data = Seq(("Alice", 34), ("Bob", 45))
val df = spark.createDataFrame(data).toDF("Name", "Age")
df.show()
        \end{lstlisting}
    \end{block}
    
    \begin{block}{Conclusion}
        Following these steps will enable you to successfully install Apache Spark and integrate it with Hadoop.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Configuration and Optimization - Overview}
    \begin{block}{Overview}
        Optimizing configuration settings for Hadoop and Spark is crucial for enhancing the performance of data processing tasks. 
        Proper configuration minimizes bottlenecks, ensures resource efficiency, and improves overall job execution times.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Configuration Settings - Hadoop}
    \begin{enumerate}
        \item \textbf{Hadoop Configuration}
        \begin{itemize}
            \item \textbf{YARN Settings}
            \begin{itemize}
                \item \texttt{yarn.nodemanager.resource.memory-mb}: Configure the total memory available to YARN for containers.
                \begin{itemize}
                    \item \textit{Example}: Setting this value to 8096MB allows each container to use up to 8GB of memory.
                \end{itemize}
                \item \texttt{yarn.nodemanager.aux-services}: Enable auxiliary services (like Spark on YARN).
                \begin{itemize}
                    \item \textit{Example}: Setting this to \texttt{spark} allows Spark jobs to run on YARN.
                \end{itemize}
            \end{itemize}
            \item \textbf{HDFS Configuration}
            \begin{itemize}
                \item \texttt{dfs.replication}: Adjust the number of replicas of data blocks.
                \begin{itemize}
                    \item \textit{Best Practice}: Start with a replication factor of 3 for robustness.
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Configuration Settings - Spark}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Spark Configuration}
        \begin{itemize}
            \item \textbf{Driver and Executor Memory}
            \begin{itemize}
                \item \texttt{spark.driver.memory}: Memory allocated to the Spark driver. 
                \begin{itemize}
                    \item \textit{Example}: \texttt{spark.driver.memory=4g} sets the driver memory to 4GB.
                \end{itemize}
                \item \texttt{spark.executor.memory}: Memory allocated to each executor.
                \begin{itemize}
                    \item \textit{Example}: \texttt{spark.executor.memory=8g} allows each executor to have 8GB of memory.
                \end{itemize}
            \end{itemize}
            \item \textbf{Cores Configuration}
            \begin{itemize}
                \item \texttt{spark.executor.cores}: Sets the number of cores per executor for parallel processing.
                \begin{itemize}
                    \item \textit{Best Practice}: Start with 4 cores \texttt{(spark.executor.cores=4)} for a balanced workload.
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Optimization Techniques}
    \begin{itemize}
        \item \textbf{Dynamic Resource Allocation}: Enable dynamic allocation of executors using \texttt{spark.dynamicAllocation.enabled}, which optimally adjusts resources based on workload.
        \item \textbf{Data Serialization}:
        \begin{itemize}
            \item Use \textbf{Kryo Serializer} for faster serialization than the default Java serializer.
            \begin{itemize}
                \item \textit{Example}: Set \texttt{spark.serializer=org.apache.spark.serializer.KryoSerializer}.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Benefits}
    \begin{itemize}
        \item \textbf{Resource Utilization}: Fine-tuning configurations ensures that both Hadoop and Spark utilize the available resources effectively, leading to faster job executions and reduced costs.
        \item \textbf{Job Performance}: Proper configurations can reduce job failures due to resource exhaustion, leading to higher throughput.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=bash]
# Hadoop Configuration Example
yarn.nodemanager.resource.memory-mb=8192
dfs.replication=3

# Spark Configuration Example
spark.driver.memory=4g
spark.executor.memory=8g
spark.executor.cores=4
spark.dynamicAllocation.enabled=true
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Testing the Setup}
    \begin{itemize}
        \item Verify installation and configuration of Hadoop and Spark.
        \item Run test jobs to validate functionality.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    After successfully installing and configuring Apache Hadoop and Apache Spark, it is crucial to verify that both frameworks are functioning as expected. 
    Testing the setup helps catch any configuration errors before diving into more complex data processing tasks.
\end{frame}

\begin{frame}
    \frametitle{Importance of Testing}
    \begin{itemize}
        \item \textbf{Validation of Installation:} Confirm that Hadoop and Spark are correctly installed.
        \item \textbf{Functionality Check:} Ensure components can communicate (e.g., HDFS and Spark).
        \item \textbf{Performance Baseline:} Observe initial performance and identify potential issues by running test jobs.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Steps to Test the Setup}
    \begin{enumerate}
        \item \textbf{Verifying Hadoop Installation:}
        \begin{itemize}
            \item \texttt{hadoop version}
            \item \textbf{Expected Output:} Message displaying Hadoop version installed.
            \item \texttt{hadoop dfs -ls /}
            \item \textbf{Expected Output:} List of files in HDFS root.
        \end{itemize}

        \item \textbf{Running a Simple Hadoop MapReduce Job:}
        \begin{itemize}
            \item \texttt{hadoop jar \$HADOOP\_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 16 100}
            \item \textbf{Expected Output:} Estimated value of Ï€.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Verifying Spark Installation and Simple Job}
    \begin{enumerate}[resume]
        \item \textbf{Verifying Spark Installation:}
        \begin{itemize}
            \item \texttt{spark-submit --version}
            \item \textbf{Expected Output:} Version information for Spark.
        \end{itemize}

        \item \textbf{Running a Simple Spark Job:}
        \begin{itemize}
            \item Create \texttt{test.txt} with example content.
            \item Submit Spark Job:
            \begin{lstlisting}[language=Python]
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")
text_file = sc.textFile("test.txt")
counts = text_file.flatMap(lambda line: line.split()) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)
output = counts.collect()
print(output)
            \end{lstlisting}
            \item \textbf{Expected Output:} List of word counts.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Testing your Hadoop and Spark setup with these simple jobs ensures that your data processing environment is ready for more complex tasks. Address any errors encountered during testing using appropriate troubleshooting techniques.
\end{frame}

\begin{frame}
    \frametitle{Common Installation Issues}
    \begin{block}{Introduction}
        Setting up a data processing environment involving tools like Spark and Hadoop can often lead to various installation challenges. Understanding these common issues and their solutions is essential for a smooth installation process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Installation Issues and Troubleshooting Tips - Part 1}
    \begin{enumerate}
        \item \textbf{Java Home Configuration}
        \begin{itemize}
            \item \textbf{Issue}: Spark and Hadoop require Java, and the \texttt{JAVA\_HOME} environment variable must be set correctly.
            \item \textbf{Solution}:
            \begin{lstlisting}
            # Check if Java is installed
            java -version
            
            # Set JAVA_HOME
            export JAVA_HOME=/path/to/java
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Incompatible Software Versions}
        \begin{itemize}
            \item \textbf{Issue}: Using mismatched versions of Spark, Hadoop, or libraries can cause installation failures.
            \item \textbf{Solution}: Verify compatibility by checking the documentation. Example for Spark 3.x: Requires Hadoop 2.7 or later.
        \end{itemize}
        
        \item \textbf{Missing Dependencies}
        \begin{itemize}
            \item \textbf{Issue}: Dependency libraries may not be installed or may be the wrong version.
            \item \textbf{Solution}:
            \begin{lstlisting}
            # Install missing dependencies with package managers
            sudo apt-get install maven
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Installation Issues and Troubleshooting Tips - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Permission Issues}
        \begin{itemize}
            \item \textbf{Issue}: Insufficient permissions can prevent installation or execution.
            \item \textbf{Solution}:
            \begin{lstlisting}
            # Run installation commands with sudo or adjust permissions
            sudo chown -R $USER:$GROUP /path/to/folder
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Firewall and Port Issues}
        \begin{itemize}
            \item \textbf{Issue}: Firewall settings may block necessary ports.
            \item \textbf{Solution}:
            \begin{lstlisting}
            # Open ports used by Hadoop and Spark
            sudo ufw allow 8080
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Configuration File Errors}
        \begin{itemize}
            \item \textbf{Issue}: Misconfigurations in XML files can lead to startup failures.
            \item \textbf{Solution}: Validate XML syntax and ensure required fields are filled in. Use online validators if needed.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points and Next Steps}
    \begin{itemize}
        \item Always check software versions for compatibility.
        \item Ensure you have the correct permissions and dependencies before installation.
        \item Regularly consult the official documentation for specific requirements and configurations.
    \end{itemize}
    
    \begin{block}{Next Steps}
        After resolving these issues, verify successful installation using simple test jobs. This will confirm that your setup is ready for data processing tasks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion of Chapter 3: Setting Up a Data Processing Environment}
    In this chapter, we explored the foundational elements of setting up a robust data processing environment. 

    \begin{itemize}
        \item \textbf{Environment Setup}: Importance of installing and configuring software packages and dependencies.
        \item \textbf{Common Pitfalls}: Identified installation issues and troubleshooting strategies.
    \end{itemize}

    This setup is critical as it serves as the backbone for efficient data management, analysis, and processing workflows.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps: Looking Ahead}
    The next chapters will cover key topics such as:

    \begin{enumerate}
        \item \textbf{Data Ingestion Techniques}
            \begin{itemize}
                \item Methods for gathering data from various sources (databases, APIs, flat files).
                \item Batch processing, real-time streaming, and hybrid approaches.
            \end{itemize}
        \item \textbf{Pipeline Development}
            \begin{itemize}
                \item Creating end-to-end data pipelines for automation.
                \item Frameworks like Apache Airflow or Prefect for orchestration.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Engagement}
    By mastering the setup of a data processing environment and preparing for upcoming topics, you will tackle modern data workflows.

    \begin{block}{Engagement Corner}
        \begin{itemize}
            \item \textbf{Reflect}: Consider a data project you would like to work on. What challenges might arise in data ingestion?
            \item \textbf{Prepare}: Familiarize yourself with different data ingestion tools and frameworks we will discuss next.
        \end{itemize}
    \end{block}

    Embrace these topics for effective data management and analysis in real-world applications.
\end{frame}


\end{document}