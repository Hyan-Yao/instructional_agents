\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5: Decision Trees]{Week 5: Decision Trees}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Decision Trees as a Classification Method}
    
    \begin{block}{What are Decision Trees?}
        Decision trees are a powerful, visual tool used in machine learning, specifically for classification and regression tasks. They represent decisions and their possible consequences in a tree-like structure, allowing for easy interpretation of decision-making processes.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Hierarchical Structure}: Comprised of nodes (decisions) and branches (possible outcomes), resembling a tree.
        \item \textbf{Recursive Splitting}: Each internal node splits the dataset based on a feature's value, leading to branches that represent the outcomes of a decision.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Use Decision Trees?}
    
    \begin{enumerate}
        \item \textbf{Intuitive and Easy to Understand}: The tree structure mimics human decision-making and is visually easy to interpret.
        \item \textbf{No Data Preparation Required}: Unlike many algorithms, decision trees do not require extensive data preprocessing.
        \item \textbf{Handles Both Numerical and Categorical Data}: They can be used for various data types, improving flexibility.
        \item \textbf{Identifies Interaction Between Features}: Decision trees capture relationships between several variables, making them useful in complex datasets.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Example}
    
    \begin{block}{Healthcare Prediction}
        In a medical setting, decision trees can help classify patients based on symptoms to predict whether they may have a particular disease. The decision tree would ask questions like:
        
        \begin{itemize}
            \item "Is the temperature above 100°F?"
            \item "Does the patient have a cough?"
        \end{itemize}
        
        Each 'yes' or 'no' leads to another question until a conclusion is reached (e.g., "Predicted Diagnosis: Influenza").
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    
    \begin{itemize}
        \item \textbf{Structure}: A decision tree consists of root nodes, internal nodes, and leaf nodes.
        \item \textbf{Splitting Criteria}: Common criteria for making splits include:
        \begin{itemize}
            \item \textbf{Gini Impurity}: Measures how often a randomly chosen element would be incorrectly labeled.
            \item \textbf{Entropy}: Measures the level of disorder or unpredictability in the dataset.
        \end{itemize}
        \item \textbf{Overfitting/Underfitting}: Decision trees can easily overfit training data; pruning (removing branches) ensures generalization.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Basic Algorithm Steps}
    
    \begin{enumerate}
        \item Start with the entire dataset.
        \item Choose the best feature to split on according to the chosen criterion (Gini, Entropy, etc.).
        \item Split the dataset into subsets based on the selected feature.
        \item Repeat steps 2-3 for each subset until stopping criteria are met (e.g., a minimum number of samples in a leaf node or maximum depth).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    
    Decision trees are a fundamental method in machine learning, offering a clear, interpretable way to make decisions based on data. By visualizing and analyzing decisions through a tree structure, they simplify complex decision-making processes and enhance understanding across various applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Steps}
    
    In the following slide, we will delve deeper into the mechanics of decision trees, including their definition, purpose, and types used in data mining.
    
    \textbf{Note:} Remember to actively engage in discussions and ask questions during this session to enhance your understanding!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Decision Trees - Overview}
    \begin{block}{Definition of Decision Trees}
        A decision tree is a graphical representation used to make decisions based on data. It consists of:
        \begin{itemize}
            \item Nodes: Represent decisions or classifications.
            \item Branches: Represent the outcome of decisions.
            \item Leaves: Represent final outcomes or classifications.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Decision Trees - Purpose}
    \begin{block}{Purpose of Decision Trees in Data Mining}
        \begin{itemize}
            \item \textbf{Interpretability:} Easy to understand for stakeholders.
            \item \textbf{Flexibility:} Applicable for both classification and regression tasks.
            \item \textbf{No Assumptions Required:} Does not assume a specific data distribution.
            \item \textbf{Handling Non-Linear Relationships:} Can capture complex relationships without transformations.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Decision Trees - Example and Key Points}
    \begin{block}{Example of a Decision Tree}
        \textbf{Scenario:} Predicting whether a customer will buy a product based on age and income.
        \begin{enumerate}
            \item Start with a decision on age group.
            \item Make a decision about income level based on age group.
            \item Final decision: ``Will Buy'' or ``Will Not Buy''.
        \end{enumerate}
        \begin{lstlisting}[basicstyle=\small]
                      [Age < 30]
                     /           \
                   [Yes]        [No]
                  /                \
          [Income < 50K]        [Income < 80K]
              /      \              /      \
           [Yes]     [No]       [Yes]    [No]
           (Will    (Will        (Will    (Will
           Buy)     Not Buy)     Buy)    Not Buy)
        \end{lstlisting}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Structure:} Comprised of nodes, branches, and leaves.
            \item \textbf{Advantages:} Clear visualization, interpretable decisions, and versatility.
            \item \textbf{Applications:} Marketing, finance, healthcare, and more.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Components of Decision Trees - Overview}
  \begin{block}{Understanding the Structure of Decision Trees}
    Decision trees are powerful tools in data mining and machine learning, commonly used for classification and regression tasks. Their fundamental components include:
  \end{block}
  
  \begin{itemize}
    \item \textbf{Nodes}
    \item \textbf{Branches}
    \item \textbf{Leaves}
    \item \textbf{Paths}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Components of Decision Trees - Detailed Breakdown}
  
  \begin{block}{1. Nodes}
    \begin{itemize}
      \item Decision points in the tree.
      \begin{itemize}
        \item \textbf{Decision Nodes:} Questions or tests on an attribute (e.g., "Is the temperature > 70°F?").
        \item \textbf{Root Node:} The topmost node containing the entire dataset at the start.
      \end{itemize}
    \end{itemize}
  \end{block}

  \begin{block}{2. Branches}
    \begin{itemize}
      \item Links between nodes, representing outcomes of a test.
      \item Example: Branches might lead to classifications such as young adults, middle-aged, seniors.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Components of Decision Trees - Continuation}

  \begin{block}{3. Leaves}
    \begin{itemize}
      \item Terminal nodes containing the final output.
      \item Example: A leaf could indicate "Yes" if the customer is likely to buy the product.
    \end{itemize}
  \end{block}

  \begin{block}{4. Paths}
    \begin{itemize}
      \item Sequence of nodes and branches leading from the root to a leaf.
      \item Example: A path might evaluate both age and income before ending with a purchase decision.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Illustrative Example of a Decision Tree}
  Consider the simple decision tree below:

  \begin{block}{Decision Tree Example}
      \begin{verbatim}
          [Age Group?]
            /    |    \
      [Young] [Adult] [Senior]
         /         |       \
    [Purchases]   [Doubt]  [Decline]
       (Yes)        |        (No)
                  [Yes]
      \end{verbatim}
  \end{block}

  \begin{itemize}
    \item \textbf{Root Node:} Age Group
    \item \textbf{Branches:} Lead to different age classifications
    \item \textbf{Leaves:} Final outcomes of purchase likelihood
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  
  \begin{itemize}
    \item Understanding these components is crucial for interpreting decision trees.
    \item Each node's decision influences the subsequent path, leading to classification or regression outcomes.
    \item Decision trees streamline data-driven decisions across various sectors.
  \end{itemize}

  \begin{block}{Real-World Context}
    Consider applications like predicting customer behavior or diagnosing conditions to enhance your understanding.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Introduction}
    \begin{block}{Introduction to Decision Trees}
        Decision Trees are a popular machine learning model used for both classification and regression tasks. They mimic human decision-making by creating a tree-like structure of decisions based on specific attributes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Steps}
    \begin{block}{Process of Constructing Decision Trees}
        The construction involves several key steps:
        \begin{enumerate}
            \item \textbf{Data Collection}: Gather relevant data that includes both features and the target variable.
            \item \textbf{Preprocessing}: Clean the data by handling missing values and encoding categorical variables.
            \item \textbf{Choosing a Splitting Criterion}: Decide how to split the dataset using metrics like:
                \begin{itemize}
                    \item Gini Impurity
                    \item Information Gain (Entropy)
                \end{itemize}
            \item \textbf{Building the Tree}:
                \begin{itemize}
                    \item Start at the root with the entire dataset.
                    \item Split the data based on the chosen criterion.
                    \item Repeat the process recursively until stopping conditions are met.
                \end{itemize}
            \item \textbf{Pruning the Tree}: Remove branches that have little importance to enhance model generalization.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Example}
    \begin{block}{Example of Building a Decision Tree}
        Consider a dataset of loan applicants with features: credit score, income, and loan amount. The target variable is loan approval (Yes/No).
        \begin{enumerate}
            \item \textbf{Initial Node}: Calculate Gini Impurity or Information Gain to determine the first feature to split.
            \item \textbf{First Split}: If splitting on 'credit score', create branches:
                \begin{itemize}
                    \item High credit score (approved)
                    \item Low credit score (not approved)
                \end{itemize}
            \item \textbf{Further Splits}: Continue splitting based on features like 'income' in the low credit score group.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Effectiveness depends on the choice of the splitting criterion.
            \item Decision Trees are interpretable and useful in scenarios like finance and healthcare.
            \item Overfitting is a common challenge; pruning and setting a maximum depth can help.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Conclusion}
    \begin{block}{Conclusion}
        The construction of decision trees combines data analysis and algorithmic techniques. Understanding the process is critical for building effective predictive models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Code Example}
    \begin{block}{Python Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier

# Example dataset
X = [[600, 30000, 2000], [700, 50000, 10000], [560, 40000, 1500]]
y = ['No', 'Yes', 'No']

# Creating the model
model = DecisionTreeClassifier(criterion='gini')
model.fit(X, y)
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building Decision Trees - Summary}
    \begin{block}{Summary}
        By understanding the steps of building decision trees, students can effectively implement these algorithms in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Criteria - Introduction}
    \begin{itemize}
        \item In decision tree construction, the choice of splitting criteria is vital for model effectiveness.
        \item Commonly used criteria include \textbf{Gini impurity} and \textbf{entropy}.
        \item These metrics help quantify the quality of a split and guide the decision-making process in tree construction.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Criteria - Gini Impurity}
    \begin{block}{Definition}
        Gini impurity measures the degree of impurity in a dataset.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
            Gini(D) = 1 - \sum_{i=1}^{C} p_i^2
        \end{equation}
        \textbf{Where:}
        \begin{itemize}
            \item \( D \) is the dataset
            \item \( C \) is the number of classes
            \item \( p_i \) is the proportion of class \( i \) in the dataset
        \end{itemize}
    \end{block}
    
    \begin{block}{Interpretation}
        A lower Gini impurity indicates a more "pure" node.
    \end{block}
    
    \begin{block}{Example}
        Given class distribution:
        \begin{itemize}
            \item Class A: 5
            \item Class B: 3
            \item Class C: 2
        \end{itemize}
        \[
        Gini(D) = 1 - \left( \left(\frac{5}{10}\right)^2 + \left(\frac{3}{10}\right)^2 + \left(\frac{2}{10}\right)^2 \right) = 0.62
        \]
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Criteria - Entropy}
    \begin{block}{Definition}
        Entropy measures the amount of uncertainty in a dataset.
    \end{block}
    
    \begin{block}{Formula}
        \begin{equation}
            Entropy(D) = - \sum_{i=1}^{C} p_i \log_2(p_i)
        \end{equation}
        \textbf{Where:}
        \begin{itemize}
            \item \( p_i \) is the proportion of class \( i \) in the dataset
        \end{itemize}
    \end{block}
    
    \begin{block}{Interpretation}
        Lower entropy indicates less disorder and higher purity.
    \end{block}
    
    \begin{block}{Example}
        Using the same dataset:
        \[
        Entropy(D) \approx 1.57
        \]
    \end{block}
    
\end{frame}

\begin{frame}[fragile]
    \frametitle{Splitting Criteria - Key Points and Summary}
    \begin{itemize}
        \item Both Gini impurity and entropy are critical for assessing dataset splits.
        \item Gini impurity is computationally faster; entropy is more precise.
        \item Their minimization directly influences the predictive performance of decision trees.
    \end{itemize}
    
    \begin{block}{Summary}
        Understanding these criteria is essential for building accurate decision trees, leading to better model performance on unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Pruning Techniques - Importance}

    \begin{block}{Overview}
        Pruning is a crucial technique in decision tree algorithms aimed at improving the model's performance by mitigating overfitting.
    \end{block}

    \begin{itemize}
        \item Overfitting occurs when a model learns too much detail from the training data, capturing noise instead of the underlying distribution.
        \item A well-pruned model performs exceptionally well on unseen data, unlike an overfitted model.
    \end{itemize}
    
    \begin{block}{Why Prune?}
        Pruning helps:
        \begin{itemize}
            \item Improve model generalization to new data
            \item Reduce model complexity for better interpretability
            \item Enhance predictive performance by eliminating noise
        \end{itemize}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Pruning Techniques - Methods}

    \begin{block}{Types of Pruning Techniques}
        \begin{enumerate}
            \item \textbf{Pre-Pruning (Early Stopping):}
                \begin{itemize}
                    \item Stops tree growth early based on criteria (e.g., minimum samples).
                    \item \textbf{Example:} A node with fewer than 10 data points may not split further.
                \end{itemize}

            \item \textbf{Post-Pruning:}
                \begin{itemize}
                    \item Grows a full tree then removes nodes with little predictive power.
                    \item \textbf{Method:} Cost Complexity Pruning - balances tree size with training accuracy.
                    \item \textbf{Example:} If a node's growth increases validation error, it can be pruned.
                \end{itemize}
        \end{enumerate}
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Pruning Techniques - Example & Conclusion}

    \begin{block}{Example Code Snippet}
        \begin{lstlisting}[language=Python]
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset and split
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

# Initialize and fit a Decision Tree
tree = DecisionTreeClassifier(ccp_alpha=0.01)  # ccp_alpha is the complexity parameter
tree.fit(X_train, y_train)

# Predictions and Evaluation
y_pred = tree.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
        \end{lstlisting}
    \end{block}

    \begin{block}{Conclusion}
        Pruning enhances decision trees by ensuring they generalize well to unseen data. Choosing the right method, whether pre-pruning or post-pruning, is crucial for optimal performance.
    \end{block}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Decision Trees - Understanding Decision Trees}
    
    A decision tree is a flow-chart-like structure used to make decisions based on various input features. 

    \begin{itemize}
        \item Each internal node represents a decision based on a given feature.
        \item Each branch represents the outcome of that decision.
        \item Each leaf node represents a final decision or classification.
    \end{itemize}

    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Nodes}:
                \begin{itemize}
                    \item \textbf{Root Node}: The top node of the tree, representing the entire dataset.
                    \item \textbf{Decision Nodes}: Nodes that split the data based on certain conditions.
                    \item \textbf{Leaf Nodes}: Final outputs or classifications after all decisions have been made.
                \end{itemize}
            \item \textbf{Branches}: Connections representing the outcome of a decision.
            \item \textbf{Depth}: Refers to the longest path from the root node to a leaf node.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Decision Trees - Reading a Decision Tree}

    To read a decision tree, follow these steps:
    \begin{itemize}
        \item Start at the \textbf{root node} and make decisions based on the features at each decision node.
        \item Move along the branches based on the answers (i.e., true/false or specific values) to reach a \textbf{leaf node}.
        \item The final classification or decision is found in the leaf node reached.
    \end{itemize}

    \begin{block}{Example}
        \textbf{Consider a simple decision tree for classifying whether a person is likely to buy a product based on age and income:}
        
        \begin{verbatim}
                          [Age ≤ 30]
                          /       \
                       Yes       No
                          |        [Income > 50k]
                       [Buy]        /     \
                                   Yes     No
                                   [Buy]  [Don't Buy]
        \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Decision Trees - Key Points and Application}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Clarity}: Decision trees visually represent the logic behind decisions.
            \item \textbf{Interpretability}: Each path can be followed to understand how a decision is derived.
            \item \textbf{Transparency}: Provides insight into decision boundaries, enabling easier adjustments.
        \end{itemize}
    \end{block}

    \begin{block}{Practical Application}
        \begin{itemize}
            \item \textbf{Business}: Evaluate customer transactions to predict acceptance of new services.
            \item \textbf{Healthcare}: Classify patient risk levels based on symptoms and demographics.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Interpreting decision trees allows stakeholders to understand and trust machine learning models, facilitating better decision-making across various contexts.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Interpreting Decision Trees - Code Snippet}

    Here is a simple example using \texttt{scikit-learn} to visualize a decision tree:

    \begin{lstlisting}[language=Python]
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_text

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Train a decision tree
clf = DecisionTreeClassifier()
clf.fit(X, y)

# Print the decision tree
print(export_text(clf, feature_names=iris.feature_names))
    \end{lstlisting}

    This code loads the Iris dataset, trains a decision tree classifier, and prints the tree structure.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages of Decision Trees}

    Decision Trees are a popular machine learning algorithm used for classification and regression tasks. They split data into branches to make decisions based on feature values, leading eventually to an outcome or a prediction.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Advantages}

    \begin{enumerate}
        \item \textbf{Simplicity}
        \begin{itemize}
            \item \textbf{Clear Structure:} Decision Trees present decisions as a series of branching nodes leading to final outcomes.
            \item \textbf{Easy to Understand:} Even non-experts can grasp the logic by following the paths.
        \end{itemize}
        
        \item \textbf{Interpretability}
        \begin{itemize}
            \item \textbf{Visual Representation:} The tree structure visually represents decisions, allowing easy tracing of predictions.
            \item \textbf{Feature Importance:} Users can identify which features are most influential, improving transparency.
        \end{itemize}
        
        \item \textbf{Performance}
        \begin{itemize}
            \item \textbf{Handling Non-Linearity:} Captures complex interactions between features without data transformation.
            \item \textbf{No Assumptions About Data Distribution:} Robust against various data types and distributions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of Advantages}

    \textbf{Simplicity Example:}
    \begin{lstlisting}
    Is it sunny?
    ├─ Yes → Is temperature > 75°F?
    │   ├─ Yes: Play outside
    │   └─ No: Stay inside
    └─ No: Stay inside
    \end{lstlisting}

    \textbf{Performance Example:}
    In a medical diagnosis, a Decision Tree can classify whether a patient has a specific disease based on symptoms and lab results, even if the relationships are not linear.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Key Points}

    \begin{itemize}
        \item \textbf{Simplicity:} Decision Trees are straightforward and understandable.
        \item \textbf{Interpretability:} They provide a clear rationale for decisions made.
        \item \textbf{Performance:} Effective in diverse datasets without strict assumptions about data distribution.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}

    Decision Trees are a potent tool in machine learning, valued for their simplicity, interpretability, and strong performance. Their ability to mirror human decision-making processes makes them persuasive for both data scientists and stakeholders.

    \textbf{Next Steps:} 
    In the following slide, we will discuss the limitations of Decision Trees, including their potential for overfitting and sensitivity to variations in data.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of Decision Trees}
  \begin{block}{Introduction}
    While decision trees are a popular tool in machine learning due to their interpretability and ease of use, they come with several limitations that can impact their effectiveness in certain scenarios. Understanding these drawbacks is crucial for any data scientist or analyst.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of Decision Trees - Overfitting}
  \begin{itemize}
    \item \textbf{Definition:} Overfitting occurs when a model performs well on training data but poorly on unseen data due to excessive complexity.
    \item \textbf{Explanation:} Decision trees can become too intricate by growing deep (with many splits), capturing noise or peculiarities in the training data rather than the underlying distribution. This makes them less generalizable.
  \end{itemize}

  \begin{block}{Example}
    Imagine training a decision tree to classify animals based on features (color, number of legs, etc.). If the tree splits excessively based on rare attributes (like a unique fur color), 
    it may misclassify other animals in real-world scenarios.
  \end{block}

  \begin{itemize}
    \item \textbf{Mitigation Techniques:}
      \begin{itemize}
        \item Pruning: This process involves cutting back the tree to prevent it from growing too complex.
        \item Setting Maximum Depth: Limiting the depth of the tree during creation.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Limitations of Decision Trees - Sensitivity to Data Variations}
  \begin{itemize}
    \item \textbf{Definition:} Decision trees can be heavily influenced by small changes in the training data.
    \item \textbf{Explanation:} A slight alteration in input data can lead to a completely different tree structure, making the model unstable.
  \end{itemize}

  \begin{block}{Example}
    If we have a dataset where a few samples are outliers (e.g., a few dogs listed as having six legs), a decision tree may create a branching path based on this incorrect information, impacting its accuracy on new data.
  \end{block}

  \begin{itemize}
    \item \textbf{Mitigation Techniques:}
      \begin{itemize}
        \item Ensemble Methods: Techniques such as Random Forests combine multiple decision trees, stabilizing predictions.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points and Conclusion}
  \begin{itemize}
    \item Decision trees can easily become too complex, leading to overfitting.
    \item They are sensitive to variations in training data; small changes can significantly impact the model’s predictions.
    \item Effective management strategies like pruning, limiting tree depth, or utilizing ensemble methods can help mitigate these limitations.
  \end{itemize}

  \begin{block}{Conclusion}
    Despite their drawbacks, understanding the limitations of decision trees allows practitioners to take informed steps 
    to enhance their predictive performance, thus improving the decision-making process in various applications.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Decision Trees}
    \begin{block}{Overview of Decision Trees}
        Decision trees are a powerful predictive modeling technique used in various industries to make decisions based on data-driven insights. They represent decisions and their possible consequences in a tree-like structure, making it easy to visualize the decision-making process.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Industries Utilizing Decision Trees}
    \begin{enumerate}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textit{Predicting Disease Outcomes}
                \item Decision trees assess the likelihood of diseases based on patient attributes (e.g., age, symptoms).
                \item Example: Diagnosing diabetes by evaluating factors like blood pressure, BMI, and glucose levels.
            \end{itemize}

        \item \textbf{Finance}
            \begin{itemize}
                \item \textit{Credit Scoring}
                \item Used to identify potential borrowers' creditworthiness based on income, repayment history, and debt-to-income ratio.
                \item Key Point: Minimizes risk and maximizes profitability using historical lending data.
            \end{itemize}

        \item \textbf{Marketing}
            \begin{itemize}
                \item \textit{Customer Segmentation}
                \item Segmentation based on purchasing behavior and preferences.
                \item Key Point: Allows targeted marketing strategies and personalized offers.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{More Industries Utilizing Decision Trees}
    \begin{enumerate}[start=4]
        \item \textbf{Manufacturing}
            \begin{itemize}
                \item \textit{Quality Control}
                \item Identify likelihood of product defects based on manufacturing parameters.
                \item Key Point: Facilitates real-time adjustments, improves quality, and reduces waste.
            \end{itemize}

        \item \textbf{Retail}
            \begin{itemize}
                \item \textit{Inventory Management}
                \item Forecasts product demand based on seasonality, market trends, and sales data.
                \item Key Point: Enables efficient inventory stocking, minimizing overstock or stockouts.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary of Decision Trees}
    \begin{itemize}
        \item \textbf{Clarity}: Their visual format simplifies complex decision-making.
        \item \textbf{Versatile Applications}: Utilized in healthcare, finance, marketing, manufacturing, and retail.
        \item \textbf{Data-Driven Decisions}: Leverage historical data for informed future actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Important Considerations}
    \begin{itemize}
        \item \textbf{Overfitting}: Balance tree depth to prevent overfitting.
        \item \textbf{Interpretability}: Easily interpretable, making them accessible to non-technical stakeholders.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Software Tools for Decision Trees}
    \begin{block}{Overview}
        Decision trees are a powerful tool for classification and regression tasks in data science. Various software tools facilitate their implementation.
    \end{block}
    \begin{itemize}
        \item Focused tools: R and Python (Scikit-learn).
        \item Other notable tools include WEKA and Microsoft Azure ML.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{R for Decision Trees}
    \begin{block}{Description}
        R is widely used for statistical computing and offers several packages for decision trees.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Packages:}
            \begin{itemize}
                \item \texttt{rpart}: Recursive partitioning for classification and prediction.
                \item \texttt{party}: Conditional inference trees to reduce bias.
                \item \texttt{tree}: Simple implementation for basic decision trees.
            \end{itemize}
    \end{itemize}
    \begin{block}{Code Example}
    \begin{lstlisting}[language=R]
# Load necessary library
library(rpart)

# Create a decision tree model
fit <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)

# Visualize the tree
plot(fit)
text(fit, use.n = TRUE)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Python (Scikit-learn) for Decision Trees}
    \begin{block}{Description}
        Python with Scikit-learn is a popular choice due to its ease of use for implementing machine learning algorithms.
    \end{block}
    \begin{itemize}
        \item \textbf{Key Features:}
            \begin{itemize}
                \item User-friendly interface for building decision trees.
                \item Visualization support via Graphviz.
                \item Handles both classification and regression trees.
            \end{itemize}
    \end{itemize}
    \begin{block}{Code Example}
    \begin{lstlisting}[language=Python]
# Import libraries
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Create and fit model
clf = DecisionTreeClassifier()
clf.fit(X, y)

# Visualize the tree
tree.plot_tree(clf)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Other Tools and Key Points}
    \begin{block}{Other Tools}
        \begin{itemize}
            \item \textbf{WEKA:} Java-based collection of machine learning algorithms; ideal for beginners with GUI.
            \item \textbf{Microsoft Azure ML:} User-friendly platform with drag-and-drop features for decision trees.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item User-friendly interfaces help beginners explore decision trees without extensive coding.
            \item Versatile applications across various domains, including healthcare and finance.
            \item Visualization tools, such as Graphviz, simplify model interpretation.
        \end{itemize}
    \end{block}
    \begin{block}{Next Steps}
        \begin{itemize}
            \item Apply decision tree algorithms in an interactive session using R or Python.
            \item Engage with materials and ask questions during the hands-on activity.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    \begin{itemize}
        \item In this hands-on activity, you'll learn to construct a decision tree using a provided dataset.
        \item Decision Trees are powerful tools for classification and regression tasks in machine learning.
        \item They recursively split the data into subsets based on feature values, leading to decisions or predictions.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Learning Objectives}
    \begin{itemize}
        \item Understand the basic principles of decision trees.
        \item Gain practical experience in building a decision tree using a real dataset.
        \item Learn to evaluate the performance of the created decision tree.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Concept Explanation}
    A Decision Tree is a flowchart-like structure where:
    \begin{itemize}
        \item Each internal node represents a feature (attribute).
        \item Each branch represents a decision rule.
        \item Each leaf node represents an outcome (class label).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Steps to Building a Decision Tree}
    \begin{enumerate}
        \item Select a Feature: Choose the feature that best separates the data.
        \item Create a Split: Divide the dataset into subsets based on the chosen feature.
        \item Repeat: Recursively repeat this for each subset until a stopping criterion is met.
        \item Prune the Tree: Simplify the tree to reduce overfitting.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example Dataset}
    Consider a simple dataset of patients:
    \begin{tabular}{|c|c|c|}
        \hline
        Age & Blood Pressure & Outcome \\
        \hline
        25  & 120 & Healthy \\
        30  & 130 & Healthy \\
        40  & 140 & Hypertension \\
        50  & 160 & Hypertension \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Building the Decision Tree}
    Key steps:
    \begin{itemize}
        \item Calculate metrics such as Gini impurity or information gain to determine the best feature for the first split.
        \item Use "Age" as the first feature because it provides the highest information gain.
        \item Split the dataset on Age:
            \begin{itemize}
                \item Age < 40: Healthy
                \item Age $\geq$ 40: Hypertension
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hands-On Instructions}
    \begin{enumerate}
        \item Dataset Access: Download the provided dataset from the link shared in class.
        \item Set Up Environment:
            \begin{itemize}
                \item Ensure Python is installed with Scikit-learn.
                \item Use Jupyter Notebook or any Python IDE.
            \end{itemize}
    \end{enumerate}
    \begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('patients.csv')

# Preprocessing
X = data[['Age', 'Blood Pressure']]
y = data['Outcome']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Train the model
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Visualize the tree
tree.plot_tree(clf)
plt.show()
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Interpretability: Decision trees are easy to understand and interpret.
        \item Feature Importance: They indicate which features are most influential in predictions.
        \item Overfitting: Pruning or setting constraints is important to prevent complexity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Next Steps}
    After constructing your decision tree:
    \begin{itemize}
        \item Evaluate its accuracy using \texttt{clf.score(X\_test, y\_test)}.
        \item Discuss potential improvements or alternative methods to streamline the model.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    This activity familiarizes you with decision trees and their applications, showcasing the impact of machine learning in real-world decision-making processes in areas such as healthcare analytics.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Overview}
    \begin{block}{Overview}
        Decision trees are powerful tools in data mining, allowing for the visualization of decision-making processes. 
        However, their implementation raises various ethical implications that practitioners must consider for responsible data use.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Bias in Data}
    \begin{itemize}
        \item \textbf{Bias in Data}:
        \begin{itemize}
            \item \textbf{Definition}: Bias occurs when certain groups are unfairly represented or treated in the data, leading to skewed outcomes.
            \item \textbf{Example}: A decision tree predicting loan approvals trained on historical data that reflects discrimination against minority groups may deny loans to deserving applicants based on race or ethnicity.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Transparency, Privacy, and Consent}
    \begin{itemize}
        \item \textbf{Transparency and Interpretability}:
        \begin{itemize}
            \item \textbf{Definition}: Decision trees are generally interpretable, but complexity can obscure clarity.
            \item \textbf{Example}: Simple decision trees show nodes clearly, but complex models can be intricate, making it hard for stakeholders to understand decisions.
        \end{itemize}

        \item \textbf{Data Privacy}:
        \begin{itemize}
            \item \textbf{Definition}: Data privacy involves obligations to protect individual information from unauthorized access and use.
            \item \textbf{Example}: Using sensitive data (e.g., health records) without consent for building decision trees can lead to legal violations.
        \end{itemize}

        \item \textbf{Informed Consent}:
        \begin{itemize}
            \item \textbf{Definition}: Participants should be aware of and agree to how their data is collected, used, and shared.
            \item \textbf{Example}: A company must inform customers of how their behavioral data will be utilized and obtain explicit consent before building decision trees.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Key Points}
    \begin{itemize}
        \item \textbf{Mitigating Bias}: Analyze datasets for bias before model training; strive for fairness in algorithms.
        \item \textbf{Ensuring Transparency}: Communicate the decision-making process clearly to stakeholders and users.
        \item \textbf{Protecting Privacy}: Cleanse and anonymize data to protect user identities and comply with legal standards.
        \item \textbf{Obtaining Consent}: Seek informed consent from data subjects before utilizing their data in decision-making models.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Gini Impurity}
    \begin{block}{Key Formula for Gini Impurity}
        \begin{equation}
            Gini(p) = 1 - \sum (p_i)^2
        \end{equation}
        Where \( p_i \) is the probability of class \( i \) in a node.
    \end{block}

    \begin{block}{Example Python Code to Compute Gini Impurity}
        \begin{lstlisting}[language=Python]
def gini_impurity(classes):
    total = sum(classes.values())
    return 1 - sum((count / total) ** 2 for count in classes.values())

# Example Usage
classes = {'A': 10, 'B': 5}
print(gini_impurity(classes))  # Output for the Gini impurity
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations - Concluding Remarks}
    \begin{block}{Concluding Remarks}
        The application of decision trees must be approached thoughtfully, taking into account the ethical dimensions that can affect individuals and communities. By acknowledging these ethical considerations, practitioners can foster responsible data usage that promotes equity, transparency, and respect for individuals.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Decision Trees}
    \begin{block}{Overview of Decision Trees}
        Decision trees are a powerful tool for classification and regression tasks in machine learning, visualizing decisions and consequences.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Nodes} represent features or attributes.
        \item \textbf{Branches} represent decision rules.
        \item \textbf{Leaves} represent outcomes or final decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts Discussed}
    \begin{enumerate}
        \item \textbf{Structure of Decision Trees}
            \begin{itemize}
                \item \textbf{Root Node}: Starting point represents the entire dataset.
                \item \textbf{Internal Nodes}: Tests on features (e.g., "Is Age > 30?").
                \item \textbf{Leaves}: Final decisions (e.g., "Approve Loan").
            \end{itemize}

        \item \textbf{Splitting Criteria}
            \begin{itemize}
                \item Importance of maximizing information gain.
                \item Common techniques: 
                \begin{itemize}
                    \item \textbf{Gini Impurity}: Measures likelihood of incorrect classification.
                    \item \textbf{Information Gain}: Reduction in uncertainty after a split.
                \end{itemize}
            \end{itemize}
    
        \item \textbf{Pruning}
            \begin{itemize}
                \item Reduces overfitting by removing ineffective tree sections.
                \item Improves model generalization.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Advantages and Disadvantages}
    \begin{block}{Advantages of Decision Trees}
        \begin{itemize}
            \item Easy to interpret and visualize.
            \item Requires minimal data preparation (no need for feature scaling).
            \item Handles both numerical and categorical data effectively.
        \end{itemize}
    \end{block}
    
    \begin{block}{Disadvantages of Decision Trees}
        \begin{itemize}
            \item Prone to overfitting if not tuned.
            \item Decision boundaries are often axis-aligned, limiting pattern capture.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications}
    \begin{itemize}
        \item \textbf{Healthcare}: Diagnose diseases based on patient symptoms.
        \item \textbf{Finance}: Evaluate credit risk using applicant attributes.
        \item \textbf{Marketing}: Segment customers to tailor sales strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item Decision trees are intuitive for classification and regression tasks.
        \item Understanding splitting criteria is crucial for effective tree building.
        \item Regular pruning enhances performance and prevents overfitting.
        \item Consider real-world context for informed model decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Basic Decision Tree for Loan Approval}
    \begin{center}
    \texttt{
    \begin{tabular}{c}
                     [Income > 50k] \\
                            /    \\
                          Yes      No \\
                          /         \\
                  [Credit Score > 700]  Decline \\
                         /   \\
                       Yes     No \\
                       /        \\
                     Approve   Review
    \end{tabular}
    }
    \end{center}
    \begin{block}{Description}
        In this simple tree, applicants are divided based on income, then assessed based on credit score.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Purpose}
    \begin{itemize}
        \item \textbf{Encourage Engagement}: This open forum provides an opportunity for students to clarify any doubts regarding decision trees—a popular machine learning model.
        \item \textbf{Facilitate Understanding}: Address complicated concepts with practical examples to enhance comprehension.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Key Concepts}
    \begin{enumerate}
        \item \textbf{What are Decision Trees?}
        \begin{itemize}
            \item A flowchart-like structure where each internal node represents a test on an attribute.
            \item Each branch represents the outcome, and each leaf represents a class label.
            \item Used for classification and regression tasks.
        \end{itemize}
        
        \item \textbf{How Do Decision Trees Work?}
        \begin{itemize}
            \item \textbf{Splitting}: Partitions data into subsets based on feature values.
            \item \textbf{Criteria for Splitting}: Includes Gini Impurity, Entropy (Information Gain), and Mean Squared Error for regression.
        \end{itemize}
        
        \item \textbf{Overfitting and Pruning}
        \begin{itemize}
            \item Overfitting: Occurs when a model learns noise in the training data.
            \item Pruning: Mitigates overfitting by removing sections of the tree that offer little classification power.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Q\&A Session - Examples and Discussion Points}
    \begin{itemize}
        \item \textbf{Example Scenarios}:
        \begin{itemize}
            \item \textbf{Classification Example}: Decision tree to determine if to play tennis based on weather conditions.
            \item \textbf{Regression Example}: Estimating housing prices through features like bedroom count and size.
        \end{itemize}
        
        \item \textbf{Points for Discussion}:
        \begin{itemize}
            \item Real-World Applications: In finance, healthcare, and marketing.
            \item Integration with Other Models: Combining decision trees with techniques like Random Forests and Gradient Boosting.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Final Thoughts}
        Decision trees provide a transparent visualization of decision-making in machine learning, emphasizing the importance of understanding fundamentals for effective application in various contexts.
    \end{block}
    
\end{frame}


\end{document}