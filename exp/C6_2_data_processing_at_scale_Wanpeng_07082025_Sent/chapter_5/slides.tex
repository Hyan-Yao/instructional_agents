\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Week 5: Data Processing with Spark]{Week 5: Data Processing with Spark}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Processing with Spark}
    \begin{block}{Overview of Spark}
        Apache Spark is a fast, general-purpose cluster-computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It excels in in-memory data processing, enhancing performance compared to traditional disk-based processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{Data Processing with Spark}
        \begin{itemize}
            \item Simplifies big data processing with a unified model for batch and stream processing.
            \item Focus this week on \textbf{batch data processing} and \textbf{Spark SQL}.
        \end{itemize}

        \item \textbf{Batch Data Processing}
        \begin{itemize}
            \item Processes large volumes of data accumulated over time as a single batch.
            \item Efficiently handles batch jobs with complex operations using a high-level API.
            \item \textbf{Example:} Analyzing historical sales data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Batch Processing}
    \begin{lstlisting}[language=Python]
    from pyspark.sql import SparkSession

    spark = SparkSession.builder.appName("Sales Analysis").getOrCreate()
    sales_df = spark.read.csv("sales_data.csv", header=True, inferSchema=True)

    total_sales_per_region = sales_df.groupBy("region").sum("amount")
    total_sales_per_region.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts (continued)}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Spark SQL}
        \begin{itemize}
            \item Enables SQL queries on big data using the DataFrames API.
            \item Combines SQL capabilities with Spark's optimization features for structured data processing.
            \item \textbf{Example: Registering a DataFrame as a SQL temporary view}.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Spark SQL}
    \begin{lstlisting}[language=Python]
    sales_df.createOrReplaceTempView("sales")

    high_sales_region = spark.sql("""
        SELECT region, SUM(amount) as total_sales 
        FROM sales 
        GROUP BY region 
        HAVING total_sales > 100000
    """)
    high_sales_region.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{In-memory Processing:} Significantly reduces latency and enhances speed.
        \item \textbf{Unified Data Processing:} Supports both batch and streaming data with a single framework.
        \item \textbf{Ease of Use:} High-level APIs and SQL support make it accessible for data engineers and analysts.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This week, we will explore how batch processing and Spark SQL interconnect for efficient data handling. Expect to learn about architecture, DataFrames, and practical implementation. Remember, Spark's timely processing of large datasets is vital for real-time analytics and decision-making across industries.
\end{frame}

\begin{frame}[fragile]{Learning Objectives - Overview}
    This week, we will delve into essential concepts surrounding data processing with Apache Spark. By the end of the week, you will be able to:
    \begin{enumerate}
        \item Understand Spark Architecture
        \item Explore DataFrames
        \item Utilize Spark SQL
        \item Implement Data Processing Tasks
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Understanding Spark Architecture}
    \begin{block}{Spark Architecture}
        Apache Spark architecture consists of a driver (the main program) and multiple executors (workers) that process distributed data.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Driver}: Coordinates and schedules tasks.
        \item \textbf{Executors}: Run tasks and store data.
        \item \textbf{Cluster Manager}: Manages resources across nodes.
    \end{itemize}
    
    \begin{block}{Illustration}
        A simple diagram showing the relationship between the driver, executors, and cluster manager can enhance understanding.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Exploring DataFrames}
    \begin{block}{DataFrames}
        DataFrames are distributed collections of data organized into named columns, similar to a table in a relational database.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Advantages}:
        \begin{itemize}
            \item Optimized execution via Catalyst optimizer and Tungsten execution engine.
            \item Ability to handle both structured and semi-structured data.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example: Loading a DataFrame}
        \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()
df = spark.read.json("data.json")
df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Utilizing Spark SQL}
    \begin{block}{Spark SQL}
        Spark SQL enables the querying of structured data via SQL and integrates relational data processing with Spark's functional programming.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Key Features}:
        \begin{itemize}
            \item Supports multiple data sources (Hive, Avro, Parquet).
            \item Can run SQL queries directly against DataFrames.
        \end{itemize}
    \end{itemize}
    
    \begin{block}{Example: Executing a SQL Query}
        \begin{lstlisting}[language=python]
df.createOrReplaceTempView("table")
sqlDF = spark.sql("SELECT * FROM table WHERE age > 30")
sqlDF.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Implementing Data Processing Tasks}
    \begin{block}{Data Processing Tasks}
        Applying transformations and actions on DataFrames to process data for analysis and reporting.
    \end{block}

    \begin{itemize}
        \item \textbf{Types of Operations}:
        \begin{itemize}
            \item \textbf{Transformations}: Lazy operations like `filter()` and `groupBy()` that create a new DataFrame.
            \item \textbf{Actions}: Immediate operations like `show()` and `count()` that trigger execution.
        \end{itemize}
    \end{itemize}

    \begin{block}{Example: Grouping and Counting Records}
        \begin{lstlisting}[language=python]
df.groupBy("city").count().show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Big Data Systems Architecture - Overview}
  \begin{block}{Overview of Big Data Systems Architecture}
    Big data systems handle large volumes of diverse data quickly. Their architecture comprises several layers:
  \end{block}
  \begin{enumerate}
    \item \textbf{Data Sources}: Includes databases, IoT devices, social media, logs, etc.
    \item \textbf{Data Ingestion Layer}: Collects data using tools like Apache Kafka and Apache Flume.
    \item \textbf{Data Processing Layer}: The core, involving batch processing and stream processing.
    \item \textbf{Data Storage Layer}: Stores processed data, utilizing HDFS, NoSQL databases, or cloud solutions.
    \item \textbf{Data Analysis \& Processing Layer}: Utilizes frameworks like Apache Spark for data analysis.
    \item \textbf{Data Presentation Layer}: Makes results accessible via dashboards and reports.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Big Data Systems Architecture - Processing Paradigms}
  \begin{block}{Batch vs. Stream Processing}
    \textbf{Batch Processing}:
    \begin{itemize}
      \item \textbf{Definition}: Processes large volumes of data at intervals.
      \item \textbf{Tools}: Hadoop MapReduce, Apache Spark (batch mode).
      \item \textbf{Examples}: Monthly sales reports, data warehousing.
    \end{itemize}
    
    \textbf{Stream Processing}:
    \begin{itemize}
      \item \textbf{Definition}: Processes data in real time for immediate insights.
      \item \textbf{Tools}: Apache Kafka, Apache Storm, Apache Spark Streaming.
      \item \textbf{Examples}: Real-time fraud detection, social media analysis.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Differences Between Processing Paradigms}
  \begin{block}{Key Differences}
    \begin{tabular}{|l|l|l|}
      \hline
      \textbf{Feature} & \textbf{Batch Processing} & \textbf{Stream Processing} \\
      \hline
      Data Handling & Large sets of data & Continuous stream of data \\
      \hline
      Latency & Higher latency (minutes/hours) & Low latency (milliseconds) \\
      \hline
      Use Cases & Historical data analysis & Real-time analytics \\
      \hline
      Processing Style & Execute once after full data load & Process on the fly \\
      \hline
    \end{tabular}
  \end{block}

  \begin{block}{Conclusion}
    Understanding architecture and processing paradigms is crucial for effective tool utilization, such as Apache Spark.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Introduction to Spark}
  \begin{itemize}
    \item Overview of Apache Spark architecture and components
    \item Advantages over traditional batch processing
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{What is Apache Spark?}
  \begin{block}{Definition}
    Apache Spark is an open-source, distributed computing system designed for fast and flexible data processing.
  \end{block}
  \begin{itemize}
    \item Handles large-scale data processing workloads efficiently.
    \item Leverages in-memory computing for faster execution times compared to traditional batch processing systems.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Key Components of Spark Architecture}
  \begin{itemize}
    \item \textbf{Driver Program}: Coordinates Spark execution via SparkContext.
    \item \textbf{Cluster Manager}: Manages resources (can be standalone, Mesos, or YARN).
    \item \textbf{Worker Nodes}: Execute tasks from the driver and run executors.
    \item \textbf{Executors}: Processes that run computations and store data.
    \item \textbf{Tasks}: Units of work assigned to executors, correlated to data partitions.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Advantages of Spark Over Traditional Batch Processing}
  \begin{enumerate}
    \item \textbf{Speed}:
      \begin{itemize}
        \item Processes data in-memory, reducing disk I/O.
      \end{itemize}
    \item \textbf{Ease of Use}:
      \begin{itemize}
        \item APIs available in Java, Scala, Python, R.
        \item High-level abstractions like DataFrames.
      \end{itemize}
    \item \textbf{Versatility}:
      \begin{itemize}
        \item Supports Batch, Streaming, Machine Learning, Graph Processing.
      \end{itemize}
    \item \textbf{Fault Tolerance}:
      \begin{itemize}
        \item Relies on Resilient Distributed Datasets (RDDs) for recovery of lost data.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example Concept: In-Memory vs. Disk Processing}
  \begin{lstlisting}[language=Python]
# Example of Spark code for a simple word count
from pyspark import SparkContext

sc = SparkContext("local", "Word Count Example")
text_file = sc.textFile("hdfs://path/to/textfile.txt")
word_counts = text_file.flatMap(lambda line: line.split(" ")) \
                        .map(lambda word: (word, 1)) \
                        .reduceByKey(lambda a, b: a + b)

word_counts.saveAsTextFile("hdfs://path/to/output_directory")
  \end{lstlisting}
  \begin{itemize}
    \item Demonstrates Spark's capabilities in handling data efficiently.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item Spark's architecture promotes speed, ease of use, and advanced capabilities.
    \item Understanding the architecture is essential for effective utilization.
    \item Flexibility in handling batch and real-time data processing.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Definition}
    \begin{block}{Definition}
        \begin{itemize}
            \item **DataFrames** in Apache Spark are a distributed collection of data organized into named columns.
            \item They can be viewed as a combination of:
            \begin{itemize}
                \item A table in a relational database
                \item An R DataFrame
                \item A Pandas DataFrame
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Structure}
    \begin{block}{Structure}
        \begin{itemize}
            \item **Schema**: Defines column names and data types, allowing optimization of query execution.
            \item **Rows and Columns**: Composed of rows (records) and columns (attributes) that can vary in data types.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example of a DataFrame Schema}
        \begin{tabular}{|l|c|l|}
            \hline
            Name    & Age & Occupation \\
            \hline
            Alice   & 30  & Engineer    \\
            Bob     & 35  & Designer    \\
            Charlie & 40  & Teacher     \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Relation to Traditional Data Formats}
    \begin{block}{Relation to Traditional Data Formats}
        \begin{itemize}
            \item **Structured Data**: Efficiently handles structured or semi-structured data, similar to SQL tables.
            \item **Unified Data Processing**: Can be created from various data sources:
            \begin{itemize}
                \item JSON files
                \item CSV files
                \item Hive tables
                \item Parquet files
            \end{itemize}
            \item Facilitates easy manipulation, analysis, and querying of large datasets.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Key Points}
    \begin{block}{Key Points}
        \begin{itemize}
            \item **Speed and Optimization**: Utilizes Sparkâ€™s Catalyst optimizer for better query execution performance.
            \item **Ease of Use**: Supports multiple programming languages (Python, Scala, Java, R).
            \item **Integration**: Works seamlessly with Spark SQL for executing SQL queries on DataFrames.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Code Snippet}
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("DataFramesExample").getOrCreate()

# Load data from a CSV file into a DataFrame
df = spark.read.csv("data/file.csv", header=True, inferSchema=True)

# Show the DataFrame
df.show()
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{DataFrames in Spark - Conclusion}
    \begin{block}{Conclusion}
        DataFrames serve as a powerful abstraction in Spark, simplifying the process of working with large datasets while bridging the gap between traditional data formats and the capabilities of distributed data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating DataFrames - Overview}
    \begin{block}{Understanding DataFrames}
        DataFrames are a key feature of Apache Spark that enable working with structured and semi-structured data in a distributed environment, providing a robust and scalable solution for big data processing.
    \end{block}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Higher-level abstraction than RDDs for structured data.
            \item Natively supports various data formats.
            \item Optimized operations using Spark's Catalyst optimizer.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating DataFrames - Methods}
    There are several ways to create DataFrames from structured data sources:
    
    \begin{enumerate}
        \item \textbf{From Existing RDDs}
        \begin{itemize}
            \item Use the \texttt{createDataFrame} method with a defined schema.
            \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("Create DataFrames").getOrCreate()
data = [("Alice", 1), ("Bob", 2)]
rdd = spark.sparkContext.parallelize(data)

schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Id", IntegerType(), True)
])

df = spark.createDataFrame(rdd, schema)
df.show()
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{From Structured Data Files}
        \begin{itemize}
            \item \textbf{From CSV:}
            \begin{lstlisting}[language=Python]
df_csv = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
df_csv.show()
            \end{lstlisting}
            \item \textbf{From JSON:}
            \begin{lstlisting}[language=Python]
df_json = spark.read.json("path/to/file.json")
df_json.show()
            \end{lstlisting}
            \item \textbf{From Parquet:}
            \begin{lstlisting}[language=Python]
df_parquet = spark.read.parquet("path/to/file.parquet")
df_parquet.show()
            \end{lstlisting}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Creating DataFrames - External Data Sources}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{From External Databases}
        \begin{itemize}
            \item Use JDBC to connect to databases like MySQL, PostgreSQL, etc.
            \begin{lstlisting}[language=Python]
df_db = spark.read.format("jdbc").options(
    url="jdbc:mysql://localhost:3306/db_name",
    driver="com.mysql.jdbc.Driver",
    dbtable="table_name",
    user="username",
    password="password").load()
df_db.show()
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Conclusion}
        \begin{itemize}
            \item Understanding how to create DataFrames from various data sources is fundamental for effective data processing in Spark.
            \item This knowledge prepares you for advanced operations covered in subsequent sections.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations and Actions - Overview}
  \begin{block}{Understanding the Concepts}
    In Apache Spark, data processing involves two main operations: \textbf{Transformations} and \textbf{Actions}. Understanding the difference between these is crucial for efficient data processing.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Transformations in Spark}
  \begin{block}{Definition}
    Transformations are operations that create a new dataset from the existing one. They are \textbf{lazy}, meaning they do not execute immediately, deferring computation until an action is called.
  \end{block}
  
  \begin{itemize}
    \item \textbf{Key Characteristics}:
    \begin{itemize}
      \item Lazy evaluation: Deferred computation until an action is executed.
      \item Return type: Produces a new dataset (RDD or DataFrame) without modifying the original.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Examples of Transformations}
  \begin{itemize}
    \item \textbf{map()}:
    \begin{lstlisting}[language=Python]
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
squared_rdd = rdd.map(lambda x: x ** 2)  # Returns [1, 4, 9, 16, 25]
    \end{lstlisting}
    
    \item \textbf{filter()}:
    \begin{lstlisting}[language=Python]
even_rdd = rdd.filter(lambda x: x % 2 == 0)  # Returns [2, 4]
    \end{lstlisting}
    
    \item \textbf{groupByKey()}:
    \begin{lstlisting}[language=Python]
paired_rdd = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3)])
grouped_rdd = paired_rdd.groupByKey().collect()  # Returns [('a', [1, 3]), ('b', [2])]
    \end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Actions in Spark}
  \begin{block}{Definition}
    Actions are operations that trigger the computation of the transformations and return a result to the driver or store it in storage.
  \end{block}
  
  \begin{itemize}
    \item \textbf{Key Characteristics}:
    \begin{itemize}
      \item Eager evaluation: Executes computations and returns a value or confirms completion.
      \item Return type: May return specific values or save data but does not create a new dataset for further transformations.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Examples of Actions}
  \begin{itemize}
    \item \textbf{collect()}:
    \begin{lstlisting}[language=Python]
results = squared_rdd.collect()  # Returns [1, 4, 9, 16, 25]
    \end{lstlisting}
    
    \item \textbf{count()}:
    \begin{lstlisting}[language=Python]
total_count = rdd.count()  # Returns 5
    \end{lstlisting}
    
    \item \textbf{saveAsTextFile()}:
    \begin{lstlisting}[language=Python]
squared_rdd.saveAsTextFile("output/squared_numbers")
    \end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item \textbf{Lazy vs. Eager}: Understanding these evaluations is critical for optimizing performance in Spark.
    \item \textbf{Intermediate vs. Final Results}: Transformations build a logical plan without output until triggered by actions.
    \item \textbf{Memory Efficiency}: Lazy evaluation allows Spark to optimize computations before execution.
  \end{itemize}
  By mastering these concepts, you can effectively manipulate large datasets in Spark.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Spark SQL}
    \begin{block}{Overview of Spark SQL}
        Spark SQL is a highly efficient component of Apache Spark that allows users to execute SQL queries on large datasets in a distributed environment. It seamlessly integrates with DataFrames and provides support for structured data processing.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Spark SQL}
    \begin{enumerate}
        \item \textbf{DataFrames}:
        \begin{itemize}
            \item A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database.
            \item It allows for optimized query execution and provides a rich API for data manipulation.
        \end{itemize}

        \item \textbf{SQL Support}:
        \begin{itemize}
            \item Spark SQL supports a subset of SQL, allowing complex queries and aggregations easily.
            \item You can execute SQL queries using the \texttt{spark.sql} API or the DataFrame API.
        \end{itemize}

        \item \textbf{Catalyst Optimizer}:
        \begin{itemize}
            \item Spark SQL uses a query optimizer known as Catalyst, which analyzes and optimizes query execution plans.
        \end{itemize}

        \item \textbf{Unified Data Processing}:
        \begin{itemize}
            \item You can run SQL queries alongside DataFrames and RDDs (Resilient Distributed Datasets).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Usage of Spark SQL}
    Consider you have a dataset of employee information stored in a DataFrame:

    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Spark SQL Example").getOrCreate()

# Sample DataFrame
data = [("Alice", 30), ("Bob", 25), ("Cathy", 27)]
columns = ["Name", "Age"]
df = spark.createDataFrame(data, columns)

# Register DataFrame as a SQL temporary view
df.createOrReplaceTempView("employees")
    \end{lstlisting}

    You can run SQL queries on this DataFrame:
    \begin{lstlisting}[language=SQL]
SELECT Name, Age
FROM employees
WHERE Age > 25
    \end{lstlisting}

    This query returns:
    \begin{tabular}{|c|c|}
        \hline
        Name  & Age \\
        \hline
        Alice & 30  \\
        Cathy & 27  \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
  \frametitle{SQL Queries in Spark - Overview}
  \begin{block}{Overview of Spark SQL}
    Spark SQL is a Spark module for structured data processing that enables the execution of SQL queries alongside DataFrame operations. This integration allows users to leverage familiar SQL syntax while utilizing the capabilities of Spark.
  \end{block}

  \begin{itemize}
    \item \textbf{DataFrames:} Immutable distributed collections of data organized into named columns.
    \item \textbf{Registering Temp Views:} Allows you to run SQL queries against a registered DataFrame.
    \item \textbf{Spark Session:} The entry point for programming with Spark SQL to create DataFrames and execute queries.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{SQL Queries in Spark - Writing SQL Queries}
  \begin{enumerate}
    \item \textbf{Creating a Spark Session:} 
      \begin{lstlisting}[language=Python]
      from pyspark.sql import SparkSession

      spark = SparkSession.builder \
          .appName("Spark SQL Example") \
          .getOrCreate()
      \end{lstlisting}

    \item \textbf{Loading Data:} 
      \begin{lstlisting}[language=Python]
      df = spark.read.csv("data.csv", header=True, inferSchema=True)
      \end{lstlisting}

    \item \textbf{Registering the DataFrame as a Temp View:} 
      \begin{lstlisting}[language=Python]
      df.createOrReplaceTempView("table_name")
      \end{lstlisting}

    \item \textbf{Executing SQL Queries:} 
      \begin{lstlisting}[language=Python]
      result = spark.sql("SELECT column1, column2 FROM table_name WHERE column3 > 100")
      result.show()
      \end{lstlisting}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{SQL Queries in Spark - Example and Conclusion}
  \begin{block}{Example SQL Query}
    \begin{lstlisting}[language=SQL]
    SELECT product, SUM(sales) AS total_sales 
    FROM sales_table 
    GROUP BY product 
    ORDER BY total_sales DESC
    \end{lstlisting}
    This query retrieves the total sales amount for each product, ordered from highest to lowest sales.
  \end{block}

  \begin{block}{Key Points}
    \begin{itemize}
      \item Familiar SQL syntax provides ease of use for those with SQL backgrounds.
      \item The combination of DataFrames and SQL enhances flexibility and performance.
      \item Utilizing SQL with Spark maximizes its distributed computation abilities for large datasets.
    \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
    Understanding SQL queries in Spark SQL enhances data processing capabilities, marrying Spark's scalability with the power of SQL for analytical tasks.
    Ensure dependencies are in place and remember to stop the Spark session:
    \begin{lstlisting}[language=Python]
    spark.stop()
    \end{lstlisting}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing Spark Applications - Introduction}
  % Introduction to Optimization in Spark
  Optimizing Spark applications is crucial for achieving high performance and efficiency during data processing tasks. Given Spark's inherent capabilities of distributed computing, the way we write and structure our applications can greatly impact performance at scale.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing Spark Applications - Key Best Practices}
  % Key best practices for optimizing Spark
  \begin{enumerate}
    \item \textbf{Data Serialization}
      \begin{itemize}
        \item Use efficient data formats like Parquet and ORC.
      \end{itemize}
    \item \textbf{Caching and Persistence}
      \begin{itemize}
        \item Cache frequently accessed DataFrames to avoid recomputation.
      \end{itemize}
    \item \textbf{Avoiding Shuffles}
      \begin{itemize}
        \item Minimize costly shuffles by using operations that preserve partitioning.
      \end{itemize}
    \item \textbf{Using Broadcast Variables}
      \begin{itemize}
        \item Leverage broadcast variables for large read-only data.
      \end{itemize}
    \item \textbf{Tuning Resource Allocation}
      \begin{itemize}
        \item Adjust executor cores and memory based on workload.
      \end{itemize}
    \item \textbf{Using the Catalyst Optimizer}
      \begin{itemize}
        \item Enable Catalyst optimizer through DataFrames and Spark SQL.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing Spark Applications - Monitoring and Troubleshooting}
  % Monitoring and Troubleshooting in Spark
  \begin{itemize}
    \item \textbf{Spark UI:} Utilize the Spark UI to visualize job execution plans and diagnose performance bottlenecks.
    \item \textbf{Logging:} Increase logging verbosity during development to analyze logs for issues.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing Spark Applications - Example Code Snippet}
  % Example of optimized Spark application code
  \begin{lstlisting}[language=python]
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Optimized Spark App") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Optimized DataFrame operations
df = spark.read.parquet("input.parquet")
df.cache()  # Cache the DataFrame
result = df.groupBy("column").agg({"value": "sum"})
result.write.parquet("output.parquet")
  \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Optimizing Spark Applications - Summary}
  % Summary of best practices for optimizing Spark applications
  By following these best practices, you can significantly enhance the performance and efficiency of your Spark applications. Using efficient data formats, avoiding shuffles, leveraging caching and broadcasting, and tuning resources are essential strategies for optimal performance.
\end{frame}

\begin{frame}
    \frametitle{Hands-on Lab: Using Spark}
    \begin{block}{Objective}
        In this hands-on lab, students will implement a data processing task using Apache Spark and DataFrames. They will gain practical experience in Spark's capabilities for handling large-scale data efficiently.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Concept Overview}
    \begin{itemize}
        \item \textbf{Apache Spark:} An open-source distributed computing system designed for fast processing of large datasets.
        \item \textbf{DataFrames:} A fundamental data structure in Spark, akin to a table in a relational database, allowing users to manipulate structured data easily.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Steps for the Lab}
    \begin{enumerate}
        \item \textbf{Set Up Spark Environment:}
        \begin{itemize}
            \item Ensure Spark is installed and configured or access a cloud-based Spark service (e.g., Databricks).
            \item Launch a new notebook or script for your Spark application.
        \end{itemize}

        \item \textbf{Load Data:}
        \begin{lstlisting}[language=Python]
        from pyspark.sql import SparkSession
        
        spark = SparkSession.builder.appName("DataProcessingLab").getOrCreate()
        df = spark.read.csv("users.csv", header=True, inferSchema=True)
        \end{lstlisting}

        \item \textbf{Exploratory Data Analysis (EDA):}
        \begin{lstlisting}[language=Python]
        df.printSchema()
        df.show(5)  # Display the first 5 rows
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Continued Steps for the Lab}
    \begin{enumerate}[resume]
        \item \textbf{Data Transformation:}
        \begin{lstlisting}[language=Python]
        adult_users = df.filter(df.age >= 18)
        \end{lstlisting}

        \item \textbf{Aggregation:}
        \begin{lstlisting}[language=Python]
        average_age_by_country = adult_users.groupBy("country").agg({'age': 'avg'})
        average_age_by_country.show()
        \end{lstlisting}

        \item \textbf{Write Output:}
        \begin{lstlisting}[language=Python]
        average_age_by_country.write.csv("average_age_by_country.csv", header=True)
        \end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Spark is built for speed and ease of use with APIs available in multiple languages (Python, Java, Scala).
        \item DataFrames offer a rich set of operations, including filtering, grouping, and aggregation, making data manipulation straightforward.
        \item Efficient data processing in Spark can significantly reduce computation time, especially with large datasets.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Tips for Success}
    \begin{itemize}
        \item Experiment with different operations: Try using functions like \texttt{join}, \texttt{drop}, or \texttt{withColumn} for deeper insights.
        \item Optimize your Spark application by considering resource usage (memory, executor counts) based on the dataset size.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Engaging in this lab equips students with hands-on experience in Spark, demonstrating its capabilities in processing and analyzing big data effectively. Be prepared to discuss your findings in the upcoming class session!
\end{frame}

\begin{frame}[fragile]
  \frametitle{Real-World Applications of Spark - Introduction}
  \begin{block}{Overview}
    Apache Spark is a powerful distributed computing system designed for fast data processing and analytics. 
    Its versatility makes it suitable for a broad range of applications across various industries. 
    In this slide, we will explore real-world applications and case studies showcasing how organizations are leveraging Spark for data processing.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Applications of Spark}
  \begin{enumerate}
    \item \textbf{E-Commerce and Retail}
      \begin{itemize}
        \item Recommendation engines for personalized suggestions (e.g., Amazon, Netflix).
        \item Collaborative filtering enhances user experience.
      \end{itemize}
    
    \item \textbf{Financial Services}
      \begin{itemize}
        \item Real-time fraud detection using transaction data analysis.
        \item Example: banks use MLlib for flagging fraudulent transactions.
      \end{itemize}
    
    \item \textbf{Telecommunications}
      \begin{itemize}
        \item Network optimization through call data analytics.
        \item Example: identifying poor service areas using Spark SQL.
      \end{itemize}
    
    \item \textbf{Healthcare}
      \begin{itemize}
        \item Patient data analysis for treatment effectiveness.
        \item Example: tracking outcomes and treatment correlations with machine learning.
      \end{itemize}
    
    \item \textbf{Social Media}
      \begin{itemize}
        \item Sentiment analysis on user-generated content (e.g., Twitter).
        \item Example: real-time sentiment scoring during marketing campaigns.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Case Study: Databricks and Netflix}
  \begin{block}{Overview}
    Databricks, which offers a cloud-based Spark platform, has helped organizations like Netflix to optimize their data analysis workflows.
    \begin{itemize}
      \item Efficient management of massive data volumes.
      \item Improved user experiences through optimized streaming and tailored content recommendations.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion: Why Choose Spark?}
  \begin{itemize}
    \item \textbf{Scalability}: Easily scales from a single server to thousands of machines.
    \item \textbf{Speed}: Processes data in-memory for rapid analysis.
    \item \textbf{Ease of Use}: Built-in libraries for SQL, machine learning, and graph processing simplify complex tasks.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Points to Emphasize}
  \begin{itemize}
    \item Spark is applied across various sectors, illustrating its versatility.
    \item Real-time processing capabilities empower organizations to make quicker decisions.
    \item The integration of Spark with machine learning libraries supports advanced analytics.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Code Snippet: Basic Spark DataFrame Operations}
  \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize a SparkSession
spark = SparkSession.builder.appName("ExampleApp").getOrCreate()

# Create DataFrame from a JSON file
df = spark.read.json("data.json")

# Show the DataFrame
df.show()

# Perform a simple transformation
filtered_df = df.filter(df['age'] > 30)

# Show the filtered DataFrame
filtered_df.show()
  \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Challenges in Data Processing}
    \begin{block}{Overview}
        Batch data processing involves handling large volumes of data collected over time to derive insights or perform analytics. However, challenges can inhibit efficiency and effectiveness. 
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Common Challenges in Batch Data Processing}
    \begin{enumerate}
        \item \textbf{Latency Issues}
            \begin{itemize}
                \item Traditional batch processing can have significant delays from data ingestion to result availability.
                \item \textbf{Spark Solution:} In-memory data processing drastically reduces processing time.
            \end{itemize}
        
        \item \textbf{Scalability}
            \begin{itemize}
                \item Growing data volume complicates system scalability.
                \item \textbf{Spark Solution:} Easy horizontal scaling by adding nodes to the Spark cluster.
            \end{itemize}
        
        \item \textbf{Data Consistency and Integrity}
            \begin{itemize}
                \item Ensuring data consistency during processing is problematic.
                \item \textbf{Spark Solution:} Built-in mechanisms like DataFrames and Datasets ensure type safety.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Continuing Challenges in Batch Data Processing}
    \begin{enumerate}[resume]
        \item \textbf{Complexity in Data Management}
            \begin{itemize}
                \item Managing diverse data sources complicates systems.
                \item \textbf{Spark Solution:} Unified framework supports various sources and formats.
            \end{itemize}
        
        \item \textbf{Resource Allocation and Management}
            \begin{itemize}
                \item Inefficient resource usage can waste computing power.
                \item \textbf{Spark Solution:} Dynamic resource allocation optimizes usage based on workload.
            \end{itemize}
        
        \item \textbf{Fault Tolerance}
            \begin{itemize}
                \item Tradition systems may lose intermediate results on failure.
                \item \textbf{Spark Solution:} Fault tolerance through data lineage and Resilient Distributed Datasets (RDDs).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item \textbf{In-Memory Processing:} Significantly reduces latency.
        \item \textbf{Scalable Architecture:} Easily handle growing datasets.
        \item \textbf{Robust Data Management:} Simplifies integration of various data sources.
        \item \textbf{Dynamic Resource Utilization:} Keeps resource allocation efficient.
        \item \textbf{Automatic Fault Recovery:} Ensures reliability during processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Code Snippet}
    \begin{lstlisting}[language=Python]
from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("Batch Data Processing with Spark") \
    .getOrCreate()

# Load data from a CSV file
data = spark.read.csv("data.csv", header=True, inferSchema=True)

# Perform a simple transformation
transformed_data = data.filter(data['value'] > 100)

# Show the results
transformed_data.show()
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Evaluation - Overview}
    This slide outlines the evaluation criteria for your exercises this week focused on data processing with Apache Spark, as well as the expectations for your upcoming capstone project. 
    \begin{itemize}
        \item Understanding these criteria is essential for successfully demonstrating your skills in data handling, transformation, and analysis using Spark.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Evaluation - Evaluation Criteria for Week's Exercises}
    \begin{enumerate}
        \item \textbf{Understanding of Concepts (20\%)}
            \begin{itemize}
                \item Ability to explain key Spark concepts such as RDDs, DataFrames, and transformations/actions.
                \item Example: Distinguish between \texttt{map()} (transformation) and \texttt{collect()} (action).
            \end{itemize}
        \item \textbf{Correctness of Implementation (40\%)} 
            \begin{itemize}
                \item Code must execute without errors and return expected results.
                \item Example: Ensure your code for data filtering identifies and outputs the correct records.
            \end{itemize}
        \item \textbf{Performance Considerations (20\%)} 
            \begin{itemize}
                \item Efficient use of Spark capabilities (e.g., avoiding unnecessary shuffles).
                \item Example: Use \texttt{.cache()} for DataFrames that are accessed repeatedly.
            \end{itemize}
        \item \textbf{Documentation and Code Quality (20\%)} 
            \begin{itemize}
                \item Code should include clear comments and follow best practices for readability.
                \item Example: Insert comments describing the function of each transformation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Assessment and Evaluation - Capstone Project Expectations}
    \begin{itemize}
        \item \textbf{Project Scope and Objectives:} Define the problem statement and objectives.
        \item \textbf{Data Processing Techniques:} 
            \begin{itemize}
                \item Data ingestion, cleaning, and preparation.
                \item Transformation operations and analysis through machine learning.
            \end{itemize}
        \item \textbf{Final Report and Presentation:} 
            \begin{itemize}
                \item Provide a comprehensive report on methodology and findings.
                \item Prepare a presentation highlighting key challenges and solutions.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Feedback and Q\&A}
  \begin{block}{Overview}
    In this session, we will open the floor for any questions or feedback regarding data processing with Apache Spark. 
    This is an essential opportunity for you to clarify concepts, discuss any challenges faced during exercises, 
    and gain deeper insights into the material covered this week.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts to Review}
  \begin{enumerate}
    \item \textbf{Introduction to Apache Spark}
      \begin{itemize}
        \item A powerful open-source cluster-computing framework designed for big data processing.
        \item Utilizes in-memory caching and optimized query execution for high performance.
      \end{itemize}
    \item \textbf{DataFrame API}
      \begin{itemize}
        \item A distributed collection of data organized into named columns.
        \item Allows for manipulation of data in a way similar to R or Pandas in Python.
      \end{itemize}
    \item \textbf{RDDs (Resilient Distributed Datasets)}
      \begin{itemize}
        \item Fundamental data structure of Spark representing an immutable distributed collection of objects.
        \item Supports fault tolerance and parallel processing.
      \end{itemize}
    \item \textbf{Transformations vs. Actions}
      \begin{itemize}
        \item \textbf{Transformations}: Create a new RDD from an existing one (e.g., \texttt{.map()}, \texttt{.filter()}).
        \item \textbf{Actions}: Return a value to the driver program or write data to external storage (e.g., \texttt{.collect()}, \texttt{.count()}).
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Encouraging Feedback}
  \begin{block}{Example Questions to Consider}
    \begin{itemize}
      \item \textbf{Conceptual Clarifications}
        \begin{itemize}
          \item What are the advantages of using DataFrames over RDDs?
          \item How does Spark handle data partitioning and shuffling?
        \end{itemize}
      \item \textbf{Practical Applications}
        \begin{itemize}
          \item Can you provide examples of use cases where Spark significantly improves data processing?
          \item How do you optimize Spark jobs for performance?
        \end{itemize}
    \end{itemize}
  \end{block}
  
  \begin{block}{Conclusion of Discussion}
    Your questions and feedback are vital for enhancing the learning experience. Remember, no query is too small; 
    addressing uncertainties helps everyone learn better. 
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Key Takeaways from Week 5}
  
  \begin{enumerate}
      \item \textbf{Introduction to Apache Spark}:
      \begin{itemize}
          \item Powerful open-source distributed computing system.
          \item In-memory data processing enhances performance significantly.
      \end{itemize}
      
      \item \textbf{Core Concepts}:
      \begin{itemize}
          \item \textbf{RDDs}: Fundamental data structure; immutable collections.
          \item \textbf{DataFrames}: Higher-level abstraction for structured data.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Transformations, Actions, and SQL}
  
  \begin{enumerate}
      \setcounter{enumi}{2}
      \item \textbf{Transformations and Actions}:
      \begin{itemize}
          \item \textbf{Transformations}: Lazy operations defining a new RDD (e.g., \texttt{map}, \texttt{filter}).
          \item \textbf{Actions}: Trigger computations, e.g., \texttt{collect}, \texttt{count}.
      \end{itemize}
      
      \item \textbf{Working with Spark SQL}:
      \begin{itemize}
          \item Query structured data using SQL syntax.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Next Steps - Next Topics}
  
  \textbf{Next Steps in the Course:}
  
  \begin{enumerate}
      \item \textbf{Advanced Data Processing Techniques} - Window functions and aggregation.
      \item \textbf{Graph Processing with Spark} - Introduction to Spark GraphX.
      \item \textbf{Real-World Use Cases of Spark} - Applications in machine learning and analytics.
      \item \textbf{Hands-On Experience} - Process a real dataset using Spark.
  \end{enumerate}
  
  \textbf{Engagement Encouragement:}
  
  \begin{itemize}
      \item Revisit examples and practice coding snippets.
      \item Prepare any questions for the next meeting.
  \end{itemize}
\end{frame}


\end{document}