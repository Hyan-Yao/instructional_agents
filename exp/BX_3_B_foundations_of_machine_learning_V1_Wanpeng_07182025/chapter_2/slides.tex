\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Supervised Learning]{Chapter 2: Supervised Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning is a fundamental paradigm in machine learning where models are trained using labeled data. It involves teaching algorithms to make predictions or decisions based on input-output pairs with known outcomes. This method is widely used across various applications due to its ability to utilize structured data for predictive analysis.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of Supervised Learning}
    \begin{enumerate}
        \item \textbf{Labeled Data:} Each training example is accompanied by an output label. For example, email classification labels emails as "spam" or "not spam."
        
        \item \textbf{Learning Process:} The algorithm finds patterns correlating inputs with outputs during training, allowing it to predict outcomes for new data.
        
        \item \textbf{Types of Supervised Learning:}
        \begin{itemize}
            \item \textbf{Classification:} Categorizes data into distinct classes (e.g., species classification).
            \item \textbf{Regression:} Predicts continuous values (e.g., housing prices based on characteristics).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance and Applications of Supervised Learning}
    \begin{block}{Significance in Machine Learning}
        \begin{itemize}
            \item \textbf{Decision Making:} Supports critical processes in healthcare, finance, and marketing.
            \item \textbf{Performance Evaluation:} Validated using metrics like accuracy and F1-score for effectiveness assessment.
        \end{itemize}
    \end{block}

    \begin{block}{Example Application: Loan Approval Prediction}
        In a loan approval scenario:
        \begin{itemize}
            \item \textbf{Features:} Input variables (e.g., credit score, income).
            \item \textbf{Label:} Output variable (approved or declined).
        \end{itemize}
        Using supervised learning, a bank can predict loan approval status based on historical data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Next Steps}
    Supervised learning is a powerful approach in machine learning that utilizes labeled data for informed predictions across various domains.

    \vspace{1em}
    \textbf{Next Content Preview:} 
    In the next slide, we will define supervised learning in detail and explore its key characteristics, including the importance of labeled data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Definition of Supervised Learning}
    \begin{block}{Definition}
        Supervised Learning is a type of machine learning where an algorithm is trained on a labeled dataset. Each training example is accompanied by a corresponding output label, representing the ground truth. The goal is to learn a mapping from input features to corresponding outputs, enabling accurate predictions on unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Characteristics of Supervised Learning}
    \begin{itemize}
        \item \textbf{Labeled Data:}
        \begin{itemize}
            \item Datasets consist of input-output pairs.
            \item Example: In animal images, each image may be labeled as "cat," "dog," or "bird."
        \end{itemize}
        
        \item \textbf{Training Phase:}
        \begin{itemize}
            \item The algorithm learns by adjusting parameters to minimize prediction errors.
            \item Utilizes optimization techniques such as gradient descent.
        \end{itemize}

        \item \textbf{Testing Phase:}
        \begin{itemize}
            \item The algorithm is evaluated on unseen data (test set).
            \item Performance metrics include accuracy, precision, recall, or F1 score.
        \end{itemize}

        \item \textbf{Types of Problems:}
        \begin{itemize}
            \item Classification: Predicting discrete labels (e.g., spam detection).
            \item Regression: Predicting continuous values (e.g., house price forecasting).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples and Illustration}
    \begin{block}{Examples}
        \begin{enumerate}
            \item \textbf{Email Spam Detection (Classification)}
            \begin{itemize}
                \item \textbf{Input:} Features from an email (e.g., word frequency, length).
                \item \textbf{Output:} Spam or not spam label.
            \end{itemize}

            \item \textbf{House Price Prediction (Regression)}
            \begin{itemize}
                \item \textbf{Input:} Features such as size, location, number of bedrooms.
                \item \textbf{Output:} Predicted house price.
            \end{itemize}
        \end{enumerate}
    \end{block}

    \begin{block}{Illustration}
        Consider the function in supervised learning:
        \begin{equation}
            \text{Prediction} = f(\text{Inputs})
        \end{equation}
        Where:
        \begin{itemize}
            \item Inputs are features (e.g., height, weight).
            \item \( f \) is the model learned from labeled data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning Algorithms - Overview}
    Supervised learning algorithms learn patterns from labeled data, where each input is paired with an output. 
    We will discuss four common supervised learning algorithms: 
    \begin{itemize}
        \item \textbf{Linear Regression}
        \item \textbf{Logistic Regression}
        \item \textbf{Decision Trees}
        \item \textbf{Support Vector Machines (SVM)}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning Algorithms - Linear Regression}
    \textbf{1. Linear Regression}
    \begin{itemize}
        \item \textbf{Concept}: Predicts a continuous target variable using a linear equation.
        \item \textbf{Equation}:
        \begin{equation}
            Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon
        \end{equation}
        where \(Y\) is the predicted value, \(X_i\) are the features, \(\beta_i\) are the coefficients, and \(\epsilon\) is the error term.
        \item \textbf{Example}: Predicting house prices based on features like size, number of bedrooms, and location.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Supervised Learning Algorithms - Further Concepts}
    \textbf{2. Logistic Regression}
    \begin{itemize}
        \item \textbf{Concept}: Used for binary classification, predicting a probability between 0 and 1.
        \item \textbf{Sigmoid Function}:
        \begin{equation}
            P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \ldots + \beta_n X_n)}}
        \end{equation}
        \item \textbf{Example}: Classifying emails as spam or not spam based on a probability threshold.
    \end{itemize}
    
    \bigskip
    
    \textbf{3. Decision Trees}
    \begin{itemize}
        \item \textbf{Concept}: A tree-like model that splits data into branches for decision-making.
        \item \textbf{Example}: Predicting customer purchasing behavior based on age and income.
    \end{itemize}
    
    \bigskip
    
    \textbf{4. Support Vector Machines (SVM)}
    \begin{itemize}
        \item \textbf{Concept}: Classifies by finding the optimal hyperplane between classes.
        \item \textbf{Hyperplane Equation}:
        \begin{equation}
            w^T x + b = 0
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Supervised Learning}
    \begin{itemize}
        \item Supervised learning involves training a model on a labeled dataset.
        \item Input-output relationship is known.
        \item This section provides a step-by-step guide using Python and Scikit-learn.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Libraries and Data Loading}
    \textbf{Step 1: Import Necessary Libraries}
    \begin{lstlisting}[language=Python]
import pandas as pd          # For data handling
import numpy as np           # For numerical operations
from sklearn.model_selection import train_test_split  # For dataset splitting
from sklearn.linear_model import LinearRegression     # Example: Linear Regression algorithm
from sklearn.metrics import mean_squared_error        # For model evaluation
    \end{lstlisting}

    \textbf{Step 2: Load the Dataset}
    \begin{lstlisting}[language=Python]
data = pd.read_csv('data.csv')  # Replace 'data.csv' with your dataset filename
print(data.head())               # Displays the first few rows of the dataset
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Data Preprocessing and Model Training}
    \textbf{Step 3: Preprocess the Data}
    \begin{lstlisting}[language=Python]
# Handle missing values
data.fillna(data.mean(), inplace=True)

# Example: Convert categorical variable to dummy variables
data = pd.get_dummies(data, drop_first=True)
    \end{lstlisting}

    \textbf{Step 4: Split the Dataset}
    \begin{lstlisting}[language=Python]
X = data.drop('target', axis=1)  # Replace 'target' with your label column
y = data['target']  # Labels

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    \end{lstlisting}

    \textbf{Step 5: Train the Model}
    \begin{lstlisting}[language=Python]
model = LinearRegression()
model.fit(X_train, y_train)  # Training the model
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Predictions and Evaluation}
    \textbf{Step 6: Make Predictions}
    \begin{lstlisting}[language=Python]
y_pred = model.predict(X_test)
    \end{lstlisting}

    \textbf{Step 7: Evaluate the Model}
    \begin{lstlisting}[language=Python]
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
    \end{lstlisting}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Libraries like Scikit-learn simplify implementation.
            \item Data preprocessing influences model performance.
            \item Always evaluate your model.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Implementing supervised learning algorithms involves several critical steps:
    \begin{itemize}
        \item Import libraries
        \item Load and preprocess data
        \item Split the dataset
        \item Train the model
        \item Make predictions
        \item Evaluate performance
    \end{itemize}
    This structured approach enables systematic development and troubleshooting of machine learning models using Python.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Training - Overview}
    \begin{block}{Understanding Model Training}
    Model training is a crucial step in supervised learning where algorithms learn from data to make predictions.
    This involves three essential datasets:
    \end{block}

    \begin{enumerate}
        \item **Training Data**: The foundation of model learning.
        \item **Validation Data**: Helps fine-tune the model's hyperparameters.
        \item **Testing Data**: Evaluates the final model's performance.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Training - Data Description}
    
    \begin{itemize}
        \item \textbf{Training Data}:
        \begin{itemize}
            \item Used to train the model.
            \item \textit{Example:} Features include square footage, number of bedrooms, and sale prices.
        \end{itemize}

        \item \textbf{Validation Data}:
        \begin{itemize}
            \item Assesses performance and fine-tunes hyperparameters.
            \item \textit{Example:} Houses with similar features not included in training to validate price predictions.
        \end{itemize}
        
        \item \textbf{Testing Data}:
        \begin{itemize}
            \item Used for an unbiased assessment of the model.
            \item \textit{Example:} Houses not seen during training or validation to evaluate real-world effectiveness.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Training - Key Points}

    \begin{itemize}
        \item **Importance of Data Splitting**: Ensures effective model training and generalization.
        
        \item **Avoiding Overfitting**: Validation data helps to monitor performance changes, preventing overfitting.
        
        \item **Real-World Significance**: The distinction among datasets emphasizes the necessity to evaluate models beyond training metrics.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Training - Formula and Code Snippet}
    
    \begin{block}{Accuracy Formula}
    To visualize the concept of model performance, use the formula:
    \begin{equation}
    \text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
    \end{equation}
    \end{block}
    
    \begin{block}{Code Snippet}
    Hereâ€™s an example of how to split your dataset in Python using \texttt{scikit-learn}:
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

# Example data
X = [...]  # Features
y = [...]  # Target variable

# First, split into training and remaining data (validation + test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# Then, split the remaining data into validation and testing
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation Metrics - Introduction}
    \begin{block}{Introduction}
        In supervised learning, assessing the effectiveness of a model is crucial to ensure that it performs well on unseen data. Various metrics allow us to quantify the performance of our models. The following are some of the most commonly used metrics:
    \end{block}
    \begin{itemize}
        \item Accuracy
        \item Precision
        \item Recall
        \item F1-score
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation Metrics - Definitions}
    \begin{block}{1. Accuracy}
        \textbf{Definition}: Accuracy measures the proportion of correct predictions made by the model out of all predictions.
        
        \textbf{Formula}:
        \begin{equation}
            \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \end{equation}
        
        Where:
        \begin{itemize}
            \item TP = True Positives
            \item TN = True Negatives
            \item FP = False Positives
            \item FN = False Negatives
        \end{itemize}
        
        \textbf{Example}: 
        A model makes 100 predictions, 70 correct (TP + TN), and 30 incorrect (FP + FN).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Performance Evaluation Metrics - Metrics Continued}
    \begin{block}{2. Precision}
        \textbf{Definition}: Precision, also known as Positive Predictive Value, measures how many of the items labeled as positive are truly positive.
        
        \textbf{Formula}:
        \begin{equation}
            \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
        \end{equation}

        \textbf{Example}: If the model identified 50 positives where 30 were true positives (TP = 30, FP = 20), then:
        \begin{equation}
            \text{Precision} = \frac{30}{30 + 20} = 0.6 \text{ or } 60\%
        \end{equation}
    \end{block}
    
    \begin{block}{3. Recall}
        \textbf{Definition}: Recall, also known as Sensitivity, measures how many actual positive cases were captured by the model.
        
        \textbf{Formula}:
        \begin{equation}
            \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
        \end{equation}

        \textbf{Example}: If there are 50 actual positive instances and the model correctly identifies 30:
        \begin{equation}
            \text{Recall} = \frac{30}{30 + 20} = 0.6 \text{ or } 60\%
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting - Introduction}
    \begin{block}{Understanding Overfitting and Underfitting}
        Supervised learning models face two significant challenges: 
        \textbf{overfitting} and \textbf{underfitting}. 
        Each adversely impacts model performance and generalization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting - Definition and Causes}
    \begin{block}{What is Overfitting?}
        Overfitting occurs when a model learns the training data too well, including its noise and outliers. 
        The model performs exceptionally on the training set but poorly on unseen data.
    \end{block}

    \begin{itemize}
        \item \textbf{Causes:}
        \begin{itemize}
            \item Complex Models: High-capacity models fit complex patterns, capturing noise.
            \item Insufficient Data: A small dataset leads to grasping specific patterns that do not generalize.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting - Example and Metrics}
    \begin{block}{Example of Overfitting}
        Imagine trying to apply a quadratic equation to predict a game outcome with a linear relationship. 
        The model fits perfectly to a few data points but fails on new data.
    \end{block}

    \begin{block}{Key Metrics}
        High training accuracy vs. low validation/test accuracy indicates overfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Underfitting - Definition and Causes}
    \begin{block}{What is Underfitting?}
        Underfitting occurs when a model is too simplistic to capture underlying patterns in data, resulting in poor performance on both training and unseen data.
    \end{block}

    \begin{itemize}
        \item \textbf{Causes:}
        \begin{itemize}
            \item Too Simple Models: Linear models for nonlinear functions lead to underfitting.
            \item Insufficient Training: Not allowing enough iterations or stopping early causes inadequate learning.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Underfitting - Example and Metrics}
    \begin{block}{Example of Underfitting}
        Using a straight line (linear regression) to model a dataset with a clear quadratic relationship leads to underfitting, as the model cannot capture the curve.
    \end{block}

    \begin{block}{Key Metrics}
        Low accuracy on both training and validation/test sets suggests underfitting.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting and Underfitting}
    \begin{enumerate}
        \item \textbf{Regularization:}
        \begin{itemize}
            \item L1 (Lasso) and L2 (Ridge) regularization constrain model flexibility.
            \item Formula for L2 Regularization:
            \[
            J(\theta) = \text{Loss} + \lambda \sum_{i=1}^n \theta_i^2
            \]
        \end{itemize}
        
        \item \textbf{Cross-Validation:}
        \begin{itemize}
            \item Use k-fold cross-validation to assess model performance on different data subsets.
        \end{itemize}

        \item \textbf{Model Complexity:}
        \begin{itemize}
            \item Adjust model complexity based on training data size.
        \end{itemize}
        
        \item \textbf{Early Stopping:}
        \begin{itemize}
            \item Monitor performance on validation set and stop training when performance degrades.
        \end{itemize}
        
        \item \textbf{More Data:}
        \begin{itemize}
            \item Acquiring more training data helps better generalization.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item \textbf{Overfitting} results in too complex models; \textbf{underfitting} leads to too simple models.
        \item Regularization, cross-validation, and proper model selection are effective strategies.
        \item Striking a balance between complexity and simplicity is key to robust model building in supervised learning.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Introduction}
    \begin{block}{Introduction to Hyperparameters}
        In machine learning, hyperparameters are parameters whose values are set before the learning process begins. 
        They govern the training process and influence model performance. Unlike model parameters learned from the data, 
        hyperparameters must be configured manually.
    \end{block}
    
    \begin{block}{Importance of Hyperparameter Tuning}
        Tuning hyperparameters is critical for improving model performance. Proper tuning can lead to:
        \begin{itemize}
            \item Increased accuracy
            \item Reduction of overfitting and underfitting
            \item Better generalization on unseen data
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Common Hyperparameters}
    Depending on the algorithm, some common hyperparameters include:
    \begin{itemize}
        \item \textbf{Learning Rate:} Controls the step size during optimization.
        \item \textbf{Number of Trees (in Random Forests):} Determines the number of trees to build.
        \item \textbf{Max Depth (in Decision Trees):} Defines the maximum depth of the tree.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Methods}
    \begin{block}{1. Grid Search}
        Grid search is an exhaustive search method that explores all possible combinations of hyperparameters specified in a grid.
        \begin{enumerate}
            \item Define a set of hyperparameters and their corresponding ranges.
            \item Evaluate the model performance for each combination using a scoring metric.
            \item Select the combination that yields the best performance.
        \end{enumerate}
        \begin{example}
            Example for SVM:
            \begin{itemize}
                \item C values: [0.1, 1, 10]
                \item Kernel types: ['linear', 'poly', 'rbf']
            \end{itemize}
        \end{example}
    \end{block}

    \begin{block}{2. Random Search}
        Random search selects random combinations of hyperparameters from defined ranges, often needing fewer iterations. 
        \begin{enumerate}
            \item Define the hyperparameters and their ranges.
            \item Randomly sample combinations and evaluate model performance.
            \item Track the best-performing model based on the scoring metric.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Comparisons}
    \begin{block}{Key Comparisons}
        \begin{tabular}{|l|l|l|l|}
            \hline
            Method & Coverage & Speed & Complexity \\
            \hline
            Grid Search & Exhaustive & Slower & Complex with many parameters \\
            Random Search & Randomized & Faster & Easier; good for large parameter spaces \\
            \hline
        \end{tabular}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Conclusion}
    Hyperparameter tuning is essential for optimizing the performance of machine learning models. 
    Both grid search and random search have distinct advantages: 
    \begin{itemize}
        \item Grid search is systematic.
        \item Random search is often more efficient in large parameter spaces.
    \end{itemize}
    Understanding these methods allows practitioners to better refine their models for improved accuracy and generalization.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hyperparameter Tuning - Practical Implementation}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC
from scipy.stats import uniform

# Define the model
model = SVC()

# Define parameters for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'poly', 'rbf']
}

# Grid Search
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Random Search
param_dist = {
    'C': uniform(loc=0, scale=10),
    'kernel': ['linear', 'poly', 'rbf']
}
random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5)
random_search.fit(X_train, y_train)

print("Best Parameters from Grid Search:", grid_search.best_params_)
print("Best Parameters from Random Search:", random_search.best_params_)
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Cross-Validation Techniques}
    \begin{block}{What is Cross-Validation?}
        Cross-validation is a statistical method used to estimate the skill of machine learning models. It assesses how well the model will generalize to an independent dataset by partitioning the dataset in various ways.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Why Use Cross-Validation?}
    \begin{itemize}
        \item \textbf{Model Reliability}: Reduces overfitting and ensures that the model performs well on unseen data.
        \item \textbf{Data Utilization}: Makes efficient use of limited datasets by training on different parts and validating on others.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Cross-Validation Techniques}
    \begin{enumerate}
        \item \textbf{K-Fold Cross-Validation}
        \begin{itemize}
            \item Divide the dataset into \(k\) equally-sized folds.
            \item For each fold:
            \begin{itemize}
                \item Train the model on \(k-1\) folds.
                \item Validate on the remaining fold.
            \end{itemize}
            \item Calculate the average of performance metrics.
            \item \textit{Example:} For 100 samples and \(k=5\), each fold has 20 samples.
        \end{itemize}
        
        \item \textbf{Stratified K-Fold Cross-Validation}
        \begin{itemize}
            \item Similar to k-fold but ensures each fold maintains the same class label proportions.
            \item \textit{Example:} For a dataset with 70\% Class A and 30\% Class B, each fold will maintain this ratio.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Selection of K}: Impacts bias-variance trade-off. A small \(k\) increases variance; a large \(k\) raises computational cost.
        \item \textbf{Implementation}: Libraries like Scikit-Learn provide easy implementations for cross-validation.
        \item \textbf{Performance Metrics}: Use consistency measures like accuracy, precision, recall, and F1 score across folds.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: K-Fold and Stratified K-Fold}
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import accuracy_score

# Example with K-Fold
kf = KFold(n_splits=5)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print(f"Accuracy: {accuracy_score(y_test, predictions)}")

# Example with Stratified K-Fold
skf = StratifiedKFold(n_splits=5)
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print(f"Accuracy: {accuracy_score(y_test, predictions)}")
    \end{lstlisting}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Cross-validation techniques, particularly k-fold and stratified k-fold, are essential for validating machine learning models. They provide insights into performance and help build reliable classifiers that generalize well to new data.
\end{frame}

\begin{frame}
    \frametitle{Use Cases in Supervised Learning}
    \begin{block}{Overview of Supervised Learning}
        Supervised learning is a type of machine learning where algorithms are trained on labeled data. The model learns the relationship between inputs (features) and outputs (labels) to make predictions on unseen data.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Use Cases Across Industries}
    \begin{enumerate}
        \item \textbf{Finance}
            \begin{itemize}
                \item \textbf{Credit Scoring:} Assesses lending risk by predicting default likelihood.
                \item \textbf{Fraud Detection:} Identifies fraudulent transactions by analyzing patterns.
            \end{itemize}
        \item \textbf{Healthcare}
            \begin{itemize}
                \item \textbf{Disease Diagnosis:} Predicts diseases based on patient data.
                \item \textbf{Patient Outcome Prediction:} Estimates likelihood of hospital readmission.
            \end{itemize}
        \item \textbf{Marketing}
            \begin{itemize}
                \item \textbf{Customer Segmentation:} Categorizes customers for targeted strategies.
                \item \textbf{Churn Prediction:} Analyses data to anticipate and prevent customer churn.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item High-quality labeled data is crucial for successful supervised learning.
        \item Various algorithms (e.g., linear regression, decision trees, SVMs) apply to different problems.
        \item The ability to generalize from training data to new, unseen data is essential.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrative Example of Supervised Learning Workflow}
    \begin{center}
        \begin{verbatim}
              Training Data
                  (Inputs)
                   / \
                  /   \
                 /     \
         Model Training  --->  Model Prediction ---> New Data
                 \        |
                  \       v
                   ----->  Output (Predictions)
        \end{verbatim}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Supervised learning is pivotal in many industries, aiding in decision-making, productivity enhancement, and valuable insights through predictive analytics. Understanding these use cases equips practitioners to implement effective solutions tailored to their needs.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Supervised Learning}
    \begin{block}{Overview}
        Supervised learning involves training models on labeled data to make predictions or decisions. However, ethical implications, particularly bias in training data and the consequences of automated decision-making, must be carefully considered.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Issues}
    \begin{enumerate}
        \item \textbf{Bias in Training Data}
        \begin{itemize}
            \item Definition: Bias occurs when the training data does not accurately represent the population, leading to unfair or discriminatory outcomes.
            \item Example: Facial recognition systems trained predominantly on lighter-skinned individuals may misidentify or exclude individuals with darker skin tones.
            \item Illustration: Hiring algorithms trained on biased historical data may unjustly prioritize certain demographic candidates, reinforcing inequalities.
        \end{itemize}
        
        \item \textbf{Implications of Decision-Making}
        \begin{itemize}
            \item Automated decisions in critical domains like healthcare and criminal justice can perpetuate disparities due to biased training data.
            \item Case Study: Predictive policing algorithms can disproportionately target minority communities based on biased historical crime data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Additional Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Transparency and Accountability}
        \begin{itemize}
            \item Lack of transparency in black-box models makes understanding decision-making difficult.
            \item \textbf{Call to Action:} Developers should provide clear documentation and methodologies behind model training and decisions.
        \end{itemize}

        \item \textbf{Examples of Bias in Action}
        \begin{itemize}
            \item Healthcare: AI systems trained on underrepresented elderly data may fail to diagnose conditions accurately in older patients.
            \item Finance: Loan approval systems may inadvertently discriminate against certain demographic groups due to historical biases.
        \end{itemize}

        \item \textbf{Conclusion}
        \begin{itemize}
            \item Addressing bias and ethical concerns is essential to ensure fairness and equity in AI.
            \item Encouraging stakeholder involvement, including affected communities, aligns AI development with ethical standards and societal values.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Summary of Key Takeaways}
    
    \begin{itemize}
        \item \textbf{Definition and Purpose:} 
        Supervised Learning involves training a model on labeled data to learn a mapping from inputs to outputs.
        
        \item \textbf{Importance in Machine Learning:}
        Essential for prediction tasks using historical data.
        \begin{itemize}
            \item Classification - e.g., spam detection.
            \item Regression - e.g., house price forecasting.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Practical Examples}

    \begin{itemize}
        \item \textbf{Example of Classification:} 
        Training a model on labeled images of cats and dogs, enabling it to classify new images.
        
        \item \textbf{Example of Regression:} 
        Using past housing data to predict new house prices based on features like size and location.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Points and Ethical Considerations}

    \begin{itemize}
        \item \textbf{Training and Testing:}
        Split data into training and test sets for evaluation.
        
        \item \textbf{Common Algorithms:}
        \begin{itemize}
            \item Linear Regression for regression tasks.
            \item Logistic Regression, Decision Trees, SVM for classification tasks.
        \end{itemize}
        
        \item \textbf{Evaluation Metrics:}
        Important metrics include accuracy, precision, recall, F1-score, and mean squared error (MSE).
        
        \item \textbf{Ethical Considerations:}
        \begin{itemize}
            \item Bias in Training Data can perpetuate unfair outcomes.
            \item Accountability in Decision-Making is vital.
        \end{itemize}
        
        \item \textbf{Conclusion:}
        Supervised Learning is foundational in machine learning applications, blending theory with responsible practice.
    \end{itemize}
\end{frame}


\end{document}