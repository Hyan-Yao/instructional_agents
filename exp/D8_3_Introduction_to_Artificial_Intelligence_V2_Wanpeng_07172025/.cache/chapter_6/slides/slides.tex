\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Machine Learning - Overview}
    \begin{itemize}
        \item Machine Learning (ML) is a subset of Artificial Intelligence (AI).
        \item It enables systems to learn from data, improve over time, and make predictions.
        \item Significance in AI:
        \begin{itemize}
            \item Autonomy in AI systems.
            \item Data-driven decision-making.
            \item Insights extraction from large datasets.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Machine Learning - Types and Applications}
    \begin{enumerate}
        \item Types of Machine Learning:
        \begin{itemize}
            \item Supervised Learning: Trained on labeled data (e.g., spam detection).
            \item Unsupervised Learning: Analyzes unlabelled data to find patterns (e.g., customer segmentation).
            \item Reinforcement Learning: Learns via feedback in an adaptive environment (e.g., AlphaGo).
        \end{itemize}
        \item Key Applications:
        \begin{itemize}
            \item Healthcare: Patient outcome prediction.
            \item Finance: Fraud detection.
            \item Marketing: Personalization algorithms.
            \item Autonomous Vehicles: Environmental awareness through sensor data.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Machine Learning - Example and Conclusion}
    \begin{block}{Example: Image Recognition}
        \begin{itemize}
            \item \textbf{Task}: Identify whether an image contains a cat or not.
            \item \textbf{Method}: Train on a labeled dataset of "cat" and "not cat" images.
            \item \textbf{Outcome}: Model can classify new images accurately.
        \end{itemize}
    \end{block}
    \begin{block}{Conclusion}
        Machine Learning revolutionizes various fields by processing vast quantities of data, enhancing productivity and innovation. Its ongoing evolution will lead to broader applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 1}
    \begin{block}{Definition of Machine Learning}
        Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on the development of algorithms that enable computers to learn from and make predictions or decisions based on data. Unlike traditional programming, where explicit instructions are given, machine learning algorithms improve their performance as they are exposed to more data over time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 2}
    \begin{block}{Role in Artificial Intelligence}
        Machine Learning plays a crucial role in AI by providing methods that enable systems to enhance their capabilities without human intervention. Below are key components of this relationship:
    \end{block}
    \begin{itemize}
        \item \textbf{Artificial Intelligence (AI):} A broad field that aims to create machines capable of simulating human intelligence.
        \item \textbf{Machine Learning (ML):} A subset of AI that emphasizes the creation of models that learn from data.      
        \item \textbf{Deep Learning:} A refinement of ML inspired by the structure and function of the human brain, utilizing neural networks.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Part 3}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Learning from Data:} ML models identify trends and patterns in large datasets.
            \item \textbf{Self-Improvement:} ML algorithms adapt and improve their accuracy over time.
            \item \textbf{Applications:} 
                \begin{itemize}
                    \item Recommendation Systems (e.g., Netflix, Amazon)
                    \item Image Recognition (e.g., facial recognition)
                    \item Natural Language Processing (NLP) (e.g., virtual assistants)
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Example}
    Consider a simple example of a spam email filter. A machine learning model is trained on thousands of emails labeled as either "spam" or "not spam." As the model processes more emails, it learns to identify specific words, phrases, or patterns that are indicative of spam, thus enhancing its accuracy over time.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Diagram}
    \centering
    \textbf{Relationship of AI, ML and Deep Learning}
    \begin{center}
        \texttt{
            \begin{tabular}{c}
                AI \\
                | \\
                +-------+-------+ \\
                |               | \\
                ML             Other AI Methods \\
                |               | \\
                +-------+-------+ \\
                |               | \\
                Supervised    Unsupervised \\
                Learning       Learning
            \end{tabular}
        }
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Machine Learning? - Summary}
    Machine Learning is an essential component of AI, enabling systems to learn from data, improve autonomously, and solve complex problems. By leveraging ML, we create intelligent systems that enrich our daily lives and enhance numerous industries.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Machine Learning}
    \begin{block}{Introduction to Machine Learning}
        Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables systems to learn from data, improve their performance, and make decisions autonomously.
        The core idea is to provide algorithms with data, allowing them to identify patterns and make predictions without explicit programming for every task.
        In this slide, we will explore the two main categories of machine learning: \textbf{Supervised Learning} and \textbf{Unsupervised Learning}.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Machine Learning - Supervised Learning}
    \begin{itemize}
        \item \textbf{Definition}: Involves training a model on a labeled dataset, pairing input data with the correct output.
        \item \textbf{Example}: Email spam detection where emails are labeled as "spam" or "not spam."
    \end{itemize}

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Requires labeled data for training
            \item Used for classification and regression tasks
            \item Common algorithms include:
            \begin{itemize}
                \item Linear Regression
                \item Decision Trees
                \item Support Vector Machines (SVM)
                \item Neural Networks
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Machine Learning - Unsupervised Learning}
    \begin{itemize}
        \item \textbf{Definition}: Trains the model on data without any labels, focusing on discovering hidden patterns or groupings within the data.
        \item \textbf{Example}: Customer segmentation based on purchasing behavior to tailor marketing strategies.
    \end{itemize}

    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item Does not require labeled data
            \item Primarily used for clustering and association tasks
            \item Common algorithms include:
            \begin{itemize}
                \item K-Means Clustering
                \item Hierarchical Clustering
                \item Principal Component Analysis (PCA)
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item Supervised learning relies on well-defined input-output pairs, whereas unsupervised learning uncovers patterns without explicit guidance.
        \item The choice between the two depends on the problem's nature and the availability of labeled data.
        \item Both methods are fundamental to applications such as image recognition and market analysis.
    \end{itemize}

    \begin{block}{Conclusion}
        Understanding the differences between supervised and unsupervised learning is crucial for selecting the appropriate approach for machine learning tasks. 
        We will further explore supervised learning in the next slide.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning - Definition}
    % Define supervised learning and its components.
    \begin{block}{Definition}
        Supervised learning is a type of machine learning where an algorithm is trained on a labeled dataset. The model learns to map inputs (features) to known outputs (labels) through examples, effectively learning by being given the right answers during training.
    \end{block}
    
    \begin{block}{Key Components}
        \begin{itemize}
            \item \textbf{Labeled Data}: Data containing input-output pairs (e.g., images of cats labeled as 'cat').
            \item \textbf{Model}: An algorithm that makes predictions or classifications based on input data.
            \item \textbf{Training}: The process of adjusting model parameters to minimize prediction errors.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning - Algorithms}
    % List common algorithms used in supervised learning.
    \begin{block}{Common Algorithms}
        Some prevalent algorithms in supervised learning include:
        \begin{itemize}
            \item \textbf{Linear Regression}: Predicting continuous values (e.g., house prices).
            \item \textbf{Logistic Regression}: Binary classification tasks (e.g., spam detection).
            \item \textbf{Decision Trees}: Classification and regression tasks.
            \item \textbf{Support Vector Machines (SVM)}: Finding hyperplanes for classification.
            \item \textbf{Neural Networks}: For complex patterns, used in image and speech recognition.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning - Examples and Applications}
    % Provide examples and real-world applications of supervised learning.
    \begin{block}{Real-World Examples}
        \begin{itemize}
            \item \textbf{Email Filtering}: Classifying emails as 'spam' or 'not spam'.
            \item \textbf{Credit Scoring}: Predicting loan default risks.
            \item \textbf{Image Recognition}: Identifying objects or faces in images.
        \end{itemize}
    \end{block}
    
    \begin{block}{Applications}
        Supervised learning finds applications in various sectors:
        \begin{itemize}
            \item \textbf{Healthcare}: Predicting disease outcomes based on patient data.
            \item \textbf{Finance}: Stock price forecasting and risk assessment.
            \item \textbf{Retail}: Analyzing customer behavior for targeted marketing.
            \item \textbf{Speech Recognition}: Converting spoken language into text.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning - Code Example}
    % Provide a code snippet illustrating linear regression in Python.
    \begin{block}{Linear Regression Example (Python)}
    Here is a simple example of linear regression using the \texttt{scikit-learn} library:
    \begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd

# Sample Data
data = pd.DataFrame({
    'Size': [1500, 1600, 1700, 1800, 2000],
    'Price': [300000, 320000, 340000, 360000, 400000]
})

# Features and Target
X = data[['Size']]
y = data['Price']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model Training
model = LinearRegression()
model.fit(X_train, y_train)

# Make Predictions
predictions = model.predict(X_test)
print(predictions)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning - Conclusion}
    % Summarize the significance of supervised learning.
    \begin{block}{Conclusion}
        Supervised learning is a powerful approach characterized by its reliance on labeled data for training. Understanding its principles, algorithms, and applications enables better leveraging of machine learning in real-world scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Unsupervised Learning - Definition}
  \begin{block}{Definition}
    Unsupervised learning is a type of machine learning where the model is trained on data without labeled responses. Unlike supervised learning, which uses input-output pairs, unsupervised learning identifies patterns, groups, or structures within the data itself.
  \end{block}
  
  \begin{itemize}
    \item \textbf{Objective:} Explore the underlying structure of data without explicit outputs.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Unsupervised Learning - Algorithms}
  \begin{block}{Algorithms}
    \begin{enumerate}
      \item \textbf{Clustering Algorithms:}
        \begin{itemize}
          \item \textbf{K-Means:} Partitions data into K distinct groups based on feature similarity.
            \begin{itemize}
              \item \textit{Example:} Grouping customers by purchasing behavior.
            \end{itemize}
          \item \textbf{Hierarchical Clustering:} Builds a tree of clusters based on a distance metric.
            \begin{itemize}
              \item \textit{Example:} Organizing family trees or taxonomies.
            \end{itemize}
        \end{itemize}
      \item \textbf{Dimensionality Reduction Algorithms:}
        \begin{itemize}
          \item \textbf{PCA:} Reduces dimensionality while retaining variance.
            \begin{itemize}
              \item \textit{Example:} Visualizing high-dimensional data in 2D/3D.
            \end{itemize}
          \item \textbf{t-SNE:} Visualizes high-dimensional data by mapping similar instances closer in lower dimensions.
            \begin{itemize}
              \item \textit{Example:} Image recognition clustering similar images.
            \end{itemize}
        \end{itemize}
      \item \textbf{Association Rule Learning:}
        \begin{itemize}
          \item Learns relationships between variables in large datasets. 
            \begin{itemize}
              \item \textit{Example:} Market Basket Analysis.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Unsupervised Learning - Applications & Summary}
  \begin{block}{Real-World Applications}
    \begin{itemize}
      \item \textbf{Customer Segmentation:} Identifies distinct customer groups for targeted marketing.
      \item \textbf{Image Compression:} Utilizes PCA to reduce pixels while preserving features.
      \item \textbf{Genetic Data Analysis:} Finds gene patterns and evolutionary relationships.
      \item \textbf{Recommendation Systems:} Analyzes user behavior to suggest products.
    \end{itemize}
  \end{block}
  
  \begin{block}{Key Points}
    \begin{itemize}
      \item \textbf{No Labeled Data:} Requires only input data, useful without labels.
      \item \textbf{Pattern Discovery:} Powerful for discovering hidden patterns.
      \item \textbf{Exploratory Data Analysis (EDA):} Crucial for understanding data structure.
    \end{itemize}
  \end{block}
  
  \begin{block}{Summary}
    Unsupervised learning plays a vital role in analyzing complex datasets by finding patterns and structures without prior knowledge or labels, enabling data-driven decisions across various domains.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing Supervised and Unsupervised Learning}
    \begin{block}{Introduction to Learning Paradigms}
        Machine learning can be broadly categorized into two primary types: \textbf{Supervised Learning} and \textbf{Unsupervised Learning}. Understanding the differences between these approaches is essential for selecting the right one for a given problem.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning}
    \begin{block}{Definition}
        Supervised learning involves training a model on a labeled dataset, which means that each training example is paired with an output label.
    \end{block}
    
    \begin{itemize}
        \item \textbf{How it Works:}
        \begin{enumerate}
            \item Input Data: Labeled data is provided, where each input comes with a corresponding output.
            \item Model Training: The algorithm learns to map inputs to the correct outputs.
            \item Prediction: After training, the model can predict the output for new, unseen data.
        \end{enumerate}
        
        \item \textbf{Key Algorithms:}
        \begin{itemize}
            \item Linear Regression
            \item Decision Trees
            \item Support Vector Machines (SVM)
            \item Neural Networks
        \end{itemize}
        
        \item \textbf{Applications:}
        \begin{itemize}
            \item Spam Detection
            \item Image Recognition
            \item Medical Diagnosis
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning}
    \begin{block}{Definition}
        Unsupervised learning identifies patterns in data without labeled responses. It discovers the inherent structure in the input data.
    \end{block}
    
    \begin{itemize}
        \item \textbf{How it Works:}
        \begin{enumerate}
            \item Input Data: Unlabeled data is given, which means there are no output labels associated with the input.
            \item Model Training: The algorithm learns to group similar data points or understand the data structure.
        \end{enumerate}
        
        \item \textbf{Key Algorithms:}
        \begin{itemize}
            \item K-Means Clustering
            \item Hierarchical Clustering
            \item Principal Component Analysis (PCA)
        \end{itemize}
        
        \item \textbf{Applications:}
        \begin{itemize}
            \item Customer Segmentation
            \item Anomaly Detection
            \item Market Basket Analysis
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points of Comparison}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Aspect} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} \\
            \hline
            Data Requirement & Requires labeled data & Works with unlabeled data \\
            \hline
            Learning Objective & Learn a mapping from input to output & Identify patterns or groupings in input data \\
            \hline
            Output & Predictive outputs based on learned relationships & Insights or categories without predefined labels \\
            \hline
            Examples & Classification, regression tasks & Clustering, dimensionality reduction \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
        Choosing between supervised and unsupervised learning algorithms depends on the nature of your data and the specific problem you wish to solve. Supervised learning is valuable when the outcome is known and labeled, while unsupervised learning is optimal for uncovering hidden patterns in unlabeled data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Supervised Learning}
    Supervised learning involves training models on labeled datasets. The primary aim is to learn mappings from input features to output labels. This presentation covers three fundamental algorithms in supervised learning:
    \begin{itemize}
        \item Linear Regression
        \item Decision Trees
        \item Support Vector Machines
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Linear Regression}
    Linear Regression is a fundamental algorithm utilized for regression tasks. It is based on the assumption of a linear relationship between the input features \(X\) and the output variable \(Y\).
    
    \begin{block}{Formula}
        The model is expressed as:
        \begin{equation}
            Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + \epsilon
        \end{equation}
        Where:
        \begin{itemize}
            \item \(Y\) is the output variable.
            \item \(X_1, X_2, \ldots, X_n\) are the input features.
            \item \(\beta_0\) is the y-intercept.
            \item \(\beta_1, \beta_2, \ldots\) are the coefficients corresponding to the features.
            \item \(\epsilon\) is the error term.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Predicting house prices based on features like size, number of bedrooms, and location.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2. Decision Trees}
    Decision Trees are versatile algorithms applicable for both classification and regression tasks. They create a tree-like structure by splitting data into subsets based on feature values.
    
    \begin{block}{Key Characteristics}
        \begin{itemize}
            \item \textbf{Structure:} Composed of nodes (decisions) and leaves (outcomes).
            \item \textbf{Splitting Criteria:} 
            \begin{itemize}
                \item Gini impurity and information gain for classification.
                \item Mean squared error for regression.
            \end{itemize}
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Classifying customer purchase behavior:
        \begin{enumerate}
            \item If Age < 30 and Income > \$50k \(\rightarrow\) Class A
            \item If Age \(\geq\) 30 \(\rightarrow\) Class B
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{3. Support Vector Machines (SVM)}
    SVM is a powerful algorithm primarily used for classification. It identifies the hyperplane that best separates different class data points.
    
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Hyperplane:} A decision boundary that maximizes the margin between classes.
            \item \textbf{Support Vectors:} The data points closest to the hyperplane that define its edge.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula}
        The decision boundary is represented as:
        \begin{equation}
            w \cdot x + b = 0
        \end{equation}
        Where:
        \begin{itemize}
            \item \(w\) is the weight vector.
            \item \(x\) is the input feature vector.
            \item \(b\) is the bias term.
        \end{itemize}
    \end{block}
    
    \begin{block}{Example}
        Classifying emails as spam or not based on word frequency features.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Algorithms in Unsupervised Learning - Overview}
    \begin{block}{Overview}
        Unsupervised learning involves training a model on unlabeled data to identify hidden patterns or structures. The key algorithms in this domain include:
        \begin{itemize}
            \item K-Means Clustering
            \item Hierarchical Clustering
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering}
    \begin{block}{Concept}
        K-Means clustering partitions the dataset into *K* distinct clusters based on feature similarity through an iterative process.
    \end{block}
    
    \begin{block}{Steps}
        \begin{enumerate}
            \item Choose the number of clusters *K*.
            \item Randomly initialize the centroids.
            \item Assign each point to the nearest centroid.
            \item Update the centroids based on the mean of assigned points.
            \item Repeat until centroids stabilize.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example}
        Clusters of customers based on purchasing behavior:
        \begin{itemize}
            \item Cluster 1: Frequent buyers
            \item Cluster 2: Occasional buyers
            \item Cluster 3: One-time buyers
        \end{itemize}
    \end{block}
    
    \begin{block}{Cost Function}
        The cost function for K-Means is:
        \begin{equation}
            J = \sum_{i=1}^{K} \sum_{x \in C_i} || x - \mu_i ||^2
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Hierarchical Clustering}
    \begin{block}{Concept}
        Hierarchical clustering builds a tree of clusters (dendrogram) by either merging or splitting clusters. This method does not require pre-specifying the number of clusters.
    \end{block}
    
    \begin{block}{Steps (Agglomerative)}
        \begin{enumerate}
            \item Treat each data point as an individual cluster.
            \item Compute distances between all clusters.
            \item Merge the two closest clusters.
            \item Repeat until only one cluster remains.
            \item Cut the dendrogram to determine final clusters.
        \end{enumerate}
    \end{block}

    \begin{block}{Example}
        Analyzing different species of flowers to visualize relationships based on petal features leads to insights of their closeness.
    \end{block}
    
    \begin{block}{Key Points}
        \begin{itemize}
            \item K-Means: Requires defining *K*, sensitive to outliers.
            \item Hierarchical: No prior cluster number needed, computationally intensive.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{K-Means Clustering Code Example}
    Hereâ€™s a simple implementation of K-Means in Python using Scikit-learn:

    \begin{lstlisting}[language=Python]
from sklearn.cluster import KMeans
import numpy as np

# Sample data
data = np.array([[1, 2], [1, 4], [1, 0],
                 [4, 2], [4, 4], [4, 0]])

# Initialize KMeans
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

# Display results
print("Centroids:", kmeans.cluster_centers_)
print("Labels:", kmeans.labels_)
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Overfitting - Introduction}
    \begin{block}{What is Overfitting?}
        \begin{itemize}
            \item \textbf{Definition}: Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and outliers, leading to a model that performs well on training data but poorly on unseen data (test set).
            \item \textbf{Analogy}: Like cramming for an exam by memorizing answers without understanding the material; high performance on the test but poor application in real scenarios.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Overfitting - Causes}
    \begin{block}{Causes of Overfitting}
        \begin{enumerate}
            \item \textbf{Complex Models}: Too many parameters or a high degree of complexity can capture noise.
            \item \textbf{Insufficient Training Data}: Small datasets lead to overfitting as the model learns too much detail.
            \item \textbf{Too Many Features}: Including irrelevant features misleads the model.
            \item \textbf{Lack of Regularization}: Without regularization techniques, models fit noise easily.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Overfitting - Impact and Conclusion}
    \begin{block}{Impact on Model Performance}
        \begin{itemize}
            \item \textbf{Training vs. Test Accuracy}:
                \begin{itemize}
                    \item \textbf{High Training Accuracy}: Model shows high accuracy on training set.
                    \item \textbf{Low Test Accuracy}: Indicates poor generalization.
                \end{itemize}
            \item \textbf{Example of Polynomial Regression}:
                \begin{itemize}
                    \item \textbf{Underfitting}: Simple models like linear regression have high bias.
                    \item \textbf{Just Right}: Polynomial degrees of 2 or 3 capture underlying trends well.
                    \item \textbf{Overfitting}: High degree polynomials fit noise perfectly, indicating high variance.
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Generalization}: The goal of any model is to generalize to unseen data.
            \item \textbf{Bias-Variance Tradeoff}: Overfitting shows high variance, while underfitting indicates high bias.
            \item \textbf{Validation}: Always use a separate validation/test dataset for assessment.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding overfitting is crucial for effective machine learning models. Recognizing symptoms enables the application of techniques to improve performance and ensure good generalization to new data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting - Introduction}
    \begin{block}{Understanding Overfitting}
        Overfitting occurs when a machine learning model learns noise in the training dataset rather than the underlying patterns. This can lead to poor generalization on unseen data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting - Cross-Validation}
    \begin{itemize}
        \item Cross-validation is used to assess the performance of a model by partitioning the training dataset into multiple subsets.
        \item The model is trained on some subsets and tested on the remaining ones.
    \end{itemize}
    
    \begin{block}{K-Fold Cross-Validation}
        \begin{itemize}
            \item Data is divided into 'K' subsets (folds).
            \item For each group, it serves as a testing set while the others are used for training.
            \item This process is repeated K times to create K different models.
        \end{itemize}
    \end{block}
    
    \begin{block}{Benefits}
        \begin{itemize}
            \item Provides a better estimate of model performance.
            \item Maximizes data utilization, especially useful with limited data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting - Regularization}
    \begin{block}{Overview}
        Regularization techniques add a penalty to the loss function based on model complexity, thereby discouraging overly complex models.
    \end{block}
    
    \begin{itemize}
        \item \textbf{L1 Regularization (Lasso):} Adds the absolute value of coefficients as a penalty, shrinking some coefficients to zero.
            \begin{equation}
                L(\theta) = \sum (y_i - \hat{y}_i)^2 + \lambda \sum |\theta_j| 
            \end{equation}
        \item \textbf{L2 Regularization (Ridge):} Adds the squared values of coefficients as a penalty to prevent large weights.
            \begin{equation}
                L(\theta) = \sum (y_i - \hat{y}_i)^2 + \lambda \sum \theta_j^2 
            \end{equation}
    \end{itemize}
    
    \begin{block}{Benefits}
        \begin{itemize}
            \item Simplifies the model.
            \item Reduces chance of overfitting by controlling complexity.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting - Pruning Techniques}
    \begin{block}{Overview}
        Pruning removes parts of the model that do not add predictive power, especially in decision trees.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Pre-Pruning:} Stops tree growth before fully splitting data (e.g., setting depth limits).
        \item \textbf{Post-Pruning:} Fully grows the tree then removes unimportant branches using validation data.
    \end{itemize}
    
    \begin{block}{Benefits}
        \begin{itemize}
            \item Reduces model complexity.
            \item Improves generalization performance.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strategies to Avoid Overfitting - Conclusion and Example}
    \begin{block}{Conclusion}
        Effective strategies like cross-validation, regularization, and pruning enhance model performance by focusing on simplicity and robustness against noise. Incorporating these techniques can improve predictions.
    \end{block}

    \begin{block}{Example Code Snippet (Python for Lasso Regression)}
    \begin{lstlisting}[language=Python]
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.01)  # alpha is the regularization parameter
model.fit(X_train, y_train)
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Studies in Machine Learning}
    \begin{block}{Introduction to Machine Learning Case Studies}
        Machine Learning (ML) is transforming industries by providing innovative solutions to complex problems. Understanding real-world applications can help illustrate both supervised and unsupervised learning techniques.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Supervised Learning Case Study: Email Spam Detection}
    \begin{itemize}
        \item \textbf{Concept:} 
            Supervised learning involves training a model on a labeled dataset.
        \item \textbf{Example:} 
            Spam email detection, where the model predicts if an email is "spam" or "not spam".
    \end{itemize}
    
    \begin{block}{Process:}
        \begin{enumerate}
            \item Data Collection: Gather a dataset of emails with labels.
            \item Feature Extraction: Identify features (e.g., frequency of certain words, sender's address).
            \item Model Training: Use algorithms such as Support Vector Machines (SVM) or Naive Bayes.
            \item Evaluation: Assess accuracy, precision, and recall.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Unsupervised Learning Case Study: Customer Segmentation}
    \begin{itemize}
        \item \textbf{Concept:} 
            Unsupervised learning identifies patterns in data without predefined labels.
        \item \textbf{Example:} 
            Customer segmentation for targeted marketing strategies.
    \end{itemize}
    
    \begin{block}{Process:}
        \begin{enumerate}
            \item Data Collection: Compile purchase data without labels.
            \item Feature Selection: Choose relevant metrics (e.g., purchase frequency, total spent).
            \item Model Application: Use clustering algorithms like K-means or Hierarchical Clustering.
            \item Insights: Analyze clusters for unique marketing strategies.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Additional Resources}
    \begin{block}{Conclusion}
        Case studies in machine learning emphasize practical applications of supervised and unsupervised learning techniques. Understanding real-world examples enhances the grasp of ML's potential to solve complex challenges.
    \end{block}
    
    \begin{block}{Additional Resources}
        \begin{itemize}
            \item Recommended Reading: 
                "Pattern Recognition and Machine Learning" by Christopher Bishop.
            \item Tools: 
                Python libraries such as Scikit-learn for implementing algorithms.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Key Takeaways}
    \begin{enumerate}
        \item \textbf{Understanding Machine Learning}
        \begin{itemize}
            \item A subset of AI focused on developing algorithms for data-driven learning.
        \end{itemize}

        \item \textbf{Types of Machine Learning}
        \begin{itemize}
            \item Supervised Learning: Learning from labeled data (e.g., predicting house prices).
            \item Unsupervised Learning: Identifying patterns in unlabeled data (e.g., customer segmentation).
        \end{itemize}

        \item \textbf{Real-World Applications}
        \begin{itemize}
            \item Healthcare: Early disease detection using classification algorithms.
            \item Finance: Fraud detection through anomaly detection.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Importance of ML Concepts}
    \begin{block}{Significance}
        \begin{itemize}
            \item Comprehending ML concepts enables effective data-driven decision-making.
            \item Helps in selecting suitable algorithms based on specific problems.
            \item Enhances collaboration with data scientists and technical teams.
        \end{itemize}
    \end{block}
    
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Iterative Nature of Learning:} Models improve with more data.
            \item \textbf{Evaluation Metrics:} Critical for assessing model performance (e.g., accuracy, precision, recall, F1 score).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Model Performance Evaluation}
    To evaluate model performance in classification, use the formula:
    \begin{equation}
        \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
    
    Where:
    \begin{itemize}
        \item \textbf{Precision} = \( \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} \)
        \item \textbf{Recall} = \( \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}} \)
    \end{itemize}
    
    \begin{block}{Closing Thought}
        Mastering ML principles is crucial for leveraging AI technologies, fostering innovation, and solving complex problems in various fields.
    \end{block}
\end{frame}


\end{document}