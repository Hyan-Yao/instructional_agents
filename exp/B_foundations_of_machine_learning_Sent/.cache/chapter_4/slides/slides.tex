\documentclass{beamer}

% Theme choice
\usetheme{Madrid} % You can change to e.g., Warsaw, Berlin, CambridgeUS, etc.

% Encoding and font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Graphics and tables
\usepackage{graphicx}
\usepackage{booktabs}

% Code listings
\usepackage{listings}
\lstset{
basicstyle=\ttfamily\small,
keywordstyle=\color{blue},
commentstyle=\color{gray},
stringstyle=\color{red},
breaklines=true,
frame=single
}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}

% Colors
\usepackage{xcolor}

% TikZ and PGFPlots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}

% Hyperlinks
\usepackage{hyperref}

% Title information
\title{Statistical Foundations}
\author{Your Name}
\institute{Your Institution}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Statistical Foundations - Overview}
    \begin{block}{1. Importance of Statistical Foundations}
        \begin{itemize}
            \item \textbf{Definition:} Statistical foundations comprise the principles of probability and statistics that form the conceptual basis for data analysis and machine learning.
            \item \textbf{Necessity:} Understanding these concepts is crucial for designing algorithms that can learn from data, make predictions, and inform decision-making processes.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Statistical Foundations - Probability}
    \begin{block}{2. Role of Probability in Machine Learning}
        \begin{itemize}
            \item \textbf{Probability:} The measure of the likelihood that an event will occur. In machine learning, we use probability to quantify uncertainty.
            \item \textbf{Applications:}
            \begin{itemize}
                \item \textbf{Modeling Uncertainty:} Using probabilistic models (e.g., Gaussian distributions) to predict outcomes.
                \item \textbf{Decision-Making:} Algorithms like Bayesian inference utilize probability to update beliefs based on new data.
            \end{itemize}
            \item \textbf{Example:} Suppose you want to predict whether it will rain tomorrow. A probability model might assign a 70\% chance of rain based on historical data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Statistical Foundations - Statistics}
    \begin{block}{3. Role of Statistics in Machine Learning}
        \begin{itemize}
            \item \textbf{Statistics:} The science of data collection, analysis, interpretation, and presentation. It provides methods for summarizing and understanding large datasets.
            \item \textbf{Core Areas:}
            \begin{itemize}
                \item \textbf{Descriptive Statistics:} Summarizes data sets (mean, median, mode, standard deviation).
                \item \textbf{Inferential Statistics:} Draws conclusions about populations based on sample data (confidence intervals, hypothesis testing).
            \end{itemize}
            \item \textbf{Example:} Analyzing a dataset of student test scores: average is 75 with a standard deviation of 10.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas and Key Points}
    \begin{block}{Formulas to Consider}
        \begin{equation}
            \mu = \frac{1}{N} \sum_{i=1}^{N} x_i \quad \text{(Mean)}
        \end{equation}
        \begin{equation}
            \sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2} \quad \text{(Standard Deviation)}
        \end{equation}
    \end{block}
    
    \begin{block}{Conclusion}
        A solid grasp of statistical concepts empowers practitioners to build robust machine learning models, leading to better predictions and insights from data. In the next section, we will explore the core concepts of machine learning, including different types of learning paradigms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Definition}
    \begin{block}{Definition of Machine Learning}
        \textbf{Machine Learning (ML)} is a subset of artificial intelligence (AI) that involves training algorithms to identify patterns and make decisions from data. Unlike traditional programming, where rules are explicitly coded, in ML, the system learns from the data provided, improving its performance over time without explicit programming for each task.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Categories}
    \textbf{Categories of Machine Learning}\\
    Machine learning can be broadly classified into three main types: 
    \begin{itemize}
        \item \textbf{Supervised Learning} 
        \item \textbf{Unsupervised Learning} 
        \item \textbf{Reinforcement Learning} 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Supervised Learning}
    \begin{block}{Supervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: Involves training a model on a labeled dataset where input comes with corresponding output labels.
            \item \textbf{Process}: The algorithm learns to predict outputs from given inputs.
            \item \textbf{Examples}:
            \begin{itemize}
                \item Classification: Determining if an email is spam or not.
                \item Regression: Predicting house prices based on features.
            \end{itemize}
            \item \textbf{Key Point}: Labeled data is crucial for the learning process.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Unsupervised Learning}
    \begin{block}{Unsupervised Learning}
        \begin{itemize}
            \item \textbf{Definition}: Operates on unlabeled data, learning the underlying structure without predefined labels.
            \item \textbf{Process}: Identifies patterns and clusters purely from input features.
            \item \textbf{Examples}:
            \begin{itemize}
                \item Clustering: Grouping customers based on behavior.
                \item Dimensionality Reduction: E.g., Principal Component Analysis.
            \end{itemize}
            \item \textbf{Key Point}: Useful for discovering hidden patterns in unlabeled data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Reinforcement Learning}
    \begin{block}{Reinforcement Learning}
        \begin{itemize}
            \item \textbf{Definition}: Focuses on decision-making through interactions with an environment, learning by taking actions that yield rewards or penalties.
            \item \textbf{Process}: The agent interacts with its environment and updates its strategy based on feedback.
            \item \textbf{Examples}:
            \begin{itemize}
                \item Game Playing: Training AI to play chess or Go.
                \item Robotics: Teaching robots to navigate obstacles.
            \end{itemize}
            \item \textbf{Key Point}: The trial-and-error approach is fundamental, allowing for progressive improvement.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Summary Table}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|l|l|l|}
            \hline
            \textbf{Learning Type} & \textbf{Data Type} & \textbf{Learning Mechanism} & \textbf{Examples} \\
            \hline
            Supervised Learning & Labeled & Mapping inputs to known outputs & Spam detection, House price prediction \\
            \hline
            Unsupervised Learning & Unlabeled & Identifying patterns or clusters in data & Customer segmentation, Topic modeling \\
            \hline
            Reinforcement Learning & Rewards/Penalties & Learning through interaction and feedback & Game AI, Robot navigation \\
            \hline
        \end{tabular}
        \caption{Summary of Machine Learning Types}
    \end{table}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Core Concepts of Machine Learning - Conclusion}
    \begin{block}{Conclusion}
        By understanding these core concepts and distinctions, we can more effectively create and apply machine learning models. The choice of learning type depends on the nature of the problem and the available data.
    \end{block}
    
    \begin{block}{Next Steps}
        Next, we will explore the foundational concepts of probability, which are integral to making informed decisions within these ML paradigms.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probability Basics - Introduction}
    \begin{block}{What is Probability?}
        \begin{itemize}
            \item \textbf{Definition:} Probability quantifies the likelihood of an event occurring, ranging from 0 (impossible) to 1 (certain).
            \item \textbf{Formula:} 
            \[
            P(A) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}}
            \]
            \item \textbf{Example:} Rolling a die:
            \[
            P(4) = \frac{1}{6} \text{ (1 favorable outcome out of 6 possible outcomes)}
            \]
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probability Basics - Conditional Probability}
    \begin{block}{Conditional Probability}
        \begin{itemize}
            \item \textbf{Definition:} Probability of event A occurring given that event B has occurred.
            \item \textbf{Formula:} 
            \[
            P(A|B) = \frac{P(A \cap B)}{P(B)}
            \]
            \item \textbf{Example:} Deck of cards:
            \begin{itemize}
                \item Let A = drawing a heart, B = drawing a red card.
                \item \( P(A) = \frac{13}{52} \) and \( P(B) = \frac{26}{52} \).
                \item Conditional Probability:
                \[
                P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{13}{52}}{\frac{26}{52}} = \frac{1}{2}
                \]
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probability Basics - Bayes' Theorem}
    \begin{block}{Bayes' Theorem}
        \begin{itemize}
            \item \textbf{Definition:} Describes how to update the probability of a hypothesis based on new evidence.
            \item \textbf{Formula:}
            \[
            P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
            \]
            \item \textbf{Example:} Testing for a disease:
            \begin{itemize}
                \item Let \( H \) = "has the disease", \( E \) = "test is positive".
                \item Given values: \( P(H) = 0.01 \), \( P(E|H) = 0.9 \), and \( P(E|\neg H) = 0.05 \).
                \item Calculate \( P(E) \):
                \[
                P(E) = P(E|H) \cdot P(H) + P(E|\neg H) \cdot P(\neg H) = 0.009 + 0.0495 = 0.0585 
                \]
                \item Applying Bayes' Theorem:
                \[
                P(H|E) = \frac{0.9 \cdot 0.01}{0.0585} \approx 0.154
                \]
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probability Basics - Key Points}
    \begin{itemize}
        \item \textbf{Understanding Probability:} Essential for decision-making in uncertain situations.
        \item \textbf{Conditional Probability:} Critical for real-world decisions based on established conditions.
        \item \textbf{Bayes' Theorem:} Key in statistics and machine learning for updating beliefs with new information.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probability Basics - Conclusion}
    \begin{block}{Conclusion}
        Grasping these fundamental concepts in probability provides the foundation for advanced statistical methods and enhances analytical skills essential in machine learning frameworks.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Descriptive Statistics - Introduction}
    \begin{block}{Introduction to Descriptive Statistics}
        Descriptive statistics are essential tools in summarizing and understanding data sets. They convey complex information in a simplified manner.
    \end{block}
    
    Here, we will explore key measures: \textbf{Mean, Median, Mode}, and \textbf{Standard Deviation}.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Descriptive Statistics - Key Measures}
    \begin{enumerate}
        \item \textbf{Mean}
            \begin{itemize}
                \item **Definition**: Average of values, calculated by summing all values and dividing by the number of values.
                \item **Formula**: 
                \[
                \text{Mean} (\mu) = \frac{\sum_{i=1}^{N} x_i}{N}
                \]
                \end{itemize}
        \item \textbf{Median}
            \begin{itemize}
                \item **Definition**: Middle value when data is ordered, or average of two middle values if even.
                \item **Example**: For \( [3, 5, 7, 10] \): 
                \[
                \text{Median} = \frac{5 + 7}{2} = 6
                \end{itemize}
        \item \textbf{Mode}
            \begin{itemize}
                \item **Definition**: Most frequently occurring value; can have no mode, one mode, or multiple modes.
                \item **Example**: In \( [1, 2, 4, 4, 5] \), mode = 4; in \( [1, 1, 2, 3, 3, 4] \), modes = 1 and 3 (bimodal).
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Descriptive Statistics - Variation and Key Points}
    \begin{itemize}
        \item \textbf{Standard Deviation}
            \begin{itemize}
                \item **Definition**: Measures the amount of variation or dispersion.
                \item **Formula**: 
                \[
                \sigma = \sqrt{\frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}}
                \]
                \item **Example**: For \( [2, 4, 4, 4, 5, 5, 7, 9] \), standard deviation \( \sigma = 2 \).
            \end{itemize}
        \item \textbf{Key Points to Emphasize}
            \begin{itemize}
                \item \textbf{Central Tendency}: Mean, median, and mode indicate the central location of data. 
                    \begin{itemize}
                        \item Mean: sensitive to outliers.
                        \item Median: robust against skewed data.
                        \item Mode: identifies most prevalent value.
                    \end{itemize}
                \item \textbf{Understanding Variation}: Standard deviation is key for understanding data reliability.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Descriptive Statistics - Conclusion}
    \begin{block}{Conclusion}
        Descriptive statistics are foundational in statistical analysis, allowing for effective data summarization. Understanding these key measures equips you to analyze data accurately, paving the way for inferential statistics.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Inferential Statistics - Overview}
    Inferential statistics allows us to make inferences and predictions about a population based on a sample of data. 
    While descriptive statistics provides a summary of the data at hand, inferential statistics takes it a step further by enabling us to draw conclusions that extend beyond the immediate data.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Hypothesis Testing}
    \begin{itemize}
        \item \textbf{Definition}: A statistical method that uses sample data to evaluate a hypothesis about a population parameter.
        \item \textbf{Types of Hypotheses}:
        \begin{itemize}
            \item \textbf{Null Hypothesis (H0)}: The statement being tested, usually positing no effect or no difference.
            \item \textbf{Alternative Hypothesis (H1)}: The statement that we aim to support, suggesting an effect or difference exists.
        \end{itemize}
        \item \textbf{Example}: 
        \begin{itemize}
            \item Testing if a new drug is more effective than an existing one:
            \begin{itemize}
                \item H0: The new drug has no effect.
                \item H1: The new drug has a positive effect.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - Confidence Intervals}
    \begin{itemize}
        \item \textbf{Definition}: A range of values that is likely to contain the population parameter with a specified level of confidence (e.g., 95%).
        \item \textbf{Construction}:
        \begin{equation}
            \text{Confidence Interval} = \bar{x} \pm z \left( \frac{\sigma}{\sqrt{n}} \right)
        \end{equation}
        \begin{itemize}
            \item Where \( \bar{x} \) = sample mean, \( z \) = z-score, \( \sigma \) = population standard deviation, \( n \) = sample size.
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item For \( \bar{x} = 10 \), \( \sigma = 2 \), and \( n = 30 \):
            \begin{equation}
                CI = 10 \pm 1.96 \left( \frac{2}{\sqrt{30}} \right) \approx [9.26, 10.74]
            \end{equation}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts - P-values}
    \begin{itemize}
        \item \textbf{Definition}: The probability of obtaining test results at least as extreme as the observed results, assuming that the null hypothesis is true.
        \item \textbf{Interpretation}:
        \begin{itemize}
            \item A low p-value (typically â‰¤ 0.05) indicates strong evidence against H0, leading to its rejection.
            \item A high p-value suggests weak evidence against H0, thus it cannot be rejected.
        \end{itemize}
        \item \textbf{Example}:
        \begin{itemize}
            \item If the p-value is 0.03, we would reject H0 at the 0.05 significance level, suggesting significant effect of the new drug.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Inferential statistics provides tools for making predictions and decisions based on data.
        \item Understanding hypothesis testing, confidence intervals, and p-values is crucial for interpreting quantitative research.
        \item These concepts are the foundation for more complex statistical analyses and are widely applicable in various fields.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Distributions in Statistics - Introduction}
    \begin{block}{Introduction to Probability Distributions}
        In statistics, \textbf{probability distributions} describe how the values of a random variable are distributed. They are fundamental to inferential statistics and form the basis for many machine learning models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Distributions in Statistics - Common Types}
    \begin{block}{Common Types of Distributions}
        \begin{enumerate}
            \item \textbf{Normal Distribution}
            \item \textbf{Binomial Distribution}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Normal Distribution}
    \begin{itemize}
        \item \textbf{Definition}: A continuous probability distribution characterized by its bell-shaped curve, defined by the mean ($\mu$) and standard deviation ($\sigma$).
        
        \item \textbf{Properties}:
        \begin{itemize}
            \item Symmetrical around the mean.
            \item Approximately 68\% of data falls within one standard deviation from the mean.
            \item Approximately 95\% lies within two standard deviations.
        \end{itemize}
        
        \item \textbf{Formula}:
        \begin{equation}
            f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
        \end{equation}

        \item \textbf{Applications in Machine Learning}:
        \begin{itemize}
            \item Used in algorithms like Logistic Regression, Naive Bayes, and in assumptions for various tests.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Binomial Distribution}
    \begin{itemize}
        \item \textbf{Definition}: A discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials (e.g., flipping a coin).
        
        \item \textbf{Properties}:
        \begin{itemize}
            \item Defined by the number of trials ($n$) and the probability of success ($p$).
            \item The trials must be independent.
        \end{itemize}
        
        \item \textbf{Formula}:
        \begin{equation}
            P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
        \end{equation}
        
        \item \textbf{Applications in Machine Learning}:
        \begin{itemize}
            \item Useful for binary classification problems (e.g., detecting whether an email is spam or not).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Examples}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Understanding probability distributions is crucial for making decisions based on data.
            \item Many machine learning algorithms rely on the underlying assumptions of these distributions to make predictions.
            \item Properly modeling the distribution can significantly improve the performance of machine learning models.
        \end{itemize}
    \end{block}

    \begin{block}{Example Illustrations}
        \begin{itemize}
            \item \textbf{Normal Distribution}: Good for continuous data like heights, test scores; provides insights into variability and central tendency.
            \item \textbf{Binomial Distribution}: Examples include modeling the number of heads in 10 coin tosses, helping in understanding outcomes in reliability testing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{Summary}
        Probability distributions are essential for understanding the statistical underpinnings necessary for effective machine learning. By mastering these concepts, you empower yourself to analyze data, apply suitable models, and interpret results more accurately.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Sampling and Data Collection - Overview}
    \begin{block}{Understanding Sampling}
        Sampling is the process of selecting a subset of individuals from a larger population to estimate characteristics of the whole population. Proper sampling is crucial to ensure that the collected data accurately reflects the population, which enables reliable statistical inferences.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Sampling Methods}
    \begin{enumerate}
        \item \textbf{Simple Random Sampling:}
        \begin{itemize}
            \item Each member has an equal chance of being selected.
            \item \textit{Example:} Drawing names from a hat.
            \item \textit{Importance:} Reduces bias and ensures diversity.
        \end{itemize}
        
        \item \textbf{Systematic Sampling:}
        \begin{itemize}
            \item Selecting every k-th member from a list.
            \item \textit{Example:} Surveying every 5th person on an alphabetical list.
            \item \textit{Importance:} Easy to implement, but may introduce bias if the list has a pattern.
        \end{itemize}
        
        \item \textbf{Stratified Sampling:}
        \begin{itemize}
            \item Dividing the population into subgroups (strata) and randomly sampling from each.
            \item \textit{Example:} Research on student performance may stratify by grade level.
            \item \textit{Importance:} Ensures representation of key subgroups.
        \end{itemize}
        
        \item \textbf{Cluster Sampling:}
        \begin{itemize}
            \item Dividing the population into clusters (often geographically) and randomly selecting entire clusters.
            \item \textit{Example:} Surveying all students in randomly selected schools.
            \item \textit{Importance:} Cost-effective, especially for large populations.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Representative Data Collection}
    \begin{itemize}
        \item \textbf{Accuracy in Inference:} Non-representative samples can lead to misleading results. Polling only a certain demographic can introduce bias.
        
        \item \textbf{Generalizability:} Findings from the sample should accurately reflect those of the entire population, crucial for applying research findings to real-world scenarios.
        
        \item \textbf{Statistical Power:} A well-chosen sampling method increases the likelihood of detecting a true effect. The larger and more representative the sample, the greater the power of your statistical tests.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item How you sample deeply affects data quality and interpretation.
            \item Sample size matters: Larger samples generally provide better estimates but may require more resources.
            \item Always consider potential biases in sampling methods.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formulas}
        \textbf{Sample Size Calculation:} 
        \begin{equation}
        n = \left(\frac{Z^2 \cdot p \cdot (1-p)}{E^2}\right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $n$ = sample size
            \item $Z$ = Z-value (e.g., 1.96 for 95\% confidence)
            \item $p$ = estimated proportion (if unknown, use 0.5 for maximum variability)
            \item $E$ = margin of error
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Choosing the right sampling method and ensuring representative data collection are pivotal to the success of statistical analyses. Understanding these concepts enables you to make informed decisions and draw valid conclusions from your data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Introduction}
    \begin{block}{Introduction}
        Model evaluation metrics are essential for assessing the performance of statistical models, especially in classification tasks. 
        Understanding these metrics helps in choosing the best model for a specific problem.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Key Metrics}
    \begin{enumerate}
        \item \textbf{Accuracy}
        \begin{itemize}
            \item \textbf{Definition}: The proportion of true results (both true positives and true negatives) among the total number of cases examined.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Cases}}
            \end{equation}
            \item \textbf{Example}: If a model predicts 90 correct out of 100 total instances (60 true positives + 30 true negatives), the accuracy is 90\%.
        \end{itemize}

        \item \textbf{Precision}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to the total predicted positives, reflecting the quality of positive predictions.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{equation}
            \item \textbf{Example}: If a model predicts 40 positives, with 30 correct and 10 incorrect, then precision is \( \frac{30}{40} = 0.75 \) (or 75\%).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Continued}
    \begin{enumerate}
        \setcounter{enumi}{2} % Start from Recall
        
        \item \textbf{Recall (Sensitivity)}
        \begin{itemize}
            \item \textbf{Definition}: The ratio of correctly predicted positive observations to all actual positives, measuring the model's ability to find all relevant cases.
            \item \textbf{Formula}: 
            \begin{equation}
            \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{equation}
            \item \textbf{Example}: If there are 50 positives in the data and the model predicts 30 correctly, recall is \( \frac{30}{50} = 0.60 \) (or 60\%).
        \end{itemize}

        \item \textbf{F1 Score}
        \begin{itemize}
            \item \textbf{Definition}: The harmonic mean of precision and recall, indicating a balance between both metrics.
            \item \textbf{Formula}: 
            \begin{equation}
            F1 \text{ Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
            \end{equation}
            \item \textbf{Example}: If precision is 0.75 and recall is 0.60, then F1 score is \( 2 \times \frac{0.75 \times 0.60}{0.75 + 0.60} \approx 0.67 \).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Model Evaluation Metrics - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Trade-offs}: Improving precision might reduce recall and vice versa. Analyzing both metrics is crucial for comprehensive model performance understanding.
            \item \textbf{Contextual Relevance}: The importance of precision vs. recall can depend on the specific application (e.g., medical diagnosis vs. spam detection).
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Each metric provides unique insights into model performance. By understanding and utilizing these evaluation metrics, you can select the best-performing statistical model for your specific use case.
    \end{block}

    \begin{block}{Remember}
        \begin{itemize}
            \item Use \textbf{Accuracy} for a quick general performance metric.
            \item Prioritize \textbf{Precision} when false positives are costly.
            \item Focus on \textbf{Recall} when it's critical to catch all positives.
            \item The \textbf{F1 Score} is your go-to for a balanced view of precision and recall.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Statistics - Introduction}
    \begin{block}{Introduction}
        Ethics in statistics involves responsible statistical analysis that includes:
        \begin{itemize}
            \item Integrity of data reporting
            \item Elimination of bias
            \item Ensuring fairness in conclusions
        \end{itemize}
        Ethical lapses can mislead stakeholders and harm society.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Ethical Implications}
    \begin{enumerate}
        \item \textbf{Data Reporting}
            \begin{itemize}
                \item Accurate representation of findings.
                \item \textit{Example:} Reporting confidence intervals for drug efficacy metrics enhances transparency.
            \end{itemize}
        
        \item \textbf{Bias in Data}
            \begin{itemize}
                \item Systematic errors leading to incorrect conclusions.
                \item \textit{Types of Bias:}
                    \begin{itemize}
                        \item Selection Bias: Non-representative samples.
                        \item Confirmation Bias: Ignoring contradictory evidence.
                    \end{itemize}
                \item \textit{Example:} Diversity in clinical trial participants is crucial to apply findings universally.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fairness and Conclusion}
    \begin{block}{Fairness in Statistical Analysis}
        \begin{itemize}
            \item Ensuring models do not discriminate based on inherent characteristics.
            \item \textit{Example:} Predictive policing algorithms can unfairly target minorities if trained on biased data.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Accountability}: Statisticians must provide clear data context.
            \item \textbf{Transparency}: Open methods foster trust and prevent unethical practices.
            \item \textbf{Inclusivity}: Diverse datasets ensure fairness.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Ethical considerations in statistics enhance credibility, social responsibility, and informed decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 1}
    \begin{block}{The Importance of Statistical Foundations in Machine Learning}
        \begin{itemize}
            \item **Understanding Statistical Foundations:**
                \begin{itemize}
                    \item Definition: Theories and methods of statistics and probability essential for analyzing data and making informed decisions.
                    \item Role in Machine Learning: Provides tools for understanding data distributions, variability, and relationships crucial for predictive modeling.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 2}
    \begin{block}{Decision-Making with Probability and Statistics}
        \begin{itemize}
            \item **Probability**: Assesses the likelihood of outcomes, facilitating predictions based on uncertainty.
            \item **Statistics**: Summarizes data, tests hypotheses, and makes inferences, crucial for model validation.
        \end{itemize}
    \end{block}

    \begin{block}{Key Concepts to Emphasize}
        \begin{itemize}
            \item **Data Distribution**: Guides model and algorithm selection.
            \item **Hypothesis Testing**: Framework for making inferences; use of p-values in model evaluation.
            \item **Confidence Intervals**: Context for model performance; given by the formula:
            \[
            CI = \bar{x} \pm z \left( \frac{s}{\sqrt{n}} \right)
            \]
            Where \( \bar{x} \) is the sample mean, \( s \) is the standard deviation, \( z \) is the Z-score, and \( n \) is the sample size.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Key Takeaways - Part 3}
    \begin{block}{Practical Application in Machine Learning}
        \begin{itemize}
            \item **Model Evaluation**: Metrics like accuracy, precision, recall, and F1-score are statistical measures for evaluating effectiveness.
            \item **Algorithm Selection**: Performance varies under certain data distribution assumptions.
            \item **Data Cleaning and Preparation**: Statistical methods help detect outliers and assess dependencies.
        \end{itemize}
    \end{block}

    \begin{block}{Final Thoughts}
        \begin{itemize}
            \item A strong grasp of statistical principles is vital for machine learning practitioners.
            \item Ensures sound interpretation of results, promotes ethical data usage, and supports robust model design.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}