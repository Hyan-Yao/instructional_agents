\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Neural Networks]{Week 7: Neural Networks}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Overview}
    \begin{itemize}
        \item Neural networks are computational models inspired by the human brain.
        \item Composed of interconnected layers of nodes (neurons) for processing input data.
        \item Significant in machine learning for tasks such as:
        \begin{itemize}
            \item Image recognition
            \item Natural language processing
            \item Speech recognition
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Examples and Use Cases}
    \begin{block}{Example Use Cases}
        \begin{itemize}
            \item \textbf{Image Classification:} Identifies images (e.g., cat or dog) through pixel value processing.
            \item \textbf{Sentiment Analysis:} Evaluates sentiment of text (positive, negative, neutral) using word embeddings.
        \end{itemize}
    \end{block}
    
    \begin{block}{Illustration of a Neural Network}
        \begin{itemize}
            \item \textbf{Input Layer:} Contains input data (e.g., image pixel values).
            \item \textbf{Hidden Layers:} Processes data (e.g., detecting edges, shapes).
            \item \textbf{Output Layer:} Provides classification results (e.g., 'Cat' or 'Dog').
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Neural Networks - Key Points and Formula}
    \begin{itemize}
        \item \textbf{Architecture:} Consists of an input layer, hidden layers, and an output layer. Can range from simple to complex structures.
        \item \textbf{Learning Process:} Algorithms like backpropagation are used to minimize prediction errors by adjusting connection weights.
        \item \textbf{Adaptability:} Learns from various data types and adapts to new problems without explicit programming for each task.
    \end{itemize}
    
    \begin{equation}
        y = f(W \cdot x + b)
    \end{equation}

    Where:
    \begin{itemize}
        \item \( y \): Output of the neuron
        \item \( W \): Weights (learned during training)
        \item \( x \): Input features
        \item \( b \): Bias term
        \item \( f \): Activation function (e.g., sigmoid, ReLU)
    \end{itemize}
    
    \begin{block}{Code Snippet}
        \begin{lstlisting}[language=Python]
import numpy as np
from keras.models import Sequential
from keras.layers import Dense

# Example: Create a simple neural network
model = Sequential()
model.add(Dense(10, activation='relu', input_shape=(input_dim,)))  # Hidden Layer
model.add(Dense(1, activation='sigmoid'))  # Output Layer
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    Neural networks have revolutionized machine learning with their ability to learn complex patterns from data. 
    As we delve deeper into the next slide, we will explore what exactly a neural network is and its functionality in more detail.
\end{frame}

\begin{frame}[fragile]{What is a Neural Network?}
    \begin{block}{Definition of Neural Networks}
        A \textbf{neural network} is a computational model inspired by the biological neural networks in the human brain. It is a cornerstone of machine learning and artificial intelligence, enabling systems to recognize patterns, make decisions, and learn from data.
    \end{block}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Basic Functionality}
    \textbf{Neural networks} consist of interconnected nodes (neurons) that work together to process input data and produce output. They generally function as follows:

    \begin{enumerate}
        \item \textbf{Input Layer}: Receives the input data (features) and passes it on to the next layer.
        \item \textbf{Hidden Layers}: Special layers that perform computations and transformations on the input data to extract patterns. The number of hidden layers and neurons can vary.
        \item \textbf{Output Layer}: Produces the final output, such as classification or regression results.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Learning Process}
    \textbf{How Neural Networks Learn}:
    Neural networks learn through a process called \textbf{training}, which involves adjusting weights based on the data:

    \begin{itemize}
        \item \textbf{Forward Pass}: Input data moves forward through the layers, passing through weights and activating neurons based on activation functions (e.g., sigmoid, ReLU).
        \item \textbf{Loss Calculation}: At the output layer, the predicted outcome is compared to the actual outcome, calculating the error (loss).
        \item \textbf{Backward Pass (Backpropagation)}: The error is propagated backward through the layers to update the weights using optimization algorithms (e.g., gradient descent).
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Key Points and Example}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Neural networks can model complex relationships in data through their multiple layers and nonlinear activation functions.
            \item They require a substantial amount of data for training to achieve accurate predictions.
            \item Common applications include image recognition, natural language processing, and game playing.
        \end{itemize}
    \end{block}

    \textbf{Example:} \\
    - \textbf{Image Classification}: In an image recognition task, the input is a pixel array representing an image. Each layer of the neural network extracts features (like edges and shapes) leading to the identification of the object in the image.
\end{frame}

\begin{frame}[fragile]{What is a Neural Network? - Key Takeaway}
    \begin{block}{Key Takeaway}
        Neural networks are powerful tools in machine learning, enabling systems to \textit{learn from experience} and make informed decisions. Their versatility and adaptability have made them central to advancements in AI.
    \end{block}
    
    \begin{block}{Diagram}
        \textbf{Illustration of a Neural Network}:\\
        - An Input Layer (3 input neurons) \\
        - One or more Hidden Layers (e.g., 2 hidden layers with 4 neurons each) \\
        - An Output Layer (2 output neurons for binary classification)
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Overview}
    \begin{block}{Overview}
        Neural networks are structured in layers, facilitating complex data processing. 
        Understanding these layers is key to grasping how neural networks function and learn from data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Key Components}
    \begin{itemize}
        \item \textbf{Input Layer}
            \begin{itemize}
                \item \textbf{Definition:} The first layer of a neural network; it receives the initial input data.
                \item \textbf{Functionality:} Each neuron corresponds to a feature of the input data.
                \item \textbf{Example:} In image classification, each pixel could be an input neuron.
            \end{itemize}
        
        \item \textbf{Hidden Layers}
            \begin{itemize}
                \item \textbf{Definition:} Layers between the input and output layers; there can be one or more.
                \item \textbf{Functionality:} They perform computations, transforming data to identify patterns.
                \item \textbf{Example:} In handwriting recognition, hidden layers may identify curves and strokes.
            \end{itemize}

        \item \textbf{Output Layer}
            \begin{itemize}
                \item \textbf{Definition:} The last layer that produces the final output.
                \item \textbf{Functionality:} Corresponds to output classes or regression targets.
                \item \textbf{Example:} In binary classification, one output neuron indicates class probability.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Architecture of Neural Networks - Key Points and Formula}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Layer Functionality:} Unique roles for each layer.
            \item \textbf{Flexibility in Design:} Architecture varies by task; complex tasks need deeper networks.
            \item \textbf{Capacity vs. Overfitting:} Deeper networks may risk overfitting; constraints like dropout can help.
        \end{itemize}
    \end{block}
    
    \begin{block}{Formula for Neuron Output}
        For any neuron in a hidden or output layer:
        \begin{equation}
            y = f\left(\sum_{i=1}^{n} w_i \cdot x_i + b\right)
        \end{equation}
        Where:
        \begin{itemize}
            \item $y$: Output of the neuron.
            \item $f$: Activation function.
            \item $w_i$: Weights of the connections.
            \item $x_i$: Inputs to the neuron.
            \item $b$: Bias.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Neurons and Activation Functions}
  \begin{block}{Introduction to Artificial Neurons}
    Artificial neurons are fundamental building blocks of neural networks, designed to mimic the function of biological neurons. They take inputs, apply weights, sum them up, and pass them through an activation function to produce an output.
  \end{block}

  \begin{itemize}
    \item \textbf{Inputs (x):} Features from the data (e.g., pixel values).
    \item \textbf{Weights (w):} Determine the influence of each input on the output.
    \item \textbf{Bias (b):} Adjusts the output independently of the input.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Neuron Equation}
  The output of a neuron can be expressed mathematically as follows:
  \begin{equation} 
  y = \phi(w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n + b) 
  \end{equation}
  Where \( \phi \) represents the activation function, determining if the neuron should be activated.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Common Activation Functions}
  \begin{enumerate}
    \item \textbf{Sigmoid Function}
      \begin{equation} 
      \sigma(x) = \frac{1}{1 + e^{-x}} 
      \end{equation}
      \begin{itemize}
        \item Output range: (0, 1)
        \item Good for binary classification, but can lead to saturation.
      \end{itemize}

    \item \textbf{ReLU (Rectified Linear Unit)}
      \begin{equation} 
      \text{ReLU}(x) = \max(0, x) 
      \end{equation}
      \begin{itemize}
        \item Output range: [0, âˆž)
        \item Introduces non-linearity, but may suffer from the "dying ReLU" issue.
      \end{itemize}
      
    \item \textbf{Tanh (Hyperbolic Tangent)}
      \begin{equation} 
      \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} 
      \end{equation}
      \begin{itemize}
        \item Output range: (-1, 1)
        \item Centered around 0, leading to faster convergence.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary of Key Points}
  \begin{itemize}
    \item \textbf{Artificial Neurons:} Simulate biological neurons with weights, bias, and activation.
    \item \textbf{Activation Functions:} Introduce non-linearity, essential for learning.
    \item \textbf{Choosing Right Function:} Affects efficiency and performance of the neural network.
  \end{itemize}
  
  By understanding these concepts, you enhance your grasp of how neural networks process information and learn from data!
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Overview}
    \begin{block}{What is Forward Propagation?}
        Forward propagation is the process through which inputs to a neural network are transformed into an output. This transformation happens as the data flows through the network layers, involving calculations at each neuron through weights, biases, and activation functions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Process}
    \begin{enumerate}
        \item \textbf{Input Layer:} Data is fed into the input layer, where each node represents a feature.
        \item \textbf{Weights and Biases:} Input nodes connect to the next layer via weighted connections. Weights determine input impact while biases adjust the weighted sum.
        \item \textbf{Weighted Sum Calculation:}
        \begin{equation}
            z = \sum (w_i \cdot x_i) + b
        \end{equation}
        where:
        \begin{itemize}
            \item $z$ is the weighted input
            \item $w_i$ is the weight for the $i$-th input
            \item $x_i$ is the $i$-th input feature
            \item $b$ is the bias of the neuron
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Activation & Output}
    \begin{enumerate}[resume]
        \item \textbf{Activation Function:} The weighted sum $z$ is passed through a non-linear activation function (e.g., Sigmoid, ReLU, Tanh):
        \begin{itemize}
            \item Sigmoid: $\sigma(z) = \frac{1}{1 + e^{-z}}$
            \item ReLU: $f(z) = \max(0, z)$
            \item Tanh: $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
        \end{itemize}
        \item \textbf{Output Layer:} The results from the last hidden layer are sent to the output layer, where a final activation function is applied.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Example}
    Consider a simple neural network with:
    \begin{itemize}
        \item 3 input neurons
        \item 2 hidden neurons (ReLU activation)
        \item 1 output neuron (Sigmoid activation)
    \end{itemize}
    
    \textbf{Input Values:} 
    \begin{itemize}
        $x_1 = 0.5, x_2 = 0.3, x_3 = 0.2$
    \end{itemize}
    
    \textbf{Weights for Hidden Layer:}
    \begin{itemize}
        $w_{11} = 0.4, w_{12} = 0.6, w_{13} = 0.2$ \\
        $w_{21} = 0.5, w_{22} = 0.1, w_{23} = 0.3$
    \end{itemize}
    
    \textbf{Biases:} 
    \begin{itemize}
        $b_1 = 0.1, b_2 = 0.2$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Forward Propagation - Calculations}
    \textbf{Hidden Neuron 1:}
    \begin{equation}
        z_1 = (0.4 \cdot 0.5) + (0.6 \cdot 0.3) + (0.2 \cdot 0.2) + 0.1 = 0.53
    \end{equation}
    \text{Activation: } $\text{ReLU}(z_1) = 0.53$

    \textbf{Hidden Neuron 2:}
    \begin{equation}
        z_2 = (0.5 \cdot 0.5) + (0.1 \cdot 0.3) + (0.3 \cdot 0.2) + 0.2 = 0.36
    \end{equation}
    \text{Activation: } $\text{ReLU}(z_2) = 0.36$

    \textbf{Output Neuron:}
    \begin{equation}
        z_{out} = (0.7 \cdot 0.53) + (0.8 \cdot 0.36) - 0.4 = 0.371
    \end{equation}
    \text{Output Activation: } $\text{Sigmoid}(z_{out}) \approx 0.591$
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Forward propagation is essential for predicting outputs from input features.
        \item It involves computations through weighted sums, biases, and activation functions.
        \item Understanding this process is crucial for grasping how neural networks learn.
    \end{itemize}

    \textbf{Conclusion:} By mastering forward propagation, students are better prepared for concepts like the loss function and learning processes in neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Loss Function and Its Importance}
    \begin{block}{Overview}
        In the context of neural networks, a \textbf{loss function} quantifies how well the model's predictions align with the actual outcomes, providing crucial feedback during training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is a Loss Function?}
    A loss function (or cost function) measures the discrepancy between the predicted outcomes and actual values.
    
    The goal of training a neural network is to \textbf{minimize} this loss:
    \begin{itemize}
        \item Quantifies prediction accuracy
        \item Guides weight adjustments in the model
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Loss Functions}
    \begin{enumerate}
        \item \textbf{Guides Learning:} Provides feedback on prediction accuracy.
        \item \textbf{Influences Performance:} The selection of loss function can impact how effectively the model learns from data.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Types of Loss Functions}
    \begin{itemize}
        \item \textbf{Mean Squared Error (MSE)}: 
        \begin{equation}
            \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
        \end{equation}
        \textit{Used for regression tasks.}
        
        \item \textbf{Binary Cross-Entropy}:
        \begin{equation}
            \text{Binary Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]
        \end{equation}
        \textit{Used for binary classification tasks.}
        
        \item \textbf{Categorical Cross-Entropy}:
        \begin{equation}
            \text{Categorical Cross-Entropy} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
        \end{equation}
        \textit{Used for multi-class classification tasks.}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Critical for the learning process; essential for weight adjustment.
        \item Correct choice of loss function is vital for effective training.
        \item Monitoring loss over iterations provides insight into performance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{block}{}
        The loss function is a fundamental component of neural networks, acting as the bridge between the model outputs and the actual results. The better the model minimizes the loss, the more accurately it makes predictions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Backpropagation: Training Neural Networks}
    Backpropagation is the cornerstone algorithm for training neural networks. It efficiently computes gradients needed for optimization by propagating error signals backward through the layers of the network. This allows us to update the weights and biases of the network, minimizing the loss function.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Backpropagation}
    \begin{itemize}
        \item Backpropagation is crucial for training neural networks.
        \item It computes gradients by propagating error signals backward.
        \item Enables weight and bias updates to minimize the loss function.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Steps of Backpropagation}
    \begin{enumerate}
        \item \textbf{Forward Pass:}
        \begin{itemize}
            \item Input data is passed through the network.
            \item Activations are applied layer by layer.
        \end{itemize}
        
        \item \textbf{Compute Loss:}
        \begin{itemize}
            \item Loss is calculated using a function (e.g., Mean Squared Error):
            \begin{equation}
                L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
            \end{equation}
        \end{itemize}
        
        \item \textbf{Backward Pass:}
        \begin{itemize}
            \item Gradients of the loss are computed using the chain rule.
            \item For each layer, calculate gradients of the loss concerning weights.
        \end{itemize}
        
        \item \textbf{Update Weights:}
        \begin{itemize}
            \item Weights are updated as follows:
            \begin{equation}
                w_i = w_i - \eta \cdot \frac{\partial L}{\partial w_i}
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Simple Neural Network}
    Consider a simple neural network with:
    \begin{itemize}
        \item 1 input layer
        \item 1 hidden layer (1 neuron)
        \item 1 output layer
    \end{itemize}
    
    \textbf{Forward pass:}
    \begin{itemize}
        \item Inputs: \(x = [x_1, x_2]\)
        \item Weights: \(w_{hidden}, w_{output}\)
        \item Output: \(\hat{y}\)
    \end{itemize}
    
    \textbf{Loss Calculation:}
    \begin{itemize}
        \item Calculate loss using MSE.
    \end{itemize}
    
    \textbf{Backward Pass:}
    \begin{itemize}
        \item Compute gradients using chain rule for hidden and output layers.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Efficiency:} Backpropagation is \(O(n)\) in time complexity for \(n\) weights.
        \item \textbf{Chain Rule Usage:} Essential for passing gradients back through layers.
        \item \textbf{Learning Rate:} Selection of \(\eta\) impacts convergence; too high can lead to divergence and too low may slow convergence.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Introduction}
    \begin{block}{Introduction to Optimization in Neural Networks}
        When training neural networks, optimization techniques play a critical role in minimizing the loss function, which quantifies how well the model predicts the target outputs. A better optimization will lead to a more accurate model.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Gradient Descent}
    \begin{block}{1. Gradient Descent}
        \textbf{Definition:} An iterative optimization algorithm used to minimize the loss function by updating the model parameters in the direction of the steepest descent.
        
        \textbf{Process:}
        \begin{itemize}
            \item Initialize weights (parameters) randomly.
            \item Compute the gradient of the loss function with respect to the parameters.
            \item Update parameters using the formula:
            \begin{equation}
                \theta = \theta - \eta \cdot \nabla L(\theta)
            \end{equation}
            where:
            \begin{itemize}
                \item $\theta$: parameters
                \item $\eta$: learning rate (a small positive value)
                \item $\nabla L(\theta)$: gradient of the loss function
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Examples}
    \begin{block}{Example of Gradient Descent}
        Imagine a simple neural network predicting house prices based on size. If the current weights lead to a loss of $50,000, we calculate the gradients and adjust the weights iteratively to find a configuration that reduces loss.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Types of Gradient Descent}
    \begin{block}{2. Types of Gradient Descent}
        \begin{itemize}
            \item \textbf{Batch Gradient Descent:}
                \begin{itemize}
                    \item Uses the whole dataset to compute gradients.
                    \item \textbf{Pros:} Stable convergence.
                    \item \textbf{Cons:} Slow and computationally intensive on large datasets.
                \end{itemize}
            \item \textbf{Stochastic Gradient Descent (SGD):}
                \begin{itemize}
                    \item Uses one random sample to compute gradients.
                    \item \textbf{Pros:} Faster, introduces noise into the optimization which can help escape local minima.
                    \item \textbf{Cons:} Can oscillate and may not converge smoothly.
                \end{itemize}
            \item \textbf{Mini-batch Gradient Descent:}
                \begin{itemize}
                    \item A compromise that uses a small batch of samples.
                    \item \textbf{Pros:} Balanced computation and variance in updates.
                    \item \textbf{Cons:} Requires tuning the batch size.
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Advanced Techniques}
    \begin{block}{3. Advanced Optimization Techniques}
        \begin{itemize}
            \item \textbf{Momentum:}
                \begin{itemize}
                    \item Builds on basic gradient descent by adding a fraction of the previous update to the current update:
                    \begin{equation}
                        v = \beta v + (1-\beta) \nabla L(\theta)
                    \end{equation}
                    \begin{equation}
                        \theta = \theta - \eta v
                    \end{equation}
                \end{itemize}
            \item \textbf{Adaptive Learning Rate Methods:}
                \begin{itemize}
                    \item Techniques like AdaGrad, RMSProp, and Adam adjust the learning rate during training.
                    \item \textbf{Adam Optimizer Example:}
                    \begin{equation}
                        m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L(\theta)
                    \end{equation}
                    \begin{equation}
                        v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L(\theta))^2
                    \end{equation}
                    \begin{equation}
                        \theta = \theta - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
                    \end{equation}
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Optimization Techniques - Key Points}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Choosing the Right Algorithm:} Selection varies based on dataset size, complexity, and hardware capabilities.
            \item \textbf{Learning Rate Matters:} A poorly chosen learning rate can cause slow convergence or divergence.
            \item \textbf{Experimentation is Essential:} Always experiment with different techniques to find the best fit for your specific model and data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    By mastering these optimization techniques, you will enhance the performance of your neural networks and achieve more robust predictions. In the next slide, we will address the challenges of overfitting and underfitting in neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting and Underfitting}
    \begin{block}{Key Concepts}
        In machine learning, particularly in neural networks, \textbf{overfitting} and \textbf{underfitting} are two fundamental problems that can significantly impact model performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting}
    \begin{itemize}
        \item \textbf{Definition:} Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather than the underlying data distribution.
        \item \textbf{Symptoms:} High accuracy on training data but poor performance on validation/test data.
    \end{itemize}
    \begin{block}{Example}
        Consider a neural network trained on a small dataset of images of cats and dogs. If the network memorizes the specific images rather than learning general features (like fur texture, shape, etc.), it will perform poorly on new images of cats and dogs.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Underfitting}
    \begin{itemize}
        \item \textbf{Definition:} Underfitting happens when a model is too simple to capture the underlying trend of the data, leading to poor performance on both training and validation datasets.
        \item \textbf{Symptoms:} Low accuracy on both training and test data.
    \end{itemize}
    \begin{block}{Example}
        Using a linear regression model to predict a complex, non-linear relationship will show underfitting as it fails to capture the variability in the data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustrations}
    \begin{itemize}
        \item \textbf{Graphical Representation:} 
        \begin{itemize}
            \item Overfitting can be illustrated with a graph showing a complex curve fitting the training data closely while missing the general trend of the validation set.
            \item Underfitting would be represented by a straight line that does not meet the peaks and valleys of the actual data.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Solutions to Mitigate Overfitting}
    \begin{enumerate}
        \item \textbf{Simplify the Model:} Reduce complexity by using fewer layers or units in the neural network.
        \item \textbf{Regularization Techniques:} Apply L1 or L2 regularization to add a penalty for larger weights.
        \item \textbf{Cross-Validation:} Use techniques like k-fold cross-validation for better performance evaluation.
        \item \textbf{Dropout:} Temporarily drop units during training to promote redundancy and generalization.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Solutions to Mitigate Underfitting}
    \begin{enumerate}
        \item \textbf{Increase Model Complexity:} Add more layers or nodes to capture complex patterns.
        \item \textbf{Feature Engineering:} Incorporate additional features or transformations.
        \item \textbf{Longer Training:} Extend training time to allow the model to learn better.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{block}{Key Points}
        \begin{itemize}
            \item \textbf{Balance is Key:} Strive to find a balance between overfitting and underfitting.
            \item \textbf{Validation is Crucial:} Continuous validation helps identify issues early.
            \item \textbf{Visual Feedback:} Visualize model performance regularly to adjust strategies.
        \end{itemize}
    \end{block}
    \begin{block}{Summary}
        Understanding and addressing overfitting and underfitting is crucial for robust models. Use regularization, adjust complexity, and select features wisely for well-generalized models.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Next Step}
    In the upcoming slide, we will dive deeper into specific \textbf{Regularization Techniques}, including methods like dropout and L1/L2 regularization, and how they can be leveraged to improve model performance.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Introduction}
    \begin{block}{Introduction to Regularization}
        Regularization helps prevent overfitting in machine learning models by discouraging them from learning noise in the training data. 
        This leads to improved performance on unseen data and enhances generalization.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - 1. Dropout}
    \begin{block}{Definition}
        Dropout randomly omits neurons during training to improve feature robustness.
    \end{block}
    \begin{itemize}
        \item At each iteration, with dropout rate \( p \), a neuron has a \( p\% \) chance of being dropped.
        \item This creates an ensemble of networks with various architectures leading to better generalization.
    \end{itemize}
    \begin{block}{Example}
        With 10 neurons and a dropout rate of 20\%:
        \begin{itemize}
            \item In one iteration, neurons 1, 2, and 3 are active.
            \item Next iteration may activate a different set of neurons.
        \end{itemize}
    \end{block}
    \begin{block}{Key Points}
        \begin{itemize}
            \item Prevents co-adaptation of neurons.
            \item Acts as a model averaging technique which reduces overfitting.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - 2. L1 and L2 Regularization}
    \begin{block}{Definition}
        L1 and L2 are techniques that add penalties to the loss function to discourage complex models.
    \end{block}
    \begin{itemize}
        \item L1 Regularization:
        \begin{equation}
            \text{Loss} = \text{Loss}_{original} + \lambda \sum_{i=1}^{n} |w_i|
        \end{equation}
        \item L2 Regularization:
        \begin{equation}
            \text{Loss} = \text{Loss}_{original} + \lambda \sum_{i=1}^{n} w_i^2
        \end{equation}
    \end{itemize}
    \begin{block}{Effects}
        \begin{itemize}
            \item L1 encourages sparsity, selecting important features by driving some weights to zero.
            \item L2 shrinks all weights gradually, maintaining all features with smaller values.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Summary}
    \begin{block}{Summary}
        \begin{itemize}
            \item Purpose of Regularization: Enhances model generalization by controlling overfitting.
            \item Techniques:
            \begin{itemize}
                \item Dropout: Random neuron removal during training.
                \item L1/L2 Regularization: Penalties added to loss functions based on weights.
            \end{itemize}
            \item Choosing the right technique is essential for good generalization to unseen data.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Regularization Techniques - Next Steps}
    \begin{block}{Next Steps}
        In the upcoming slides, we will explore various types of neural networks and their applications of these regularization techniques within practical scenarios.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Overview}
    \begin{block}{Overview of Neural Networks}
        Neural networks are computational models inspired by the human brain, designed to recognize patterns and learn from data. They consist of interconnected layers of nodes (neurons) that process input information to produce an output.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Types}
    \begin{enumerate}
        \item \textbf{Feedforward Neural Networks (FNN)}
            \begin{itemize}
                \item The simplest type with unidirectional connections.
                \item Composed of input, hidden, and output layers.
                \item Example: Image classification tasks.
            \end{itemize}
        
        \item \textbf{Convolutional Neural Networks (CNN)}
            \begin{itemize}
                \item Specialized for grid data like images.
                \item Components include convolutional and pooling layers.
                \item Example: Image recognition.
            \end{itemize}
        
        \item \textbf{Recurrent Neural Networks (RNN)}
            \begin{itemize}
                \item Designed for sequential data with loops.
                \item Capture context using a hidden state.
                \item Example: Text generation or translation.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Neural Networks - Advanced Types}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Long Short-Term Memory Networks (LSTM)}
            \begin{itemize}
                \item A RNN variant addressing vanishing gradients.
                \item Capable of learning long-term dependencies.
                \item Example: Chatbots and music generation.
            \end{itemize}

        \item \textbf{Generative Adversarial Networks (GAN)}
            \begin{itemize}
                \item Comprises a generator and a discriminator.
                \item These networks compete to enhance performance.
                \item Example: Realistic image generation.
            \end{itemize}
    \end{enumerate}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Differentiated architectures for specific data types.
            \item Understanding strengths aids model selection.
            \item Significant impact on various fields, including computer vision and natural language processing.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Introduction}
  Neural networks, inspired by the human brain's structure and function, have revolutionized various fields by solving complex problems that traditional algorithms struggled with. 
  This presentation explores key real-world applications in multiple domains, showcasing how neural networks enhance processes and contribute to innovative solutions.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Healthcare}
  \begin{itemize}
      \item \textbf{Disease Diagnosis:} Neural networks analyze medical images for early detection of diseases.
      \begin{itemize}
          \item \textbf{Example:} CNNs can diagnose skin cancer with a diagnostic accuracy as high as trained dermatologists.
      \end{itemize}
      
      \item \textbf{Drug Discovery:} They accelerate the process of drug development by predicting molecule interactions.
      \begin{itemize}
          \item \textbf{Example:} Neural networks help screen millions of compounds to find candidates for testing, reducing time and cost.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Finance and Social Media}
  \begin{itemize}
      \item \textbf{Finance:}
      \begin{itemize}
          \item \textbf{Fraud Detection:} Identify unusual patterns indicating fraudulent activity.
          \begin{itemize}
              \item \textbf{Example:} Credit card companies use RNNs to analyze transaction sequences and flag suspicious behavior in real-time.
          \end{itemize}
          
          \item \textbf{Algorithmic Trading:} Predict stock price movements using historical data.
          \begin{itemize}
              \item \textbf{Example:} Neural networks analyze massive historical market data to predict future price trends.
          \end{itemize}
      \end{itemize}

      \item \textbf{Social Media:}
      \begin{itemize}
          \item \textbf{Content Recommendation:} Power recommendation engines for personalized user experiences.
          \begin{itemize}
              \item \textbf{Example:} Platforms like Facebook and Instagram utilize deep learning for recommending posts and ads.
          \end{itemize}

          \item \textbf{Sentiment Analysis:} Analyze social media posts to gauge public sentiment.
          \begin{itemize}
              \item \textbf{Example:} Neural networks classify emotions expressed in tweets, providing insights into public opinion during elections or product launches.
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Applications of Neural Networks - Key Points and Conclusion}
  \begin{block}{Key Points to Emphasize}
      \begin{itemize}
          \item Neural networks excel in recognizing patterns and making predictions across diverse data types.
          \item Their ability to learn from large datasets makes them invaluable in handling complex problems.
          \item As technology advances, neural networks continue to evolve, leading to more sophisticated applications.
      \end{itemize}
  \end{block}

  \begin{block}{Conclusion}
      The versatility and power of neural networks extend into many critical sectors, showcasing their potential to drive innovation. 
      Understanding these applications is vital for grasping the impact of artificial intelligence in our world today.
  \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Network Training - Overview}
    Training neural networks can be complex due to various challenges that impact model development. Understanding these challenges is crucial for the effective use of neural networks.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Data Requirements}
    \begin{block}{Key Challenges}
        \begin{itemize}
            \item \textbf{Quantity of Data}:
            Neural networks, particularly deep learning models, require large amounts of labeled data.
            \begin{itemize}
                \item Example: ImageNet contains over 14 million images across 20,000 categories.
            \end{itemize}
            
            \item \textbf{Quality of Data}:
            Data must be representative and clean to avoid overfitting and misclassification.
            \begin{itemize}
                \item Example: In spam detection, the training data should accurately reflect typical emails.
            \end{itemize}
            
            \item \textbf{Data Imbalance}:
            Often, classes in real-world datasets can be imbalanced.
            \begin{itemize}
                \item Example: Fraud detection may have far fewer fraudulent transactions compared to legitimate ones.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges in Neural Networks - Computational Power}
    \begin{block}{Key Challenges}
        \begin{itemize}
            \item \textbf{Infrastructure Needs}:
            Training deep networks demands significant computational resources (GPUs/TPUs).
            
            \item \textbf{Scalability}:
            As models and datasets grow, the computational demand increases, leading to longer training times.
            \begin{itemize}
                \item Example: Training GPT-3 required thousands of petaflop/s-days of computation.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Mitigation Strategies}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Data Dependency}:
            Performance heavily relies on the quality and quantity of training data.
            \item \textbf{Resource Intensity}:
            Training is resource-intensive, requiring specialized hardware and extended periods.
        \end{itemize}
    \end{block}
    
    \begin{block}{Mitigation Strategies}
        Techniques such as:
        \begin{itemize}
            \item Data augmentation
            \item Transfer learning
            \item Using pre-trained models
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Conclusion}
    Being aware of challenges in data requirements and computational power is essential for successful neural network training. The application of techniques like data enhancement or pre-trained models can significantly improve outcomes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications of Neural Networks - Introduction}
    \begin{block}{Overview}
        Neural networks have revolutionized fields such as healthcare, finance, and entertainment. 
        However, their deployment raises critical ethical considerations necessary for responsible and fair use.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications of Neural Networks - Key Ethical Considerations}
    \begin{enumerate}
        \item \textbf{Algorithmic Bias}  
        \begin{itemize}
            \item \textit{Definition}: Bias in results due to flawed training data or assumptions.
            \item \textit{Example}: A facial recognition system favoring light-skinned individuals; poor performance on darker skin tones.
            \item \textit{Impact}: Perpetuates stereotypes and social inequalities.
        \end{itemize}

        \item \textbf{Data Privacy}  
        \begin{itemize}
            \item \textit{Definition}: Concerns with personal information processed without consent or protection.
            \item \textit{Example}: Health data must comply with HIPAA regulations; breaches can lead to loss of trust.
            \item \textit{Impact}: Strong data protection and user consent are vital for privacy.
        \end{itemize}
        
        \item \textbf{Transparency and Accountability}  
        \begin{itemize}
            \item \textit{Definition}: Many neural networks are "black boxes," obscuring decision-making processes.
            \item \textit{Example}: Investigating reasons behind wrongful loan denials is crucial for fairness.
            \item \textit{Impact}: Strive for Explainable AI (XAI) to enhance transparency and accountability.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Implications of Neural Networks - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Diverse Training Data}: Use diverse datasets to minimize bias and ensure fairness.
            \item \textbf{Robust Privacy Measures}: Implement strict data protection regulations and practices.
            \item \textbf{Explainability}: Make AI decisions interpretable and transparent to build trust.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        As neural networks influence society, it is critical to address their ethical implications. Prioritizing fairness, transparency, and privacy fosters a responsible approach to this technology.
    \end{block}

    \begin{block}{Discussion Questions}
        \begin{itemize}
            \item How can we measure bias in neural networks?
            \item What strategies can enhance data privacy?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks}
    \begin{block}{Overview}
        Neural networks are evolving rapidly, driven by advancements in technology, increased computational power, and massive datasets. The future promises to enhance the capabilities of machine learning across various fields.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Part 1}
    \begin{enumerate}
        \item \textbf{Autonomous Learning and Self-Training Networks}
            \begin{itemize}
                \item \textbf{Explanation}: Future neural networks will learn from interactions with their environment without needing extensive labeled data.
                \item \textbf{Example}: Robots navigating through trial and error, adjusting strategies based on real experiences.
            \end{itemize}
        
        \item \textbf{Explainable AI (XAI)}
            \begin{itemize}
                \item \textbf{Explanation}: Increased need for models that allow users to understand decision-making processes.
                \item \textbf{Example}: A neural network diagnosing diseases with insights into influencing features of medical images.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Trends in Neural Networks - Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Hybrid Neural Networks}
            \begin{itemize}
                \item \textbf{Explanation}: Combining different types of networks to optimize performance across tasks.
                \item \textbf{Example}: An autonomous vehicle leveraging convolutional networks for image processing and recurrent networks for predicting driver behavior.
            \end{itemize}

        \item \textbf{Efficient Neural Networks}
            \begin{itemize}
                \item \textbf{Explanation}: Focus on smaller networks with fewer parameters achieving competitive performance using techniques such as pruning and distillation.
                \item \textbf{Example}: Mobile devices performing complex tasks like image recognition without draining battery life.
            \end{itemize}
            
        \item \textbf{Integration with Quantum Computing}
            \begin{itemize}
                \item \textbf{Explanation}: Quantum neural networks leveraging quantum mechanics for faster computations.
                \item \textbf{Example}: Improvements in optimization and pattern recognition in fields like transportation and finance.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Summary}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Shift toward self-training reduces dependency on labeled data.
            \item Growing need for explainability to build trust in AI systems.
            \item Hybrid architectures enable more robust applications.
            \item Efficiency in model design is vital amidst resource constraints.
            \item Quantum computing may accelerate developments in machine learning.
        \end{itemize}
    \end{block}

    \begin{block}{Summary}
        The future of neural networks is innovative and transformative, promising advancements in understanding, efficiency, and computational capability, impacting sectors from healthcare to finance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 1}
  \begin{block}{Overview of Neural Networks}
    Neural networks are computational models inspired by the human brain that are used to recognize patterns and solve complex problems in various fields such as machine learning, artificial intelligence, and data science.
  \end{block}

  \begin{block}{Key Points to Recap}
    \begin{enumerate}
      \item Structure of Neural Networks
      \item Training Process
      \item Applications of Neural Networks
      \item Future Trends
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 2}
  \begin{block}{Structure of Neural Networks}
    \begin{itemize}
      \item \textbf{Neurons and Layers}: Interconnected nodes (neurons) arranged in layers: input, hidden, and output.
      \item \textbf{Activation Functions}: Functions such as ReLU, Sigmoid, and Tanh determine the output of neurons, introducing non-linearity.
    \end{itemize}
    
    \textbf{Example:} In a house price prediction model, input features could include the number of rooms, location, and square footage; the output would be the predicted price.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 3}
  \begin{block}{Training Process}
    \begin{itemize}
      \item \textbf{Backpropagation}: Method to update weights via gradient descent by minimizing the difference between predicted and actual outputs.
      \item \textbf{Loss Functions}: Quantify how well predictions align with actual outputs (e.g., Mean Squared Error for regression).
    \end{itemize}
    
    \begin{equation}
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    \end{equation}
    where \(y\) is the actual output, \(\hat{y}\) is the predicted output, and \(n\) is the number of samples.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 4}
  \begin{block}{Applications of Neural Networks}
    \begin{itemize}
      \item \textbf{Image and Speech Recognition}: Effective in unstructured data fields like computer vision and natural language processing.
      \item \textbf{Generative Models}: Architectures like Generative Adversarial Networks (GANs) create new data instances similar to the training dataset.
    \end{itemize}
    
    \textbf{Example:} Image classification tasks (e.g., identifying cats vs. dogs) are efficiently handled by convolutional neural networks (CNNs).
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Summary and Key Takeaways - Part 5}
  \begin{block}{Future Trends}
    \begin{itemize}
      \item \textbf{Transfer Learning}: Utilizing pre-trained models for improved efficiency on new tasks.
      \item \textbf{Explainable AI}: Development of models providing transparency in their decision-making processes.
    \end{itemize}
    
    \textbf{Key Point:} Emphasizing the importance of ethical AI is crucial as neural networks increasingly integrate into decision-making frameworks across industries.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion}
  Neural networks have revolutionized how we approach problem-solving in technology. Understanding their structure, training methods, and applications is vital. As advancements continue to reshape the landscape, staying informed on new trends will help leverage these powerful tools effectively.
\end{frame}


\end{document}