\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Code Listing Style
\lstdefinestyle{customcode}{
  backgroundcolor=\color{mycodebackground},
  basicstyle=\footnotesize\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  commentstyle=\color{mygreen}\itshape,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{myorange},
  numbers=left,
  numbersep=8pt,
  numberstyle=\tiny\color{mygray},
  frame=single,
  framesep=5pt,
  rulecolor=\color{mygray},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  captionpos=b
}
\lstset{style=customcode}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\source}[1]{\vspace{0.2cm}\hfill{\tiny\textcolor{mygray}{Source: #1}}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Footer and Navigation Setup
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.3\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.2\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
    \usebeamerfont{date in head/foot}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

% Turn off navigation symbols
\setbeamertemplate{navigation symbols}{}

% Title Page Information
\title[Decision Making under Uncertainty]{Week 10: Decision Making under Uncertainty: MDPs}
\author[J. Smith]{John Smith, Ph.D.}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Decision Making under Uncertainty}
    
    \begin{block}{Definition}
        Decision-making under uncertainty refers to the process of making choices when outcomes are not guaranteed and may be influenced by unpredictable variables. 
    \end{block}
    
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Uncertainty}: A situation where not all information is available, leading to various potential outcomes.
            \item \textbf{Risk} vs. \textbf{Uncertainty}:
            \begin{itemize}
                \item \textbf{Risk}: Outcomes are known with their probabilities (e.g., gambling).
                \item \textbf{Uncertainty}: Outcomes are unknown (e.g., weather forecasts).
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Decision Making Under Uncertainty}
    
    \begin{enumerate}
        \item \textbf{Real-World Complexity}: Many scenarios (e.g., stock market, weather) involve multiple uncertain elements affecting outcomes.
        \item \textbf{Optimizing Outcomes}: Informed decisions can maximize potential gains or minimize losses in uncertain situations.
        \item \textbf{Resource Management}: Efficiently allocating resources in dynamic environments can lead to better decision outcomes across industries.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Introduction to Markov Decision Processes (MDPs)}
    
    Markov Decision Processes provide a formal framework for modeling decision-making in uncertain environments.

    \begin{block}{Components of MDPs}
        \begin{itemize}
            \item \textbf{States (S)}: Different situations in which decisions can be made.
            \item \textbf{Actions (A)}: Choices available to the decision-maker at any state.
            \item \textbf{Transition Probabilities (P)}: Likelihood of moving from one state to another given a specific action.
            \item \textbf{Rewards (R)}: Immediate benefit received after taking an action in a specific state.
        \end{itemize}
    \end{block}

    \begin{block}{Example: Robot Navigating a Maze}
        \begin{itemize}
            \item \textbf{States}: Different positions within the maze.
            \item \textbf{Actions}: Move up, down, left, or right.
            \item \textbf{Transition}: Moving to a new state may succeed or fail based on the environment.
            \item \textbf{Rewards}: Reaching the exit may yield positive rewards, whereas hitting walls leads to negative rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{What are Markov Decision Processes?}
    \begin{block}{Definition}
        A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in uncertain situations where outcomes depend on both the current state and the actions taken. MDPs form the basis for developing algorithms that create optimal strategies (policies) to maximize expected rewards in uncertain environments.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Key Components of MDPs}
    \begin{enumerate}
        \item \textbf{States (S)}:
            \begin{itemize}
                \item Definition: The conditions or situations in which an agent can find itself.
                \item Example: In a grid-world navigation task, each cell represents a state (e.g., "A1", "A2", ... "B1").
            \end{itemize}
        
        \item \textbf{Actions (A)}:
            \begin{itemize}
                \item Definition: The set of all possible moves or decisions an agent can make while in a particular state.
                \item Example: From state "A1," the agent may move up, down, left, or right.
            \end{itemize}
        
        \item \textbf{Rewards (R)}:
            \begin{itemize}
                \item Definition: A scalar value received after transitioning from one state to another via an action, reflecting the immediate benefit.
                \item Example: Moving to a goal state could yield +10, while hitting an obstacle might result in -5.
            \end{itemize}
        
        \item \textbf{Transition Probabilities (P)}:
            \begin{itemize}
                \item Definition: The probability of moving from one state to another given a specific action, capturing environmental uncertainty.
                \item Example: If an agent in "A1" moves right to "A2," it might have a 70% chance of success and a 30% chance of moving to "B1."
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Key Points and Relevant Formula}
    \begin{block}{Key Points}
        \begin{itemize}
            \item MDPs formalize decision-making under uncertainty, differing from deterministic approaches due to their probabilistic focus.
            \item Each component interacts to define MDP dynamics.
            \item Understanding MDPs is crucial for reinforcement learning algorithms, where agents learn to optimize actions based on feedback.
        \end{itemize}
    \end{block}

    \begin{equation}
        V(s) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V(s')
    \end{equation}
    \begin{itemize}
        \item Where:
        \begin{itemize}
            \item $V(s)$ = expected value of the state,
            \item $R(s, a)$ = reward for action $a$ in state $s$,
            \item $\gamma$ = discount factor ($0 \leq \gamma < 1$),
            \item $P(s' \mid s, a)$ = probability of reaching state $s'$ from state $s$ by taking action $a$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Overview}
    Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the control of a decision-maker. The main components of an MDP are:
    \begin{enumerate}
        \item State (S)
        \item Action (A)
        \item Reward (R)
        \item Policy ($\pi$)
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - State, Action, Reward}
    \begin{itemize}
        \item \textbf{State (S)}
        \begin{itemize}
            \item \textbf{Definition:} A 'state' represents a snapshot of the environment at a given time.
            \item \textbf{Example:} Different locations on a map with specific attributes (e.g., traffic conditions).
            \item \textbf{Key Point:} States must satisfy the Markov property.
        \end{itemize}

        \item \textbf{Action (A)}
        \begin{itemize}
            \item \textbf{Definition:} An 'action' is a decision made by the agent that affects the environment.
            \item \textbf{Example:} Possible actions could include 'move forward' or 'pick up an object'.
            \item \textbf{Key Point:} The allowable actions may vary across states.
        \end{itemize}

        \item \textbf{Reward (R)}
        \begin{itemize}
            \item \textbf{Definition:} A feedback signal received after taking an action in a state.
            \item \textbf{Example:} Collecting a coin yields +10 points; running into an obstacle gives -5 points.
            \item \textbf{Key Point:} The reward function defines the objective of the decision-making process.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Components of MDPs - Policy and Summary}
    \begin{itemize}
        \item \textbf{Policy ($\pi$)}
        \begin{itemize}
            \item \textbf{Definition:} A strategy that maps states to actions.
            \item \textbf{Example:} In a chess game, a policy might dictate a move based on the opponent's action.
            \item \textbf{Key Point:} The policy guides the agent's actions to maximize rewards.
        \end{itemize}
    \end{itemize}

    \begin{block}{Summary of MDP Components}
        \begin{itemize}
            \item \textbf{State (S):} Snapshot of the environment.
            \item \textbf{Action (A):} Choices available to the agent.
            \item \textbf{Reward (R):} Feedback post-action influencing future decisions.
            \item \textbf{Policy ($\pi$):} Strategy for taking actions in states.
        \end{itemize}
    \end{block}

    \begin{block}{Formulation of a Simple MDP}
        An MDP can be defined as a tuple (S, A, R, P):
        \begin{itemize}
            \item \textbf{S:} Finite set of states.
            \item \textbf{A:} Finite set of actions.
            \item \textbf{R:} Reward function - R(s, a) gives the reward for taking action a in state s.
            \item \textbf{P:} Transition function - P(s'|s, a) gives the probability of moving to state s' given the current state s and action a.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States and Actions - Introduction}
    \begin{block}{Overview}
        This section discusses the crucial concepts of states and actions in Markov Decision Processes (MDPs) and their impact on decision-making.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States and Actions - Concepts}
    \begin{itemize}
        \item \textbf{States (S):}
        \begin{itemize}
            \item A specific situation or configuration in an MDP.
            \item Can be finite (limited) or infinite (unbounded).
        \end{itemize}

        \item \textbf{Actions (A):}
        \begin{itemize}
            \item Decisions or moves an agent can take in a certain state.
            \item Impact the transition to the next state.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding States and Actions - Examples}
    \begin{block}{Examples of States}
        \begin{enumerate}
            \item Chess: Unique arrangement of pieces on the board.
            \item Self-driving car: Position, speed, obstacles, and traffic signals.
        \end{enumerate}
    \end{block}

    \begin{block}{Examples of Actions}
        \begin{enumerate}
            \item Chess: Moving a piece or capturing an opponent's piece.
            \item Self-driving car: Accelerating, braking, or turning.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision-Making in MDPs}
    \begin{itemize}
        \item Decision-making relies on balancing exploration and exploitation to maximize long-term rewards.
        \item The selected action from a state determines transitions to the next state, which are probabilistic.
    \end{itemize}

    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Transition Dynamics:} Probabilities affect success, e.g., 'moving north' in a maze might succeed 80\% of the time.
            \item \textbf{Policy (\(\pi\))}: Defines agent behavior; can be deterministic or stochastic.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Mathematical Representation}
    \begin{block}{Definitions}
        Let:
        \begin{itemize}
            \item \( S \) = Set of all states
            \item \( A \) = Set of all actions
            \item \( P(s' | s, a) \) = Probability of transitioning to state \( s' \) from state \( s \) given action \( a \)
        \end{itemize}
    \end{block}
    
    \begin{equation}
        P: S \times A \to S
    \end{equation}

    Using this framework, agents can optimize their actions based on expected rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary}
    \begin{itemize}
        \item **States** represent the current situation, while **actions** are the potential moves.
        \item The interaction between states and actions is vital for decision-making in MDPs.
        \item Understanding these core concepts sets the stage for analyzing rewards in decision-making.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Introduction}
    In Markov Decision Processes (MDPs), rewards play a crucial role in guiding decision-making. They are numerical values assigned to actions taken in specific states and reflect the immediate benefit of those actions. 
    \begin{itemize}
        \item Understanding how rewards function helps us analyze and optimize decision strategies in uncertain environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Key Concepts}
    \begin{enumerate}
        \item \textbf{Definition of Rewards:}
        \begin{itemize}
            \item Rewards are scalar values received after taking an action in a given state (e.g., \( R(s, a) \)).
        \end{itemize}
        
        \item \textbf{Purpose of Rewards:}
        \begin{itemize}
            \item Influence Behavior: Rewards shape an agent's behavior by incentivizing certain actions over others.
            \item Guiding Learning: Agents learn to maximize cumulative rewards through exploration and exploitation.
        \end{itemize}
        
        \item \textbf{Types of Rewards:}
        \begin{itemize}
            \item Immediate Reward (R): Awarded after taking an action in a state.
            \item Cumulative Reward: Maximizing the total reward over time, referred to as the return \( G \).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Example and Cumulative Reward}
    \textbf{Illustrative Example:}
    Imagine an agent in a simple grid world, navigating from the starting point to a goal point.
    \begin{itemize}
        \item \textbf{States (S):} Each position on the grid represents a state.
        \item \textbf{Actions (A):} The agent can move up, down, left, or right.
        \item Rewards: 
        \begin{itemize}
            \item +10 reward for reaching the goal state,
            \item -1 reward for each step taken.
        \end{itemize}
    \end{itemize}
    
    \textbf{Cumulative Reward Calculation:}
    \begin{equation}
    G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
    \end{equation}
    Where:
    \begin{itemize}
        \item \( t \) is the current time step,
        \item \( R_t \) is the immediate reward at time \( t \),
        \item \( \gamma \) (0 â‰¤ \( \gamma \) < 1) is the discount factor.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Rewards in MDPs - Importance and Conclusion}
    \textbf{Importance of Discount Factor \( \gamma \):}
    \begin{itemize}
        \item Values future rewards: A higher \( \gamma \) makes future rewards more valuable.
        \item Forces timely decision-making: A lower \( \gamma \) leads to a focus on immediate rewards.
    \end{itemize}

    \textbf{Key Points to Emphasize:}
    \begin{itemize}
        \item Rewards are fundamental in shaping optimal decisions in MDPs.
        \item The design of the reward function significantly impacts learned policies.
        \item Understanding immediate vs. cumulative rewards aids in crafting strategies.
    \end{itemize}

    \textbf{Conclusion:}
    Rewards are critical components of MDPs, driving agent behavior and decision-making. A well-structured reward system encourages desired behaviors and affects performance in uncertain situations.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Transition Dynamics}
    Discussion on the probabilities associated with state transitions after taking an action.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Transition Dynamics}
    \begin{itemize}
        \item In Markov Decision Processes (MDPs), transition dynamics represent the probabilistic nature of moving from one state to another after performing a specific action.
        \item Understanding how choices impact future states is crucial in an uncertain environment.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    \begin{enumerate}
        \item \textbf{States (S)}:
            \begin{itemize}
                \item Various situations or configurations in which an agent can find itself.
                \item Example: In a grid world, each cell is a state.
            \end{itemize}
        \item \textbf{Actions (A)}:
            \begin{itemize}
                \item Choices available to an agent at a given state.
                \item Examples: Moving north, south, east, or west in a grid.
            \end{itemize}
        \item \textbf{Transition Probability (P)}:
            \begin{itemize}
                \item Denoted as $P(s' | s, a)$, representing the probability of moving to state $s'$ from state $s$ after taking action $a$.
                \item Indicates uncertainty in action outcomes.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Probabilistic Transition Dynamics}
    \begin{itemize}
        \item Taking an action in MDPs does not lead to a single new state; instead, it results in various possible states with associated probabilities.
    \end{itemize}
    
    \textbf{Example:} An agent in state $S1$ (e.g., at position (2,2)) decides to move right (Action $A1$):
    \begin{itemize}
        \item $P(S2 | S1, A1) = 0.7$ (successful move to (2,3))
        \item $P(S3 | S1, A1) = 0.2$ (slips back to (2,2))
        \item $P(S4 | S1, A1) = 0.1$ (mistakenly moves to (1,3))
    \end{itemize}
    Here, $S2$, $S3$, and $S4$ represent the new possible states.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item \textbf{Dynamic Programming:}
            \begin{itemize}
                \item Transition dynamics are crucial for predictive algorithms in dynamic programming.
            \end{itemize}
        \item \textbf{Markov Property:}
            \begin{itemize}
                \item Transition probabilities depend only on the current state and chosen action, independent of the sequence of prior events.
            \end{itemize}
        \item \textbf{Expected Reward Calculation:}
            \begin{itemize}
                \item Transition probabilities enable the calculation of expected rewards critical for evaluating policies in MDPs.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Formulas}
    \textbf{Transition Probability Formula:}
    \begin{equation}
    P(s' | s, a) = \text{Probability of transitioning to state } s' \text{ from state } s \text{ after action } a
    \end{equation}
    
    \textbf{Expected Immediate Reward:}
    \begin{equation}
    R(s, a) = \sum_{s'} P(s' | s, a) \times R(s, a, s')
    \end{equation}
    Here, $R(s, a, s')$ is the reward received for transitioning to state $s'$.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Next Steps}
    \begin{itemize}
        \item Understanding transition dynamics in MDPs equips agents to navigate uncertainty and make informed decisions.
        \item It forms a foundational component for evaluating policies and maximizing long-term rewards.
    \end{itemize}
    
    \textbf{Next Steps:} In the upcoming slide, we will explore \textbf{Policies in MDPs}, discussing how agents' behavior is shaped by transition dynamics and expected rewards.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in MDPs - Definition}
    \begin{block}{Definition of Policies}
        In the context of Markov Decision Processes (MDPs), a \textit{policy} defines the behavior of an agent by providing a strategy for making decisions. Formally, a policy, denoted as \( \pi \), is a mapping from states to actions.
    \end{block}
    
    \begin{itemize}
        \item \textbf{Deterministic Policy}: Specifies exactly one action in a given state.
            \begin{itemize}
                \item Example: If the agent is in state \( s \), then \( \pi(s) = a \) (e.g., Move left).
            \end{itemize}
        \item \textbf{Stochastic Policy}: Specifies a probability distribution over actions in a given state.
            \begin{itemize}
                \item Example: If the agent is in state \( s \), then \( \pi(a|s) \) represents probabilities (e.g., Move left with 70%, Move right with 30%).
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in MDPs - Role in Decision Making}
    \begin{block}{Role of Policies}
        Policies are crucial in MDPs as they guide the agent's actions in different states, effectively shaping the decision-making process:
    \end{block}
    
    \begin{enumerate}
        \item \textbf{Behavior Determination}: The choice of policy affects how the agent interacts with its environment and can be adapted based on learning or experience.
        
        \item \textbf{Goal Achievement}: The aim of MDPs is to find an optimal policy, \( \pi^* \), that maximizes cumulative rewards over time.
        
        \item \textbf{State Action Value}: The impact of a policy can be evaluated using the state-action value function \( Q(s, a) \).
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policies in MDPs - Example and Summary}
    \begin{block}{Example}
        Consider a simple grid environment where an agent can move in four directions:
        
        If the agent is at position \( (2,2) \):
        \begin{itemize}
            \item A \textbf{deterministic policy} might say: \( \pi((2,2)) = \text{Up} \).
            \item A \textbf{stochastic policy} might say: \( \pi(\text{Up}|(2,2)) = 0.8 \) and \( \pi(\text{Down}|(2,2)) = 0.2 \).
        \end{itemize}
    \end{block}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Policies are central to how agents make choices in MDPs.
            \item Finding an optimal policy involves assessing the long-term impacts of actions through rewards.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Goal of MDPs}
    MDPs aim to maximize cumulative rewards and establish long-term decision-making strategies.
    
    \begin{itemize}
        \item Mathematical framework for modeling decision-making scenarios.
        \item Characterized by states, actions, rewards, and a transition model.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Maximizing Cumulative Rewards}
    The primary objective of MDPs is to maximize cumulative rewards over time, known as the "return."

    \begin{block}{Cumulative Reward}
        \[
        R = \sum_{t=0}^{\infty} \gamma^t r_t
        \]
    \end{block}
    
    Where:
    \begin{itemize}
        \item \( R \) = cumulative reward
        \item \( r_t \) = reward received at time \( t \)
        \item \( \gamma \) = discount factor (0 \(\leq \gamma < 1\)), representing the value of future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Long-Term Decision-Making Strategies}
    MDPs emphasize long-term strategies rather than short-term gains.

    \begin{itemize}
        \item **Discount Factor (\( \gamma \))**: Determines the importance of future rewards.
        \item **Policies**: Specify actions to take in each state, aiming to find optimal policies for maximum expected cumulative reward.
        \item **Exploration vs. Exploitation**: Balance between exploring new actions for better rewards and exploiting known high-reward actions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Scenario: Navigation Problem}
    Consider a robot navigating a grid:

    \begin{itemize}
        \item **States**: Each cell in the grid represents a state.
        \item **Actions**: Move in four directions (up, down, left, right).
        \item **Rewards**: Specific cells yield different rewards (e.g., reaching a goal gives a positive reward, hitting a wall incurs a negative reward).
    \end{itemize}

    The goal is to determine a policy that maximizes total reward through careful evaluation of paths.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Remember}
    \begin{itemize}
        \item MDPs focus on maximizing cumulative rewards through strategic decision-making.
        \item Decisions today can impact future outcomes, emphasizing the importance of the discount factor.
        \item Identifying and executing the optimal policy allows agents to effectively navigate complex environments under uncertainty.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    The goal of MDPs is to equip decision-makers with frameworks for maximizing cumulative rewards over time through strategic planning.
    
    Next, we will explore methods for solving MDPs using techniques such as value iteration and policy iteration.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Solving MDPs}
    \begin{block}{Overview of Methods}
        Markov Decision Processes (MDPs) are powerful models for decision-making in uncertain environments. Solving MDPs involves finding an optimal policy that maximizes cumulative rewards over time.
    \end{block}
    \begin{itemize}
        \item \textbf{Value Iteration}
        \item \textbf{Policy Iteration}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration}
    \begin{block}{Concept}
        An iterative algorithm that updates the value of each state until convergence to the optimal state value, using the Bellman equation.
    \end{block}
    \begin{enumerate}
        \item \textbf{Initialization}: Start with arbitrary values for all states (commonly zeros).
        \item \textbf{Bellman Update}:
        \begin{equation}
            V_{new}(s) = \max_{a} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V(s')]
        \end{equation}
        \item \textbf{Convergence Check}: Repeat until changes in value are below a predefined threshold.
    \end{enumerate}
    \begin{block}{Example}
        Consider states on a grid; the Bellman update calculates values for moving in each direction, factoring in rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration}
    \begin{block}{Concept}
        Involves two main steps: policy evaluation and policy improvement to reach an optimal policy.
    \end{block}
    \begin{enumerate}
        \item \textbf{Initialization}: Start with an arbitrary policy $\pi$.
        \item \textbf{Policy Evaluation}:
        \begin{equation}
            V^\pi(s) = \sum_{s'} P(s'|s, \pi(s)) [R(s, \pi(s), s') + \gamma V^\pi(s')]
        \end{equation}
        \item \textbf{Policy Improvement}:
        \begin{equation}
            \pi_{new}(s) = \arg\max_{a} \sum_{s'} P(s'|s, a) [R(s, a, s') + \gamma V^\pi(s')]
        \end{equation}
        \item \textbf{Check for Convergence}: Repeat until the policy stabilizes.
    \end{enumerate}
    \begin{block}{Example}
        A naive starting policy for a grid world, where evaluation computes expected returns and improvement refines actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration}
    \begin{block}{What is Value Iteration?}
        Value Iteration is an algorithm used in Markov Decision Processes (MDPs) to find the optimal policy that maximizes the expected cumulative reward. It systematically updates the value of each state until convergence is achieved.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts of MDP}
    \begin{itemize}
        \item \textbf{MDP Components:}
        \begin{itemize}
            \item \textbf{States (S)}: Possible conditions in the environment.
            \item \textbf{Actions (A)}: Choices available to the agent.
            \item \textbf{Transition Probabilities (P)}: Probabilities of reaching the next state given current state and action.
            \item \textbf{Rewards (R)}: Immediate returns after transitioning from one state to another.
            \item \textbf{Discount Factor ($\gamma$)}: Value between 0 and 1 prioritizing immediate rewards over future rewards.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value Iteration Steps}
    \begin{enumerate}
        \item \textbf{Initialization}:
        \begin{itemize}
            \item Start with an arbitrary value function $V(s)$ for all states $s \in S$, usually initialized to zero.
        \end{itemize}

        \item \textbf{Value Update}:
        \begin{equation}
            V_{new}(s) = R(s) + \gamma \sum_{s'} P(s' | s, a) V(s')
        \end{equation}

        \item \textbf{Policy Extraction} (optional):
        \begin{equation}
            \pi^*(s) = \underset{a \in A}{\operatorname{argmax}} \left( \sum_{s'} P(s' | s, a) \left( R(s, a) + \gamma V(s') \right) \right)
        \end{equation}

        \item \textbf{Convergence Check}:
        \begin{equation}
            \| V_{new} - V \| < \epsilon
        \end{equation}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example of Value Iteration}
    Consider a simplified MDP with states $S = \{s_1, s_2\}$ and action $A = \{a\}$:
    \begin{itemize}
        \item Rewards: $R(s_1) = 5, R(s_2) = 10$
        \item Transition Probabilities:
        \begin{itemize}
            \item $P(s_1 | s_1, a) = 0.7, P(s_2 | s_1, a) = 0.3$
            \item $P(s_1 | s_2, a) = 0.4, P(s_2 | s_2, a) = 0.6$
        \end{itemize}
        \item Assume $\gamma = 0.9$
    \end{itemize}

    \textbf{Initialization:} $V(s_1) = 0, V(s_2) = 0$

    \textbf{First Update:}
    \begin{align*}
        V_{new}(s_1) &= 5 + 0.9(0.7 \cdot 0 + 0.3 \cdot 0) = 5 \\
        V_{new}(s_2) &= 10 + 0.9(0.4 \cdot 0 + 0.6 \cdot 0) = 10
    \end{align*}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Value Iteration converges to the optimal value function and policy.
        \item Each iteration smooths out the value estimates leading to the final policy.
        \item Understand the trade-off between immediate and future rewards using the discount factor.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Value Iteration}
    \begin{lstlisting}[language=Python]
def value_iteration(MDP, gamma, epsilon):
    V = {s: 0 for s in MDP.states}
    while True:
        delta = 0
        for s in MDP.states:
            v = V[s]
            V[s] = max(sum(MDP.transitions[s, a, s_prime] * (MDP.rewards[s] + gamma * V[s_prime])
                           for s_prime in MDP.states)
                           for a in MDP.actions)
            delta = max(delta, abs(v - V[s]))
        if delta < epsilon:
            break
    return V
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Overview}
    \begin{block}{What is Policy Iteration?}
        Policy Iteration is a dynamic programming algorithm used for finding the optimal policy in Markov Decision Processes (MDPs). 
        It operates by iteratively evaluating and improving a policy until convergence to the optimal policy.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Key Concepts}
    \begin{itemize}
        \item \textbf{Policy ($\pi$)}: A mapping from states to actions, defining the agent's behavior.
        \item \textbf{Value Function ($V$)}: For a given policy, the value function provides the expected return (cumulative reward) from each state under that policy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Steps}
    \begin{enumerate}
        \item \textbf{Initialization}:
            \begin{itemize}
                \item Start with an arbitrary policy ($\pi$) for each state.
                \item Initialize $V$ to arbitrary values (often zero).
            \end{itemize}
            
        \item \textbf{Policy Evaluation}:
            \begin{equation}
                V(s) = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s,a) \left( R(s,a,s') + \gamma V(s') \right)
            \end{equation}
            Calculates the expected state values under the given policy.

        \item \textbf{Policy Improvement}:
            \begin{equation}
                \pi'(s) = \arg\max_{a \in A} \sum_{s'} P(s'|s,a) \left( R(s,a,s') + \gamma V(s') \right)
            \end{equation}
            Update policy if new policy $\pi'$ is different from old policy $\pi$.

        \item \textbf{Iteration}:
            Repeat until policy no longer changes.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Example}
    Consider a simple MDP with states $s_1$ and $s_2$:
    \begin{itemize}
        \item \textbf{Transition Probabilities}:
            \begin{itemize}
                \item $P(s_1|s_1, a_1) = 0.7, P(s_2|s_1, a_1) = 0.3$
                \item $P(s_1|s_2, a_2) = 0.4, P(s_2|s_2, a_2) = 0.6$
            \end{itemize}
        
        \item \textbf{Rewards}:
            \begin{itemize}
                \item $R(s_1, a_1, s_1) = 10, R(s_1, a_1, s_2) = 5$
                \item $R(s_2, a_2, s_1) = 2, R(s_2, a_2, s_2) = 8$
            \end{itemize}
    \end{itemize}
    \begin{enumerate}
        \item Initialize Policy: Let $\pi(s_1) = a_1, \pi(s_2) = a_2$.
        \item Evaluate $V$ for $\pi$ using the Bellman equation.
        \item Improve Policy by selecting actions that maximize expected returns.
        \item Repeat until convergence.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Key Points}
    \begin{itemize}
        \item \textbf{Convergence}: Policy Iteration is guaranteed to converge to the optimal policy.
        \item \textbf{Efficiency}: May be more efficient than Value Iteration for larger state spaces.
        \item \textbf{Applications}: Widely used in reinforcement learning and AI for decision-making in uncertain environments.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy Iteration - Conclusion}
    Policy Iteration is a powerful technique for solving MDPs by alternating between evaluating a policy and improving it. Understanding this algorithm is fundamental for exploring advanced strategies in sequential decision-making.
    
    \begin{block}{Note}
        Make sure to review mathematical concepts and their derivations for a clearer understanding, as they are fundamental to the algorithm's performance.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Overview}
    \begin{block}{Understanding Markov Decision Processes (MDPs)}
        MDPs provide a mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision maker.
    \end{block}
    \begin{itemize}
        \item MDPs consist of:
        \begin{itemize}
            \item States
            \item Actions
            \item Transition probabilities
            \item Rewards
            \item Policies
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Key Applications}
    \begin{enumerate}
        \item \textbf{Robotics}
        \begin{itemize}
            \item \textbf{Autonomous Navigation}: Robots make decisions in uncertain environments (e.g., navigating a home).
            \item \textbf{Example}: A robotic vacuum uses sensors to perceive its location, chooses to move, and receives feedback.
        \end{itemize}
        \item \textbf{Finance}
        \begin{itemize}
            \item \textbf{Portfolio Optimization}: Investors model choices under uncertainty to maximize returns.
            \item \textbf{Example}: An investor can buy, sell, or hold stocks based on observed market states.
        \end{itemize}
        \item \textbf{Healthcare}
        \begin{itemize}
            \item \textbf{Treatment Planning}: MDPs aid in managing chronic illnesses by making informed treatment decisions.
            \item \textbf{Example}: For diabetes management, states represent blood sugar levels influencing treatment decisions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Applications of MDPs - Key Points and Formula}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item \textbf{Uncertainty Management}: MDPs handle uncertainty effectively.
            \item \textbf{Flexibility}: Adaptable to various scenarios in different fields.
            \item \textbf{Policy and Optimization}: The core goal is to develop optimal policies for maximizing rewards.
        \end{itemize}
    \end{block}
    \begin{block}{Formula Overview}
        The value function \( V(s) \) is defined as:
        \begin{equation}
            V(s) = \max_a \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a)V(s') \right)
        \end{equation}
        Where:
        \begin{itemize}
            \item \( V(s) \): Value of state \( s \)
            \item \( R(s, a) \): Reward for taking action \( a \) in state \( s \)
            \item \( P(s'|s, a) \): Transition probability to state \( s' \)
            \item \( \gamma \): Discount factor (importance of future rewards)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Overview}
    \begin{block}{Introduction}
        Markov Decision Processes (MDPs) are frameworks used for decision-making where outcomes are partially random and partially controlled by the decision-maker. 
    \end{block}
    
    \begin{block}{Key Challenges}
        The following are common challenges faced when utilizing MDPs:
        \begin{enumerate}
            \item High-Dimensional State Spaces
            \item Computational Complexity
            \item Curse of Dimensionality
            \item Non-Stationarity
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - High-Dimensional State Spaces}
    \begin{block}{High-Dimensional State Spaces}
        \begin{itemize}
            \item As the number of states increases, model complexity escalates. 
            \item Results from:
                \begin{itemize}
                    \item Interacting multiple state variables.
                    \item Large product spaces from combining various factors or actions.
                \end{itemize}
            \item \textbf{Impact}: Increased memory and processing demands.
            \item \textbf{Example}: A robot navigating a complex environment with numerous obstacles leading to a high-dimensional state space.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Computational Complexity}
    \begin{block}{Computational Complexity}
        \begin{itemize}
            \item Solving MDPs involves iterative computations over large state spaces.
            \item \textbf{Scalability Issues}:
                \begin{itemize}
                    \item \textbf{Time Complexity}: Increases with state space size.
                    \item \textbf{Space Complexity}: Storing value function/policy can be massive.
                \end{itemize}
            \item \textbf{Example}: For a simple MDP with \( n \) states, standard policy evaluation can require \( O(n^2) \) time leading to inefficiencies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Curse of Dimensionality and Non-Stationarity}
    \begin{block}{Curse of Dimensionality}
        \begin{itemize}
            \item Refers to the exponential increase in volume when adding dimensions.
            \item \textbf{Impact}: Vast data requirements can hinder effective learning.
            \item \textbf{Example}: In healthcare, considering multiple patient attributes leads to a rapidly increasing state space.
        \end{itemize}
    \end{block}
    
    \begin{block}{Non-Stationarity}
        \begin{itemize}
            \item Real-world parameters affecting MDPs can change, resulting in non-stationary environments.
            \item \textbf{Impact}: Learned policies may become outdated requiring frequent adjustments.
            \item \textbf{Example}: Stock trading algorithms may underperform if market dynamics shift unexpectedly.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Challenges with MDPs - Summary and Key Points}
    \begin{block}{Summary}
        While MDPs are valuable for decision-making under uncertainty, challenges such as high-dimensional state spaces, computational complexity, curse of dimensionality, and non-stationarity must be addressed for effective application.
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item High-dimensional state spaces increase computational and memory demands.
            \item Computational complexity can lead to inefficiencies in large MDPs.
            \item The curse of dimensionality complicates learning and data collection.
            \item Non-stationarity can render policies obsolete over time.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion and Summary}
    \begin{block}{Understanding MDPs in Decision-Making Under Uncertainty}
        Markov Decision Processes (MDPs) provide a mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision-maker.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Components of MDPs}
    \begin{itemize}
        \item \textbf{Set of States (S)}: Possible situations for the decision maker.
        \item \textbf{Set of Actions (A)}: All possible choices to be made.
        \item \textbf{Transition Probabilities (P)}: Probabilities of moving between states.
        \item \textbf{Rewards (R)}: Immediate gain received after actions.
        \item \textbf{Discount Factor ($\gamma$)}: Prioritizes immediate rewards over future rewards.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points about MDPs}
    \begin{enumerate}
        \item \textbf{Sequential Decision-Making}: MDPs enable strategic planning over time.
        \item \textbf{Uncertainty Representation}: Captures uncertainty in environment and outcomes.
        \item \textbf{Optimal Policy}: Best action to take in each state to maximize expected rewards.
        \item \textbf{Algorithms}: 
        \begin{itemize}
            \item \textit{Value Iteration}: Updates value of states based on future rewards.
            \item \textit{Policy Iteration}: Alternates between policy evaluation and improvement.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Examples of MDPs}
    \begin{itemize}
        \item \textbf{Robotics Navigation}: 
        \begin{itemize}
            \item States: Different positions in a maze.
            \item Actions: Movements of the robot.
            \item Rewards: Gained by reaching the goal.
        \end{itemize}
        
        \item \textbf{Financial Investment}:
        \begin{itemize}
            \item States: Stock prices.
            \item Actions: Possible investment choices.
            \item Rewards: Performance-based returns over time.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in Real World Applications}
    \begin{itemize}
        \item \textbf{Autonomous Systems}: Decision-making in AI-driven vehicles.
        \item \textbf{Healthcare}: Optimizing patient treatment plans by considering health states and outcomes.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Closing Thoughts and Formula}
    \begin{block}{Conclusion}
        MDPs are essential models for decision-making under uncertainty, enhancing strategies across various fields.
    \end{block}
    
    \begin{equation}
        R(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s')
    \end{equation}
    \begin{block}{Where}
        $V(s)$ is the value function representing the maximum expected cumulative reward from state $s$.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions}
    Open floor for discussion and questions about MDPs and their applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of MDPs (Markov Decision Processes)}
    \begin{block}{Definition}
        An MDP is a mathematical framework used for modeling decision-making in situations where outcomes are uncertain. It consists of states, actions, transition probabilities, rewards, and a policy.
    \end{block}
    
    \begin{itemize}
        \item \textbf{States (S)}: Various conditions or situations in which a decision-maker might find themselves.
        \item \textbf{Actions (A)}: Choices available to the decision-maker that can affect the state.
        \item \textbf{Transition Probabilities (P)}: Probabilities of moving from one state to another given a specific action.
        \item \textbf{Rewards (R)}: Immediate return received after transitioning from one state to another due to an action.
        \item \textbf{Policy ($\pi$)}: A strategy that specifies the action to take in each state to achieve the best long-term outcome.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions to Facilitate Understanding}
    \begin{enumerate}
        \item \textbf{Understanding Trade-offs}: 
        \begin{itemize}
            \item How does an MDP facilitate decision-making when considering both immediate rewards and long-term benefits?
            \item \textit{Example}: In a navigation application, how does the app weigh the time taken to reach a destination versus fuel consumption?
        \end{itemize}
        
        \item \textbf{Real-world Applications}: 
        \begin{itemize}
            \item Which industries can benefit from MDPs? Examples include robotics, finance, healthcare, and autonomous systems.
            \item \textit{Example}: How do hospitals use MDPs to allocate resources effectively during high patient influx periods?
        \end{itemize}

        \item \textbf{Policy Optimization}: 
        \begin{itemize}
            \item What methods can be used to find the optimal policy in an MDP? Discuss Value Iteration and Policy Iteration.
            \item \textit{Illustration}: Draw the process flow of policy iteration showing how the policy improves over iterations until optimal.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Discussion Questions Continued}
    \begin{enumerate}
        \setcounter{enumi}{3} % Resume enumeration
        \item \textbf{Uncertainty in Decision Making}:
        \begin{itemize}
            \item How do MDPs handle uncertainty and variability in outcomes?
            \item Discuss the role of transition probabilities in capturing the uncertainty of actions.
        \end{itemize}

        \item \textbf{Limitations of MDPs}: 
        \begin{itemize}
            \item What are some challenges or limitations of using MDPs in complex environments?
            \item Discuss examples of where computational power might limit MDP applications, especially in very large state spaces.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item MDPs are powerful tools for optimal decision-making under uncertainty.
        \item Understanding the components of MDPs is crucial for applying them effectively.
        \item Real-world scenarios illustrate practical applications of MDPs, reinforcing theoretical concepts.
        \item Engage actively: Asking questions and providing examples fosters a better understanding of MDPsâ€™ applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    This slide opens the floor for discussion, allowing students to explore various aspects of MDPs through critical thinking and application-based questions. Encourage participation for deeper learning and clarity on complex concepts.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Overview of MDPs}
    
    \begin{block}{What are Markov Decision Processes (MDPs)?}
        \begin{itemize}
            \item \textbf{Definition:} MDPs are mathematical frameworks for decision-making where outcomes are partially random and partially controlled.
            \item \textbf{Components:} An MDP is defined by the tuple \((S, A, P, R, \gamma)\):
            \begin{itemize}
                \item \(S\): set of states
                \item \(A\): set of actions
                \item \(P\): state transition probability function
                \item \(R\): reward function
                \item \(\gamma\): discount factor
            \end{itemize}
        \end{itemize}
    \end{block}

    Understanding MDPs is crucial for mastering decision-making under uncertainty, especially in areas like artificial intelligence and operations research.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Recommended Readings}
    
    \begin{enumerate}
        \item \textbf{â€œMarkov Decision Processes: Algorithms and Applicationsâ€ by Steven M. LaValle}
        \begin{itemize}
            \item Provides an in-depth analysis of MDPs, covering both theoretical and practical aspects, including algorithms.
            \item \textbf{Key Takeaway:} Balance between theory and application.
        \end{itemize}
        
        \item \textbf{â€œDynamic Programming and Optimal Controlâ€ by Dimitri P. Bertsekas}
        \begin{itemize}
            \item Comprehensive guide on dynamic programming, foundational to MDP algorithms.
            \item \textbf{Key Takeaway:} Relationship between dynamic programming and optimal decision-making.
        \end{itemize}
        
        \item \textbf{â€œReinforcement Learning: An Introductionâ€ by Richard S. Sutton and Andrew G. Barto}
        \begin{itemize}
            \item Foundational text in reinforcement learning linking clearly to MDPs.
            \item \textbf{Key Takeaway:} MDPs as a basis for various reinforcement learning algorithms.
        \end{itemize}
        
        \item \textbf{Research Papers and Articles}
        \begin{itemize}
            \item â€œA Survey of Markov Decision Process Applicationsâ€ 
            \item \textbf{Key Takeaway:} Diverse applications across industries.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Further Reading and Resources - Online Resources}
    
    \begin{itemize}
        \item \textbf{MDP Packages in Python:}
        \begin{itemize}
            \item \textbf{OpenAI Gym:} A toolkit for developing and comparing reinforcement learning algorithms.
            \item \textit{Example code snippet:}
            \begin{lstlisting}[language=Python]
import gym
env = gym.make("Taxi-v3")
state = env.reset()
env.render()
            \end{lstlisting}
        \end{itemize}
        
        \item \textbf{Interactive Learning:}
        \begin{itemize}
            \item \textbf{Coursera:} Offers courses on reinforcement learning integrating MDPs.
            \item \textbf{edX:} Provides free courses on decision-making and optimization.
        \end{itemize}
    \end{itemize}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Understanding MDPs lays groundwork for advanced decision theory topics.
            \item Applications span across fields like robotics and finance.
            \item Engaging with theoretical and practical resources enhances comprehension.
        \end{itemize}
    \end{block}
\end{frame}


\end{document}