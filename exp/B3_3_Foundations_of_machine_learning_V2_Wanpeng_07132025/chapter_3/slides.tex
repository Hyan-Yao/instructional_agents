\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Title Page Information
\title[Data Collection and Cleaning]{Chapter 3: Data Collection and Cleaning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Data Collection and Cleaning}
    \begin{block}{Concept Overview}
        Data collection and cleaning are foundational steps in the machine learning lifecycle. Without high-quality data, even the most sophisticated algorithms may yield poor results. 
        In this section, we explore their essential roles in effective model training.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Data Collection?}
    \begin{itemize}
        \item Data collection is the process of gathering information from various sources to create a dataset suitable for analysis.
        \item \textbf{Importance:}
        \begin{itemize}
            \item Ensures diverse and representative samples to train your model.
            \item Affects the model's ability to generalize to unseen data.
        \end{itemize}
        \item \textbf{Examples:}
        \begin{itemize}
            \item Surveys and Questionnaires
            \item Web Scraping
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{What is Data Cleaning?}
    \begin{itemize}
        \item Data cleaning involves identifying and correcting inaccuracies or inconsistencies in data.
        \item \textbf{Importance:}
        \begin{itemize}
            \item Enhances data reliability and reduces errors.
            \item Improves model performance by ensuring inputs reflect true conditions.
        \end{itemize}
        \item \textbf{Key Cleaning Processes:}
        \begin{itemize}
            \item Removing Duplicates
            \item Handling Missing Values
            \end{itemize}
            Common methods include:
            \begin{itemize}
                \item Mean/Median Imputation
                \item Deletion
            \end{itemize}
            \item \textbf{Illustration:} 
            Imagine training a model on housing prices with inconsistent entries (e.g., "2000 square feet" vs. "20,000 square feet"). Data cleaning standardizes such entries to improve accuracy.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Preparing for Model Training}
    \begin{itemize}
        \item Once data is collected and cleaned, it lays the groundwork for robust machine learning models.
        \item \textbf{Key Steps:}
        \begin{itemize}
            \item Structure data properly (categorical vs numerical)
            \item Split data into training, validation, and testing sets for accurate model evaluation.
        \end{itemize}
        \item \textbf{Example:} Cleaning weather data (temperature, humidity, wind speed) leads to better predictive outcomes.
    \end{itemize}

    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Data quality impacts model performance—\textbf{Garbage in, garbage out.}
            \item Effective data collection and cleaning are iterative processes.
            \item A high-quality dataset is crucial for reliable, useful machine learning solutions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Takeaway Question}
    \begin{block}{Discussion}
        How might poor data collection and cleaning affect a machine learning project you're interested in? 
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data in Machine Learning - Introduction}
    \begin{itemize}
        \item \textbf{Foundation of Machine Learning}: Data is the backbone of all machine learning (ML) models.
        \item \textbf{Training Models}: Models learn to recognize patterns, make predictions, and generate insights from data.
        \item \textbf{Example}: A spam detection system trained on thousands of labeled emails to classify new emails correctly.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data in Machine Learning - Data Quality Matters}
    \begin{itemize}
        \item \textbf{Impact on Outcomes}: Quality of data influences ML model performance.
        \begin{itemize}
            \item \textbf{Accuracy}: Is the data correct and free of errors?
            \item \textbf{Completeness}: Are all necessary data points available?
            \item \textbf{Consistency}: Are the formats and values coherent?
        \end{itemize}
        \item \textbf{Example}: Datasets with missing values or outliers lead to poor model performance in real-world applications.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data in Machine Learning - Real-World Implications}
    \begin{itemize}
        \item \textbf{Success Stories}: Prioritizing data quality leads to better ML performance.
        \begin{itemize}
            \item \textbf{Healthcare}: Accurate patient data improves diagnosis and treatment plans.
            \item \textbf{Marketing}: Quality customer data enhances personalized recommendations and increases sales.
        \end{itemize}
        \item \textbf{Conversely}: Biased or incomplete data can propagate unfair practices or missed opportunities.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data in Machine Learning - Reflective Questions}
    \begin{itemize}
        \item How does the quality of your data reflect on decision-making in your field of interest?
        \item What strategies can you implement to ensure data quality in your projects?
        \item Can you think of an example where poor data led to negative outcomes in a real-world application?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance of Data in Machine Learning - Conclusion}
    \begin{itemize}
        \item Data is not just an input; it informs the structure and success of ML applications.
        \item Quality data leads to robust models capable of reliable predictions.
        \item Understanding data collection and cleaning is essential for harnessing ML's full potential.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Types of Data Used in Machine Learning}
    \begin{block}{Overview of Data Types}
        In machine learning, data plays a crucial role in building effective models. Understanding the types of data is essential for successful data collection and cleaning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Types - Structured and Unstructured}
    \begin{enumerate}
        \item \textbf{Structured Data}
            \begin{itemize}
                \item \textbf{Definition}: Organized in a fixed format (rows and columns), stored in databases.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Customer information table (name, age, email)
                        \item Sales data (date, product, revenue)
                    \end{itemize}
                \item \textbf{Cleaning Techniques}:
                    \begin{itemize}
                        \item Handling missing values (imputation)
                        \item Normalization and scaling (min-max scaling)
                    \end{itemize}
            \end{itemize}
        
        \item \textbf{Unstructured Data}
            \begin{itemize}
                \item \textbf{Definition}: Lacks a predefined format or structure.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Text data (emails, social media posts)
                        \item Image data (photographs)
                    \end{itemize}
                \item \textbf{Cleaning Techniques}:
                    \begin{itemize}
                        \item Text preprocessing (removing stopwords, tokenization)
                        \item Image preprocessing (resizing, denoising)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Types - Semi-Structured, Time-Series, and Categorical}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Semi-Structured Data}
            \begin{itemize}
                \item \textbf{Definition}: Does not conform to a strict schema but has some organizational properties.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item JSON, XML files
                        \item Log files from applications
                    \end{itemize}
                \item \textbf{Cleaning Techniques}:
                    \begin{itemize}
                        \item Schema definition for extraction
                        \item Data transformation for consistency
                    \end{itemize}
            \end{itemize}

        \item \textbf{Time-Series Data}
            \begin{itemize}
                \item \textbf{Definition}: Collected at specific time intervals, used for forecasting.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Stock prices over time
                        \item Environmental data (temperature, humidity)
                    \end{itemize}
                \item \textbf{Cleaning Techniques}:
                    \begin{itemize}
                        \item Handling missing time points (interpolation)
                        \item Smoothing techniques (moving averages)
                    \end{itemize}
            \end{itemize}

        \item \textbf{Categorical Data}
            \begin{itemize}
                \item \textbf{Definition}: Divided into distinct categories without intrinsic ordering.
                \item \textbf{Examples}:
                    \begin{itemize}
                        \item Gender (male, female)
                        \item Product type (electronics, clothing)
                    \end{itemize}
                \item \textbf{Cleaning Techniques}:
                    \begin{itemize}
                        \item Encoding categorical variables (one-hot encoding, label encoding)
                    \end{itemize}
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item Data type influences approaches for collection and cleaning.
            \item Structured data is easier to manage, while unstructured data may require complex cleaning techniques.
            \item Understanding data types affects modeling processes, impacting choice of algorithms and methods.
        \end{itemize}
    \end{block}

    \begin{block}{Engaging Thought Question}
        "How might the transformation of unstructured data (like text or images) unlock new insights for businesses compared to purely structured data?"
    \end{block}
    
    \begin{block}{Conclusion}
        Recognizing and appropriately handling different data types is foundational in machine learning, guiding collection and cleaning processes for effective model training and deployment.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Overview}
    \begin{block}{Introduction}
        Data collection is a crucial step in any analytical process, as the quality and type of data gathered can significantly affect the results. There are several methods for collecting data, each with its advantages and applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Surveys}
    \begin{itemize}
        \item \textbf{Description}: Surveys involve collecting data through questionnaires or interviews, which can be administered online, via phone, or in-person.
        \item \textbf{Example}: In healthcare, a hospital may conduct a survey to understand patient satisfaction after a treatment, helping improve services.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Easily customizable to target specific information.
            \item Can reach a large audience fairly quickly.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Web Scraping}
    \begin{itemize}
        \item \textbf{Description}: This technique involves extracting data from websites, automating the collection process, thus saving time over manual methods.
        \item \textbf{Example}: A social media researcher might scrape data from Twitter to analyze trending topics or user sentiments, gauging public opinion on various issues.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Requires knowledge of programming (e.g., Python libraries like Beautiful Soup or Scrapy).
            \item Must respect the website’s terms of service.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Public Datasets}
    \begin{itemize}
        \item \textbf{Description}: Public datasets are collections of data available for anyone to use, sourced from government databases, research institutions, or community-driven projects.
        \item \textbf{Example}: The CDC (Centers for Disease Control and Prevention) provides a variety of public health datasets for research and analysis.
        \item \textbf{Key Points}:
        \begin{itemize}
            \item Offers a wealth of data without the need for collection.
            \item May require careful cleaning and processing before use.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Summary}
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Technique} & \textbf{Description} & \textbf{Example}  \\
        \hline
        Surveys & Questionnaires or interviews to collect structured data. & Patient satisfaction surveys in hospitals. \\
        \hline
        Web Scraping & Automated extraction of data from websites. & Analyzing Twitter data for public sentiment. \\
        \hline
        Public Datasets & Pre-collected datasets available for public use. & CDC health datasets for research purposes. \\
        \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Collection Techniques - Conclusion}
    \begin{block}{Conclusion}
        Choosing the right method of data collection is vital for ensuring the relevance and accuracy of the data. Each method has its strengths and challenges, and the best choice often depends on the specific goals of the research or analysis being conducted.
    \end{block}
    
    \begin{block}{Next Steps}
        As we delve further into this chapter, we will explore the essential ethical considerations surrounding data collection practices to ensure responsible use of data.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ethical Considerations in Data Collection}
    Ethical considerations in data collection guide researchers to prioritize the rights and well-being of individuals.
\end{frame}

\begin{frame}[fragile]
    \frametitle{What Are Ethical Considerations?}
    \begin{block}{Definition}
        Ethical considerations are essential principles that ensure the rights and well-being of individuals in data collection.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts}
    
    \begin{enumerate}
        \item \textbf{Informed Consent}
        \begin{itemize}
            \item Definition: Participants must know how their data will be used and agree voluntarily.
            \item Example: Informing participants about survey objectives and confidentiality before a mental health study.
            \item Importance: Builds trust and promotes transparency.
        \end{itemize}

        \item \textbf{Privacy and Confidentiality}
        \begin{itemize}
            \item Definition: Privacy protects personal information; confidentiality manages data sharing.
            \item Example: Anonymizing social media data to safeguard individual identities.
            \item Importance: Protects participant data, fostering future research involvement.
        \end{itemize}

        \item \textbf{Use of Datasets Without Proper Context}
        \begin{itemize}
            \item Definition: Understanding context is crucial to avoid misleading conclusions.
            \item Example: Ignoring demographic factors in healthcare data may lead to ineffective solutions.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Ethical data practices are mandatory for credibility in research.
        \item Neglecting ethics may result in privacy violations or legal consequences.
        \item Encourage active engagement with questions about data sources and ethics.
    \end{itemize}
    
    \textbf{Question for Discussion:} 
    How can we ensure our data collection methods uphold ethical standards?
\end{frame}

\begin{frame}[fragile]
    \frametitle{Illustration: The Informed Consent Process}
    \begin{center}
        \textbf{Informed Consent Flowchart}
    \end{center}
    \begin{itemize}
        \item Explain Purpose 
        \item Describe Data Usage 
        \item Answer Questions 
        \item Obtain Consent
    \end{itemize}
    \begin{block}{Importance}
        This flowchart ensures that informed consent is adequately obtained before data collection begins.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Data Quality}
    \begin{block}{What is Data Quality?}
        Data quality refers to the condition of data based on several factors 
        that reflect its suitability for its intended purpose. High-quality data 
        is critical for making decisions, especially within machine learning 
        workflows, as it directly influences the model’s performance and reliability.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why Does Data Quality Matter?}
    \begin{itemize}
        \item \textbf{Decision-Making}: Accurate analysis and predictions rely on quality data. Poor-quality data can lead to wrong conclusions.
        \item \textbf{Model Performance}: The effectiveness of machine learning algorithms hinges on the training data quality. High-quality data yields better models and predictions.
        \item \textbf{Trustworthiness}: Stakeholder confidence is impacted by data quality. Reliable data builds trust in systems that make automated decisions.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Dimensions of Data Quality}
    \begin{enumerate}
        \item \textbf{Accuracy}:
            \begin{itemize}
                \item Degree to which data correctly reflects the real-world scenario.
                \item \textit{Example}: Historical weather data showing incorrect temperatures.
            \end{itemize}
        \item \textbf{Completeness}:
            \begin{itemize}
                \item Extent to which all required data is present.
                \item \textit{Example}: Customer database missing email addresses.
            \end{itemize}
        \item \textbf{Consistency}:
            \begin{itemize}
                \item Uniformity of data across datasets.
                \item \textit{Example}: Different naming conventions for a city.
            \end{itemize}
        \item \textbf{Timeliness}:
            \begin{itemize}
                \item Data being up-to-date when needed for analysis.
                \item \textit{Example}: Outdated stock prices in forecasting models.
            \end{itemize}
        \item \textbf{Validity}:
            \begin{itemize}
                \item Degree to which data fits defined formats or standards.
                \item \textit{Example}: Date of birth format adherence.
            \end{itemize}
        \item \textbf{Uniqueness}:
            \begin{itemize}
                \item Ensuring each data record is distinct, with no duplicates.
                \item \textit{Example}: Duplicate entries in contact lists.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Data Cleaning Processes}
    \begin{block}{Introduction to Data Cleaning}
        Data cleaning is the process of correcting or removing inaccurate records from a dataset. 
        Quality data is crucial for effective analysis and machine learning, as it directly impacts the results and insights derived.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Common Data Cleaning Methods}
    \begin{enumerate}
        \item Handling Missing Data
        \item Removing Duplicates
        \item Detecting and Handling Outliers
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Handling Missing Data}
    \begin{itemize}
        \item \textbf{Identifying Missing Data}
        \begin{itemize}
            \item Use visualizations (e.g., heat maps) to spot missing values.
        \end{itemize}
        \item \textbf{Ways to Handle Missing Data}
        \begin{itemize}
            \item \textit{Removal:} Exclude rows or columns with missing values.
            \item \textit{Imputation:} Fill in missing values using techniques such as:
            \begin{itemize}
                \item Mean/Median Imputation
                \item Predictive Imputation using machine learning models
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Removing Duplicates}
    \begin{itemize}
        \item \textbf{Identifying Duplicates}
        \begin{itemize}
            \item Use functions in programming languages (e.g., Python) to identify duplicates.
            \item \textit{Example Code:}
            \begin{lstlisting}
df.drop_duplicates(inplace=True)
            \end{lstlisting}
        \end{itemize}
        \item \textbf{Elimination}
        \begin{itemize}
            \item Remove duplicate records to ensure dataset uniqueness.
            \item Significance: Duplicates can skew analysis results.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Detecting and Handling Outliers}
    \begin{itemize}
        \item \textbf{Identifying Outliers}
        \begin{itemize}
            \item Use statistical methods like Z-score or IQR to detect anomalies.
        \end{itemize}
        \item \textbf{Methods to Handle Outliers}
        \begin{itemize}
            \item Removal: Exclude outliers if due to errors in data collection.
            \item Transformation: Apply logarithmic transformations.
            \item Capping: Limit outlier values to a specified range.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Importance of Data Cleaning: Clean data leads to reliable models and better decisions.
        \item Balance: Understand the impact between losing information and retaining inconsistencies.
        \item Iteration: Data cleaning is an iterative process requiring ongoing quality checks.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    Effective data cleaning is vital for creating a robust dataset for analysis.  
    Employing the right methods ensures the integrity and accuracy of your data.
\end{frame}

\begin{frame}
    \frametitle{Additional Resources}
    \begin{itemize}
        \item Recommended readings on data quality and data cleaning techniques.
        \item Data cleaning libraries such as Pandas (Python) for practical application.
    \end{itemize}
\end{frame}

\begin{frame}{Tools for Data Cleaning}
    \frametitle{Introduction to Data Cleaning Tools}
    Data cleaning is a critical step in preparing datasets for analysis. It ensures data accuracy and completeness, leading to meaningful insights. Popular tools include:
    \begin{itemize}
        \item Google AutoML
        \item Python Libraries (e.g., Pandas, NumPy)
        \item Excel
        \item OpenRefine
    \end{itemize}
\end{frame}

\begin{frame}{Google AutoML}
    \frametitle{1. Google AutoML}
    \begin{itemize}
        \item \textbf{Overview}: Suite of machine learning products for training models with minimal coding required.
        \item \textbf{User-Friendly Features}:
            \begin{itemize}
                \item Drag-and-Drop Interface for easy dataset uploads.
                \item Automated cleaning of missing data, duplicates, and outliers.
            \end{itemize}
        \item \textbf{Example Use Case}: A marketing analyst uploads customer data to ensure no duplicates and fills missing entries.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Python Libraries}
    \frametitle{2. Python Libraries}
    Python is popular for data science with multiple libraries for data cleaning:
    
    \begin{itemize}
        \item \textbf{Pandas}: Powerful for data manipulation.
            \begin{itemize}
                \item Detect & fill missing values: \texttt{df.fillna(value)}
                \item Drop duplicates: \texttt{df.drop_duplicates()}
                \item \textbf{Example Code}:
                \begin{lstlisting}[language=Python]
import pandas as pd

# Load dataset
df = pd.read_csv('data.csv')

# Fill missing values
df['column_name'].fillna(df['column_name'].median(), inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)
                \end{lstlisting}
            \end{itemize}
        \item \textbf{NumPy}: For numerical operations.
        \item \textbf{Dask}: For larger-than-memory datasets with parallel processing.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Data Cleaning in Practice}
    
    \begin{block}{Overview}
        In this case study, we explore data cleaning through the example of ShopSmart, a fictional online retail company. 
        This analysis highlights the challenges faced, the solutions implemented, and the impact on data quality.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Initial Data Quality Assessment}
    
    \begin{itemize}
        \item \textbf{Data Collected:}
        \begin{itemize}
            \item Transaction ID
            \item Customer ID
            \item Purchase Date
            \item Product ID
            \item Quantity
            \item Price
            \item Payment Method
        \end{itemize}
        
        \item \textbf{Challenges Identified:}
        \begin{enumerate}
            \item Missing Values: Crucial information missing for 'Payment Method' and 'Price'.
            \item Inconsistencies: Variations in 'Payment Method' formatting.
            \item Outliers: Unusually high quantities indicating potential errors.
            \item Duplicate Entries: Repeated transaction records inflating sales data.
        \end{enumerate}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Data Cleaning Process}
    
    \begin{enumerate}
        \item \textbf{Handling Missing Values:}
        \begin{itemize}
            \item Impute missing 'Payment Method' values with the mode.
            \item Remove entries missing crucial fields like 'Price'.
        \end{itemize}
        
        \item \textbf{Standardizing Values:}
        \begin{itemize}
            \item Create a mapping system for 'Payment Method'.
            \item Normalize entries to a standard format.
        \end{itemize}
        
        \item \textbf{Identifying and Removing Outliers:}
        \begin{itemize}
            \item Use the Interquartile Range (IQR) method for detection.
            \item Remove records exceeding 1.5 * IQR above the third quartile.
        \end{itemize}
        
        \item \textbf{Resolving Duplicates:}
        \begin{itemize}
            \item Implement checks against 'Transaction ID' and 'Customer ID'.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Tools Utilized and Key Takeaways}
    
    \begin{itemize}
        \item \textbf{Tools Utilized:}
        \begin{itemize}
            \item Python Libraries: Pandas, NumPy.
            \item Data Visualization: Matplotlib.
        \end{itemize}
        
        \item \textbf{Key Takeaways:}
        \begin{itemize}
            \item Clean data is fundamental for reliable analysis.
            \item A systematic approach mitigates errors and improves data fidelity.
            \item Automation can significantly enhance the data cleaning workflow.
        \end{itemize}
    \end{itemize}
    
    By the end of this case study, we acknowledge that effective data cleaning is vital for any organization's success in data-driven decisions.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices for Data Collection and Cleaning - Overview}
    To achieve high-quality datasets, effective data collection and cleaning are critical. Consider the following essential best practices:
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices - Objectives and Sources}
    \begin{enumerate}
        \item \textbf{Define Clear Objectives}
        \begin{itemize}
            \item Clearly define what you intend to achieve with your data collection.
            \item \textit{Example}: Specify whether you want quantitative metrics (ratings) or qualitative insights (comments) from customer feedback.
        \end{itemize}

        \item \textbf{Use Reliable Data Sources}
        \begin{itemize}
            \item Ensure data sources are reputable and trustworthy.
            \item \textit{Example}: Use verified retail platforms for sales data instead of user-generated sites that may be inaccurate.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices - Data Entry and Validation}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Standardize Data Entry}
        \begin{itemize}
            \item Establish formats to minimize inconsistencies.
            \item \textit{Example}: Use dropdown menus for categorical variables (e.g., state selection).
        \end{itemize}

        \item \textbf{Implement Real-time Validation}
        \begin{itemize}
            \item Use validation checks during data entry to catch errors.
            \item \textit{Example}: Validate email formats or restrict age inputs to numerical values (e.g., 0-120).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices - Audits and Cleaning Plans}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Conduct Regular Audits}
        \begin{itemize}
            \item Frequently review datasets for errors or anomalies.
            \item \textit{Example}: Run periodic checks to identify outliers in sales transactions.
        \end{itemize}

        \item \textbf{Create a Robust Cleaning Plan}
        \begin{itemize}
            \item Develop a systematic approach to cleaning, handling missing values, duplicates, and inconsistencies.
            \item \textit{Example}: Use mean/mode imputation for missing data and remove duplicates.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Best Practices - Documentation and Ethics}
    \begin{enumerate}
        \setcounter{enumi}{6}
        \item \textbf{Document Your Processes}
        \begin{itemize}
            \item Keep a detailed record of data collection and cleaning methods.
            \item \textit{Example}: Document why certain data points were removed or altered for transparency.
        \end{itemize}

        \item \textbf{Ensure Ethical Data Practices}
        \begin{itemize}
            \item Respect privacy and comply with regulations (e.g., GDPR).
            \item \textit{Example}: Anonymize sensitive data during collection.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points and Conclusion}
    \begin{itemize}
        \item High-quality datasets stem from clear objectives and rigorous methodologies.
        \item Regular audits and validations are essential for maintaining data integrity.
        \item Ethical considerations are crucial throughout data handling.
    \end{itemize}

    \textbf{Conclusion:} Implementing these best practices builds the foundation for high-quality data, essential for effective analysis and informed decision-making.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion: Data Collection and Cleaning}
    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item \textbf{Importance of Data Quality}
            \item \textbf{Data Collection}
            \item \textbf{Data Cleaning}
            \item \textbf{Impact on Machine Learning Outcomes}
            \item \textbf{Iterative Process}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 1}
    \begin{itemize}
        \item \textbf{Importance of Data Quality:}  
        High-quality datasets lead to accurate models; poor quality can result in misleading outcomes.  
        \textit{Example:} Inaccurate property data can misinform housing price models.

        \item \textbf{Data Collection:}  
        The initial step in building a successful model involves identifying sources and ensuring data representation.  
        \textit{Best Practices:}
        \begin{itemize}
            \item Define clear objectives for data collection.
            \item Utilize diverse sources.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion - Part 2}
    \begin{itemize}
        \item \textbf{Data Cleaning:}  
        Processing datasets to improve quality by addressing errors and inconsistencies.  
        \textit{Critical Steps:}
        \begin{itemize}
            \item Removing duplicates
            \item Filling or removing missing values
            \item Correcting inaccuracies (e.g., outliers)
        \end{itemize}

        \item \textbf{Impact on Machine Learning Outcomes:}  
        Quality data influences model effectiveness and generalization to unseen data.  
        \textit{Statistical Insight:} Clean datasets lead to lower variance and bias.

        \item \textbf{Iterative Process:}  
        Data tasks are ongoing; as new data appears, models and datasets must be updated.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Final Thoughts and Reflection}
    \begin{block}{Final Thoughts}
        Machine learning thrives on well-collected and cleaned data. Engaging with data nuances lays the foundation for impactful predictive models.
    \end{block}

    \begin{block}{Reflection Questions}
        \begin{itemize}
            \item How can poor data collection affect business decisions?
            \item In what ways can automated tools assist in data cleaning?
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Discussion and Questions - Introduction}
    \begin{block}{Data Integrity in Machine Learning}
        Data integrity refers to the accuracy, consistency, and reliability of data throughout its lifecycle. 
        In the context of machine learning (ML), data integrity is crucial for producing valid and actionable insights.
    \end{block}
\end{frame}

\begin{frame}[fragile]{Discussion and Questions - Key Topics for Discussion}
    \begin{itemize}
        \item \textbf{Importance of Data Quality}
            \begin{itemize}
                \item A healthcare ML model trained on faulty data may incorrectly diagnose diseases, leading to harmful patient outcomes.
                \item \textbf{Discussion Question:} How can we ensure that the dataset we’re using is accurate and representative of the real-world scenario?
            \end{itemize}
        
        \item \textbf{Common Data Issues}
            \begin{itemize}
                \item Missing Data: Can result from errors in collection or reporting.
                \item Outliers: Significant deviations from other data points.
                \item \textbf{Discussion Question:} What strategies might we employ to manage outliers effectively in our datasets?
            \end{itemize}
        
        \item \textbf{Data Cleaning Techniques}
            \begin{itemize}
                \item Techniques such as normalization, deduplication, and transformation help refine data before training.
                \item Example: Converting all text to lowercase for consistency in text data analysis.
                \item \textbf{Discussion Question:} What’s one cleaning technique you think is essential before feeding data into a machine learning model?
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Discussion and Questions - Impact and Ethics}
    \begin{itemize}
        \item \textbf{Impact of Poor Data Practices}
            \begin{itemize}
                \item Real-world implications, like biased hiring algorithms due to non-representative training data.
                \item \textbf{Discussion Question:} Can you think of examples in recent news where data integrity issues led to negative consequences?
            \end{itemize}
        
        \item \textbf{Data Ethics}
            \begin{itemize}
                \item Reflect on how data practices affect personal privacy and societal values.
                \item \textbf{Discussion Question:} How do we balance the need for data with the ethical implications of its collection and use?
            \end{itemize}
        
        \item \textbf{Encouraging Deep Thought}
            \begin{itemize}
                \item What practices should be standardized to enhance data integrity?
                \item How can emerging technologies like AI help in upholding data standards?
            \end{itemize}
    \end{itemize}
\end{frame}


\end{document}