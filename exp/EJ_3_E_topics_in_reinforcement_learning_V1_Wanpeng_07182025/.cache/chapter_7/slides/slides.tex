\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
\setbeamerfont{caption}{size=\small}
\setbeamerfont{footnote}{size=\tiny}

% Custom Commands
\newcommand{\hilight}[1]{\colorbox{myorange!30}{#1}}
\newcommand{\concept}[1]{\textcolor{myblue}{\textbf{#1}}}
\newcommand{\separator}{\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}}

% Title Page Information
\title[Deep Reinforcement Learning]{Week 7: Deep Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \frametitle{Introduction to Deep Reinforcement Learning}
    \begin{block}{Overview}
        Deep Reinforcement Learning (DRL) combines reinforcement learning principles with deep learning methodologies. It empowers agents to learn optimal behaviors in complex environments by utilizing neural networks to model and predict outcomes from past experiences and rewards.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Significance in AI Applications}
    \begin{itemize}
        \item \textbf{Complex Decision-Making:}
            \begin{itemize}
                \item DRL excels in dynamic and complex environments such as robotics, gaming, and autonomous vehicles.
                \item \textit{Example:} AlphaGo by DeepMind plays Go at a superhuman level using DRL.
            \end{itemize}
        
        \item \textbf{Real-time Learning:}
            \begin{itemize}
                \item Agents adapt in real-time based on feedback from their actions.
                \item \textit{Example:} Autonomous drones learn navigation through trial and error.
            \end{itemize}
        
        \item \textbf{Scalability:}
            \begin{itemize}
                \item DRL scales from simple environments to high-dimensional spaces, versatile across various fields.
                \item \textit{Example:} OpenAI's Dota 2 bot learns from millions of simulations before competing.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Concepts and DRL Workflow}
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Agent:} The decision-maker (e.g., robot, software).
            \item \textbf{Environment:} The setting in which the agent operates (e.g., game board).
            \item \textbf{Actions:} Possible moves by the agent (e.g., move left or jump).
            \item \textbf{Rewards:} Feedback signal reflecting the effectiveness of actions (e.g., points in a game).
        \end{itemize}
    \end{block}

    \begin{block}{DRL Workflow}
        \begin{enumerate}
            \item \textbf{Observation:} The agent perceives the current state.
            \item \textbf{Action Selection:} The agent chooses an action based on its policy.
            \item \textbf{Reward \& Next State:} The agent receives feedback and observes the new state.
            \item \textbf{Policy Update:} The agent updates knowledge for improved decision-making.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example Formula}
    The update rule for modifying the policy can be expressed using the Bellman Equation:
    \begin{equation}
        Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
    \end{equation}
    \begin{itemize}
        \item \(Q(s, a)\): Estimated value of action \(a\) in state \(s\).
        \item \(\alpha\): Learning rate, controlling adaptation speed.
        \item \(r\): Reward received after action \(a\).
        \item \(\gamma\): Discount factor, determining future reward importance.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Engage with DRL}
    For practical understanding, implement a simple DRL agent using a library like TensorFlow or PyTorch to explore environments in OpenAI's Gym. Creating your own agent offers deeper comprehension and application of DRL principles.
    
    \begin{block}{Emphasis Points}
        \begin{itemize}
            \item DRL represents a powerful convergence of traditional RL and deep learning techniques.
            \item It is pivotal for advancing AI in fields requiring adaptive and complex decision-making capabilities.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamentals of Reinforcement Learning - Key Concepts}
    \begin{block}{1. Agent}
        \begin{itemize}
            \item \textbf{Definition}: An agent is the decision-maker in reinforcement learning. It interacts with the environment to achieve a specific goal or maximize cumulative reward.
            \item \textbf{Example}: In a self-driving car, the car itself is the agent, making decisions based on the current state of the road and its surroundings.
        \end{itemize}
    \end{block}

    \begin{block}{2. Environment}
        \begin{itemize}
            \item \textbf{Definition}: The environment is the setting in which the agent operates, encompassing everything the agent interacts with, including conditions and feedback.
            \item \textbf{Example}: In the self-driving car scenario, the environment includes the road, other vehicles, traffic signals, and pedestrians.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamentals of Reinforcement Learning - Actions and Rewards}
    \begin{block}{3. Actions}
        \begin{itemize}
            \item \textbf{Definition}: Actions are the choices made by the agent that influence its state within the environment. An action can change the environment and the subsequent state observed by the agent.
            \item \textbf{Example}: For a self-driving car, actions include accelerating, braking, turning left, or turning right.
        \end{itemize}
    \end{block}

    \begin{block}{4. Rewards}
        \begin{itemize}
            \item \textbf{Definition}: Rewards are feedback signals received by the agent after it performs an action in a given state, quantifying the immediate benefit of an action.
            \item \textbf{Example}: In the self-driving scenario, the car may receive a reward of +10 for safely navigating a traffic light and -10 for running a red light.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Fundamentals of Reinforcement Learning - Key Points}
    \begin{itemize}
        \item Reinforcement Learning (RL) revolves around the interaction between the agent and its environment.
        \item The agent’s goal is to learn a strategy (policy) that maximizes its cumulative rewards over time.
        \item The cycle of choosing actions, receiving rewards, and updating knowledge is fundamental to the learning process in RL.
    \end{itemize}

    \begin{block}{Summary}
        In reinforcement learning, the agent learns to make optimal decisions through exploration and exploitation within an environment. By understanding actions and rewards, agents adjust their strategies to achieve better performance over time.
    \end{block}
    
    \begin{block}{Diagram of Interaction}
    \centering
    \includegraphics[width=0.8\linewidth]{diagram.png} % Add your diagram as an image
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Deep Q-Networks (DQN)}
    \begin{block}{Overview}
        Deep Q-Networks (DQN) combine deep learning with reinforcement learning (RL) to approximate Q-values. This enables agents to learn optimal policies in complex environments effectively.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Key Components of DQNs}
    \begin{enumerate}
        \item \textbf{Q-Learning}
            \begin{itemize}
                \item Model-free RL algorithm for maximizing cumulative rewards.
                \item Updates Q-values using the Bellman equation:
                \begin{equation}
                    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_a Q(s', a) - Q(s, a) \right)
                \end{equation}
            \end{itemize}
        \item \textbf{Neural Network Architecture}
            \begin{itemize}
                \item Deep neural network estimates Q-values for actions based on state inputs.
            \end{itemize}
        \item \textbf{Experience Replay}
            \begin{itemize}
                \item Stabilizes training by breaking correlation between consecutive experiences.
            \end{itemize}
        \item \textbf{Target Network}
            \begin{itemize}
                \item Slower-updating network to calculate target Q-values, aiding in learning stability.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Example: Atari Breakout}
    \begin{block}{Functionality of DQNs}
        DQNs can derive an action-selection policy from approximated Q-values, often using an epsilon-greedy strategy to balance exploration and exploitation.
    \end{block}
    \begin{block}{Illustration}
        Consider the game **Atari Breakout**:
        \begin{itemize}
            \item DQN receives the game screen as input.
            \item The network predicts Q-values for actions: "move left," "move right," "launch ball."
            \item Agent takes actions based on predictions, collects rewards, and updates the network through experience replay.
        \end{itemize}
    \end{block}
    \begin{lstlisting}[language=Python, basicstyle=\footnotesize]
import numpy as np
import gym
from keras.models import Sequential
from keras.layers import Dense

# Initialize DQN network
def build_model(state_size, action_size):
    model = Sequential()
    model.add(Dense(24, input_dim=state_size, activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(action_size, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    return model
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation of DQNs - Overview}
    Deep Q-Networks (DQNs) combine reinforcement learning with deep learning, enabling agents to learn optimal actions in high-dimensional state spaces. 
    This presentation outlines a step-by-step guide to implement a DQN, emphasizing two key components:
    \begin{itemize}
        \item Experience Replay
        \item Target Networks
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Environment Setup}
    \begin{enumerate}
        \item \textbf{Setup Environment}
        \begin{itemize}
            \item Choose an environment compatible with OpenAI Gym (e.g., CartPole).
            \item Install necessary libraries (TensorFlow or PyTorch).
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - DQN Architecture}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item \textbf{Initialize the DQN Architecture}
        \begin{itemize}
            \item Create a neural network that predicts Q-values for each action given a state.
        \end{itemize}
        \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)
        \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Experience Replay}
    \begin{enumerate}
        \setcounter{enumi}{2}
        \item \textbf{Experience Replay}
        \begin{itemize}
            \item Purpose: Break the correlation between consecutive experiences, enhancing stability.
            \item Implementation:
            \begin{itemize}
                \item Maintain a replay buffer for past experiences.
                \item Sample random mini-batches for network updates.
            \end{itemize}
        \end{itemize}
        \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
from collections import deque
import random

class ReplayBuffer:
    def __init__(self, max_size):
        self.buffer = deque(maxlen=max_size)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
        \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Target Network}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Target Network}
        \begin{itemize}
            \item Purpose: Improve learning stability by evaluating target Q-values using a separate network.
            \item Implementation:
            \begin{itemize}
                \item Create a target network, updated periodically from the primary network.
                \item Use the target network for Bellman equation Q-value targets.
            \end{itemize}
        \end{itemize}
        \begin{block}{Example Code}
        \begin{lstlisting}[language=Python]
primary_network = DQN(input_dim, output_dim)
target_network = DQN(input_dim, output_dim)
target_network.load_state_dict(primary_network.state_dict())

def update_target_network():
    target_network.load_state_dict(primary_network.state_dict())
        \end{lstlisting}
        \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Step-by-Step Implementation - Training DQN}
    \begin{enumerate}
        \setcounter{enumi}{4}
        \item \textbf{Training the DQN}
        \begin{itemize}
            \item Initialize the environment and replay buffer.
            \item For each episode:
            \begin{enumerate}
                \item Choose an action using an epsilon-greedy policy.
                \item Store experience in the replay buffer.
                \item Sample a batch and compute the loss using:
                \begin{equation}
                \text{Loss} = \frac{1}{N} \sum_{i=1}^{N} \left( r + \gamma \max_a Q_{\text{target}}(s', a) - Q(s, a) \right)^2
                \end{equation}
                \item Update the primary network using backpropagation.
            \end{enumerate}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item Experience Replay enhances training stability and performance by reducing the correlation between samples.
        \item Target Networks mitigate rapid oscillations in Q-value updates for more reliable learning.
        \item Both components are crucial for successful DQN implementation and effective training results.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges and Solutions in DQNs - Introduction}
  \begin{block}{Introduction to DQNs}
    Deep Q-Networks (DQNs) combine deep learning with Q-learning, allowing agents to learn effective policies in high-dimensional state spaces. However, training DQNs is not without its challenges.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Challenges in Training DQNs}
  \begin{enumerate}
    \item \textbf{Instability and Divergence}
      \begin{itemize}
        \item \textbf{Problem}: DQNs can be sensitive to hyperparameters and initialization, leading to unstable training.
        \item \textbf{Examples}: Sudden drops in performance or failure to converge.
      \end{itemize}

    \item \textbf{Overestimation Bias}
      \begin{itemize}
        \item \textbf{Problem}: Q-learning tends to overestimate action values, leading to suboptimal policies.
        \item \textbf{Example}: An agent may prefer actions that seem better than they truly are, resulting in poor performance.
      \end{itemize}

    \item \textbf{Experience Correlation}
      \begin{itemize}
        \item \textbf{Problem}: Consecutive experiences can be highly correlated, leading to inefficient learning.
        \item \textbf{Example}: If an agent is in a similar state multiple times, it may not explore new actions effectively.
      \end{itemize}

    \item \textbf{Sample Inefficiency}
      \begin{itemize}
        \item \textbf{Problem}: DQNs often require vast amounts of experience to learn, making them sample inefficient.
        \item \textbf{Example}: Complex environments may take millions of episodes to achieve a competent policy.
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Solutions to Enhance Performance}
  \begin{enumerate}
    \item \textbf{Experience Replay}
      \begin{itemize}
        \item \textbf{Solution}: Store past experiences in a replay buffer to break the correlation.
        \item \textbf{Implementation}:
        \begin{lstlisting}[language=Python]
        replay_buffer.add(state, action, reward, next_state, done)
        batch = replay_buffer.sample(batch_size)
        \end{lstlisting}
      \end{itemize}
      
    \item \textbf{Target Network}
      \begin{itemize}
        \item \textbf{Solution}: Maintain a separate target network updated less frequently.
        \item \textbf{Implementation}:
        \begin{lstlisting}[language=Python]
        if episode % target_update_frequency == 0:
            target_net.load_state_dict(main_net.state_dict())
        \end{lstlisting}
      \end{itemize}
      
    \item \textbf{Double DQN}
      \begin{itemize}
        \item \textbf{Solution}: Use two networks to decouple action selection and evaluation, reducing overestimation bias.
        \item \textbf{Illustration}:
        \begin{equation}
          Q_{target}(s, a) = r + \gamma Q_{eval}(s', \arg\max_a Q_{main}(s', a))
        \end{equation}
      \end{itemize}
      
    \item \textbf{Prioritized Experience Replay}
      \begin{itemize}
        \item \textbf{Solution}: Prioritize experiences based on expected importance.
        \item \textbf{Implementation}:
        \begin{lstlisting}[language=Python]
        probabilities = calculate_probabilities(replay_buffer)
        batch = sample_based_on_priorities(probabilities, batch_size)
        \end{lstlisting}
      \end{itemize}
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Policy Gradients Overview}
  % Introduction to policy gradient methods
  \begin{block}{Introduction}
    Policy gradient methods optimize the policy directly in reinforcement learning, focusing on maximizing expected returns through parameter adjustments.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Policy Gradient Methods}
  % Explanation of policy gradient methods
  \begin{itemize}
    \item **What are Policy Gradient Methods?**
      \begin{itemize}
        \item They optimize the policy directly.
        \item They parameterize the policy and use optimization to maximize expected returns.
      \end{itemize}
    
    \item **Why Use Policy Gradients?**
      \begin{itemize}
        \item Directly optimize the expected cumulative reward.
        \item Handle continuous action spaces effectively.
        \item Promote exploration due to their probabilistic nature.
      \end{itemize}      
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Key Concepts}
  % Important formulas and concepts
  \begin{enumerate}
    \item **Policy (\(\pi\))**: A mapping from states to action probabilities:
      \[
      \pi(a|s; \theta) \text{ where } \theta \text{ are the parameters.}
      \]

    \item **Expected Return**: Cumulative future reward following policy \(\pi\) from state \(s\):
      \[
      J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ R(\tau) \right]
      \]

    \item **Gradient Ascent**: Finding the optimal policy parameters \(\theta\) using:
      \[
      \nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla \log \pi(a|s; \theta) R(\tau) \right]
      \]
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Simple Cartpole Problem}
  % Example for better understanding
  In the Cartpole problem, we model our policy as a neural network:
  \begin{itemize}
    \item **Inputs**: Cart position, pole angle, and their velocities.
    \item **Output**: Probability of moving left or right.
  \end{itemize}
  
  \textbf{Training Steps}:
  \begin{enumerate}
    \item Collect episodes using a random policy.
    \item Calculate cumulative rewards for each episode.
    \item Use rewards to compute gradients and update policy parameters.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Strengths and Weaknesses}
  % Summary of strengths and weaknesses
  \begin{block}{Strengths}
    \begin{itemize}
      \item Handles high-dimensional action spaces and continuous actions.
      \item Stable learning with proper exploration strategies.
    \end{itemize}
  \end{block}
  
  \begin{block}{Weaknesses}
    \begin{itemize}
      \item Higher variance compared to value-based approaches may slow convergence.
      \item Requires a larger number of samples for policy evaluation.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Conclusion and Key Points}
  % Conclusion and summary points to remember
  Policy gradient methods are vital for solving complex reinforcement learning problems, especially in continuous action spaces.
  
  \begin{itemize}
    \item Direct optimization of the policy function.
    \item Effective in continuous action scenarios.
    \item Use expected return to guide policy updates.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview}
    In Reinforcement Learning (RL), decision-making strategies can be categorized into two main types:
    \begin{itemize}
        \item \textbf{Value-Based} methods
        \item \textbf{Policy-Based} methods
    \end{itemize}
    Understanding these methods is crucial for selecting the right approach for various applications.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Value-Based Methods}
    
    \textbf{Definition:} 
    Value-based methods aim to derive a \textbf{value function}, estimating the expected return (reward) for being in a particular state or taking a specific action.
    
    \textbf{Example: Deep Q-Networks (DQNs)}
    \begin{itemize}
        \item In DQNs, a neural network approximates the \textbf{Q-value} function to determine the best action for each state.
        \item Update rule based on the \textbf{Bellman Equation}:
    \end{itemize}
    
    \begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_a Q(s', a) - Q(s, a) \right)
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strengths and Weaknesses of Value-Based Methods}

    \textbf{Strengths:}
    \begin{itemize}
        \item \textbf{Efficiency}: Learns from fewer updates using a value function.
        \item \textbf{Off-Policy Learning}: Can learn from experiences not gathered during the current policy execution.
    \end{itemize}

    \textbf{Weaknesses:}
    \begin{itemize}
        \item \textbf{Stability Issues}: Can diverge with function approximation, leading to instability.
        \item \textbf{Exploration Challenges}: May converge to suboptimal policies without adequate exploration strategies.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Policy-Based Methods}
    
    \textbf{Definition:} 
    Policy-based methods directly parameterize the policy (the agent's decision-making function) and optimize it to maximize expected cumulative reward.

    \textbf{Example: REINFORCE Algorithm}
    \begin{itemize}
        \item Update rule for policy parameters \( \theta \):
    \end{itemize}

    \begin{equation}
    \theta \leftarrow \theta + \alpha \cdot \nabla_\theta \log \pi_\theta(s, a) \cdot R
    \end{equation}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Strengths and Weaknesses of Policy-Based Methods}

    \textbf{Strengths:}
    \begin{itemize}
        \item \textbf{Stability}: Directly optimizes the policy, leading to more stable learning.
        \item \textbf{Continuous Action Spaces}: Effective for problems with continuous action spaces.
    \end{itemize}

    \textbf{Weaknesses:}
    \begin{itemize}
        \item \textbf{Sample Inefficiency}: Requires many samples to converge, does not reuse past experiences.
        \item \textbf{High Variance}: Updates can be unstable due to the stochastic nature of policy gradient estimates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Comparison Points}

    \begin{center}
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{Aspect} & \textbf{Value-Based Methods} & \textbf{Policy-Based Methods} \\
    \hline
    Goal & Learn value function & Optimize policy \\
    \hline
    Learning Type & Indirect & Direct \\
    \hline
    Example & Q-learning, DQNs & REINFORCE, Actor-Critic \\
    \hline
    Exploration & Epsilon-greedy & Often more exploratory \\
    \hline
    Stability & Convergence issues & More stable updates \\
    \hline
    Sample Efficiency & More efficient & Less efficient \\
    \hline
    Action Type & Discrete & Continuous or discrete \\
    \hline
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}

    Both value-based and policy-based methods have distinct advantages and disadvantages. The choice between them should consider:
    \begin{itemize}
        \item The specifics of the problem domain
        \item Available computational resources
        \item Desired characteristics of the learning process
    \end{itemize}
    
    \textbf{Next Steps:} Transition into practical guidance on implementing policy gradients with examples in Python using TensorFlow/PyTorch in the next slide.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Policy Gradients - Introduction}
    \begin{block}{Introduction to Policy Gradients}
        \begin{itemize}
            \item Policy gradient methods parameterize and optimize the policy.
            \item Useful in environments with large or continuous action spaces.
            \item Directly optimize the policy instead of the value function.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Policy Gradients - Key Concepts}
    \begin{block}{Key Concepts}
        \begin{enumerate}
            \item \textbf{Policy}: A function defining actions given a state.
            \item \textbf{Objective Function}: Maximize expected reward:
            \begin{equation}
                J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
            \end{equation}
            \item \textbf{Gradient Ascent}: Update parameters:
            \begin{equation}
                \theta \leftarrow \theta + \alpha \nabla J(\theta)
            \end{equation}
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementing Policy Gradients}
    \begin{block}{Steps to Implement}
        \begin{enumerate}
            \item Initialize the environment and policy network.
            \item Collect trajectories by sampling actions from the policy.
            \item Compute returns for each state-action pair.
            \item Calculate the policy gradient using REINFORCE:
            \begin{equation}
                \nabla J(\theta) \approx \frac{1}{N} \sum_{t=0}^{N} (\nabla \log \pi_\theta(a_t | s_t))(R_t)
            \end{equation}
            \item Update the policy with computed gradients.
        \end{enumerate}
    \end{block}
    
    \begin{block}{Example Code Snippet}
    \begin{lstlisting}[language=Python]
# Example code using PyTorch for policy gradient
import torch
import torch.nn as nn
import torch.optim as optim
import gym

class PolicyNN(nn.Module):
    def __init__(self, state_size, action_size):
        super(PolicyNN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_size, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Softmax(dim=-1)
        )
    def forward(self, x):
        return self.fc(x)
# Additional setup and loop omitted for brevity
    \end{lstlisting}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Combining Value-Based and Policy-Based Approaches - Overview}
    \begin{block}{Overview}
        In reinforcement learning, two primary classes of algorithms dominate:
        \begin{itemize}
            \item \textbf{Value-Based} methods
            \item \textbf{Policy-Based} methods
        \end{itemize}
        This slide focuses on hybrid approaches, particularly the \textbf{Actor-Critic} method, which integrates the strengths of both categories.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Combining Value-Based and Policy-Based Approaches - Value-Based and Policy-Based Methods}
    \begin{block}{Value-Based Methods}
        \textbf{Definition}: Estimate the value function predicting future rewards based on state or state-action pairs.
        
        \textbf{Example}: Q-learning, which learns a Q-value function mapping state-action pairs to expected rewards.
    \end{block}
    
    \begin{block}{Policy-Based Methods}
        \textbf{Definition}: Directly parameterize the policy to decide actions based on the state, allowing more flexibility.
        
        \textbf{Example}: Policy gradient methods like REINFORCE adjust the policy based on performance in previous actions.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Combining Value-Based and Policy-Based Approaches - Actor-Critic Methods}
    \begin{block}{Actor-Critic Methods Overview}
        The Actor-Critic method is a hybrid approach utilizing:
        \begin{itemize}
            \item \textbf{Actor}: The policy network that selects actions.
            \item \textbf{Critic}: The value network that evaluates actions taken by the actor.
        \end{itemize}
    \end{block}
    
    \begin{block}{How It Works}
        \begin{enumerate}
            \item Actor selects an action based on the state using policy ($\pi$).
            \item Critic computes the value of the action and evaluates performance.
            \item Critic's feedback updates both the actor's policy and the critic's value function.
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Combining Value-Based and Policy-Based Approaches - Mathematical Foundation}
    \begin{block}{Policy Gradient Update}
        \begin{equation}
            \theta \leftarrow \theta + \alpha \nabla J(\theta)
        \end{equation}
        where: 
        \begin{itemize}
            \item $\theta$: parameters of the policy
            \item $\alpha$: learning rate
            \item $\nabla J(\theta)$: estimated using advantages from the critic
        \end{itemize}
    \end{block}

    \begin{block}{Value Function Update}
        \begin{equation}
            V(s) \leftarrow V(s) + \beta \delta
        \end{equation}
        where $\delta$ is:
        \begin{equation}
            \delta = r + \gamma V(s') - V(s)
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Combining Value-Based and Policy-Based Approaches - Advantages and Conclusion}
    \begin{block}{Advantages of Actor-Critic}
        \begin{itemize}
            \item \textbf{Stability and Efficiency}: Combines strengths of both methods.
            \item \textbf{Lower Variance}: Reduces variance in policy gradient estimates via the critic's input.
        \end{itemize}
    \end{block}

    \begin{block}{Key Points}
        \begin{itemize}
            \item Integration of both paradigms enhances efficiency.
            \item Real-world applicability in robotics, gaming, etc.
            \item Foundations for advanced techniques like A3C and DDPG.
        \end{itemize}
    \end{block}

    \textbf{Conclusion}: Blending explorative policy-based methods with evaluative value-based techniques creates robust reinforcement learning models ideal for complex environments.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning - Introduction}
    Deep Reinforcement Learning (DRL) is a robust approach for solving complex decision-making problems across various domains, fusing the strengths of deep learning and reinforcement learning.
    
    \begin{block}{Key Applications of DRL}
        \begin{enumerate}
            \item Game Playing
            \item Robotics
            \item Autonomous Vehicles
            \item Finance and Trading
            \item Healthcare
        \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning - Game Playing}
    \begin{itemize}
        \item \textbf{Case Study: AlphaGo}
            \begin{itemize}
                \item Developed by DeepMind, AlphaGo uses deep neural networks and reinforcement learning to play Go.
                \item First AI to defeat a professional human player.
                \item \textbf{Key Takeaway:} DRL excels in mastering complex strategies in vast state and action spaces.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning - Robotics and Autonomous Vehicles}
    \begin{itemize}
        \item \textbf{Robotics: OpenAI’s Dactyl}
            \begin{itemize}
                \item Trained to manipulate objects through DRL.
                \item Learns dexterous skills in simulated environments, which transfer to real-world applications.
                \item \textbf{Key Takeaway:} DRL enhances adaptability through trial and error.
            \end{itemize}
            
        \item \textbf{Autonomous Vehicles: Waymo}
            \begin{itemize}
                \item Uses DRL for optimizing real-time traffic decision-making.
                \item Adapts to dynamic conditions, enhancing safety and navigation.
                \item \textbf{Key Takeaway:} DRL significantly improves decision-making in complex environments.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning - Finance and Healthcare}
    \begin{itemize}
        \item \textbf{Finance: Algorithmic Trading}
            \begin{itemize}
                \item Creates adaptive trading algorithms to optimize buy/sell strategies based on historical data.
                \item \textbf{Key Takeaway:} DRL enhances investment outcomes through continuous learning.
            \end{itemize}
            
        \item \textbf{Healthcare: Personalized Treatment Plans}
            \begin{itemize}
                \item Tailors treatment plans by learning from diverse patient data over time.
                \item \textbf{Key Takeaway:} DRL allows for dynamic and personalized healthcare strategies.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Applications of Deep Reinforcement Learning - Conclusion and Code Snippet}
    \begin{block}{Conclusion}
        The applications of Deep Reinforcement Learning are impactful across various fields. DRL facilitates intelligent systems that learn in real-time, providing innovative solutions to complex problems.
    \end{block}
    
    \begin{block}{Key Points to Remember}
        \begin{itemize}
            \item DRL combines reinforcement learning feedback with deep learning's representation.
            \item Operates effectively in high-dimensional state spaces.
            \item Its adaptability positions DRL as a leading technology for future advancements across fields.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code Snippet: Simple DRL Setup}
    \begin{lstlisting}[language=Python]
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Neural Network for Q-Value approximation
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 24)
        self.fc2 = nn.Linear(24, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# Initialize environment and model
env = gym.make('CartPole-v1')
model = DQN(env.observation_space.shape[0], env.action_space.n)
optimizer = optim.Adam(model.parameters())
    \end{lstlisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Introduction}
    \begin{block}{Overview}
        Deep Reinforcement Learning (DRL) continues to evolve, showcasing remarkable successes in various applications. This section presents emerging trends and research areas that will influence DRL's future.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Key Trends}
    \begin{itemize}
        \item **Scalability of Algorithms**
            \begin{itemize}
                \item Challenges in scaling complex environments.
                \item Example: Hierarchical Reinforcement Learning (HRL) for task decomposition.
            \end{itemize}
        \item **Sample Efficiency**
            \begin{itemize}
                \item Reducing training data requirements.
                \item Example: Model-Based Reinforcement Learning for environment modeling.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Additional Trends}
    \begin{itemize}
        \item **Transfer Learning**
            \begin{itemize}
                \item Knowledge transfer improves learning across domains.
                \item Example: Pre-training on simpler tasks (e.g., Pong) to tackle more complex tasks (e.g., Dota 2).
            \end{itemize}
        \item **Incorporating Human Feedback**
            \begin{itemize}
                \item Human feedback aligns learning with human values.
                \item Example: Deep TAMER framework integrating human preferences.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Continued Trends}
    \begin{itemize}
        \item **Robustness and Safety**
            \begin{itemize}
                \item Ensuring safe operation in unpredictable environments.
                \item Example: Policies focusing on safe exploration, crucial for applications like autonomous vehicles.
            \end{itemize}
        \item **Interdisciplinary Approaches**
            \begin{itemize}
                \item Incorporating insights from neuroscience and cognitive science.
                \item Example: Utilizing neurobiology principles to inform DRL architecture and learning.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Final Trends}
    \begin{itemize}
        \item **Ethics and Fairness**
            \begin{itemize}
                \item Addressing ethical concerns in DRL applications.
                \item Example: Guidelines to mitigate biases and promote equitable outcomes.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Future Directions in Deep Reinforcement Learning - Conclusion and Key Points}
    \begin{block}{Key Takeaways}
        \begin{itemize}
            \item The DRL landscape is dynamic and full of potential breakthroughs.
            \item Scalability, efficiency, and safety are vital for future advancements.
            \item Collaborative research across disciplines will enrich DRL models.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Interactive Discussion \& Q\&A - Introduction}
  % Introduction to the discussion session
  \begin{block}{Introduction to the Session}
    This slide marks a pivotal moment in our exploration of Deep Reinforcement Learning (DRL). 
    It is designed to foster an interactive environment where thoughts and inquiries can flourish. 
    Engaging in discussion will reinforce your understanding and allow for the exchange of diverse perspectives and insights.
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Interactive Discussion \& Q\&A - Key Discussion Points}
  % Discussion points to guide the session
  \begin{block}{Key Discussion Points}
    \begin{itemize}
      \item \textbf{Understanding Deep Reinforcement Learning (DRL):}
      \begin{itemize}
        \item DRL combines reinforcement learning with deep learning techniques.
        \item Enables agents to learn optimal behaviors through trial and error.
      \end{itemize}
      \item \textbf{Recent Trends:}
      \begin{itemize}
        \item Reflect on trends covered earlier (e.g., Transfer Learning, Multi-Agent Systems).
      \end{itemize}
      \item \textbf{Applications of DRL:}
      \begin{itemize}
        \item Identify real-world applications in robotics, gaming, and finance.
      \end{itemize}
      \item \textbf{Implementing DRL:}
      \begin{itemize}
        \item Discuss challenges and common pitfalls in DRL.
      \end{itemize}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Interactive Discussion \& Q\&A - Conclusion and Participation}
  % Encourage participation and mention concluding points
  \begin{block}{Key Questions to Consider}
    \begin{itemize}
      \item What specific aspects of DRL do you find most intriguing or complex?
      \item Can you share an example from your own experience that relates to DRL principles?
      \item How do you envision the ethical implications of DRL applications in society?
    \end{itemize}
  \end{block}

  \begin{block}{Encouragement to Participate}
    The floor is open for your contributions! 
    Feel free to ask questions, share insights, or suggest examples related to the discussed concepts.
  \end{block}

  \begin{block}{Conclusion}
    This interactive session is an opportunity for growth and collaboration.
    Engaging thoughtfully can solidify DRL principles and broaden our collective understanding. 
  \end{block}

  \begin{block}{Reminder}
    As we transition into the Q\&A, please respect others' contributions and ensure everyone has a chance to participate.
  \end{block}
\end{frame}


\end{document}