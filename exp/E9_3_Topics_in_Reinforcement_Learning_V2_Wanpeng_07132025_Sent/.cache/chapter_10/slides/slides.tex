\documentclass[aspectratio=169]{beamer}

% Theme and Color Setup
\usetheme{Madrid}
\usecolortheme{whale}
\useinnertheme{rectangles}
\useoutertheme{miniframes}

% Additional Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{positioning}
\usepackage{hyperref}

% Custom Colors
\definecolor{myblue}{RGB}{31, 73, 125}
\definecolor{mygray}{RGB}{100, 100, 100}
\definecolor{mygreen}{RGB}{0, 128, 0}
\definecolor{myorange}{RGB}{230, 126, 34}
\definecolor{mycodebackground}{RGB}{245, 245, 245}

% Set Theme Colors
\setbeamercolor{structure}{fg=myblue}
\setbeamercolor{frametitle}{fg=white, bg=myblue}
\setbeamercolor{title}{fg=myblue}
\setbeamercolor{section in toc}{fg=myblue}
\setbeamercolor{item projected}{fg=white, bg=myblue}
\setbeamercolor{block title}{bg=myblue!20, fg=myblue}
\setbeamercolor{block body}{bg=myblue!10}
\setbeamercolor{alerted text}{fg=myorange}

% Set Fonts
\setbeamerfont{title}{size=\Large, series=\bfseries}
\setbeamerfont{frametitle}{size=\large, series=\bfseries}
% Additional Beamer font settings can be added as needed

% Title Page Information
\title[Performance Metrics in RL]{Week 10: Performance Metrics in Reinforcement Learning}
\author[J. Smith]{John Smith, Ph.D.}
\institute[University Name]{
  Department of Computer Science\\
  University Name\\
  \vspace{0.3cm}
  Email: email@university.edu\\
  Website: www.university.edu
}
\date{\today}

% Document Start
\begin{document}

\frame{\titlepage}

\begin{frame}[fragile]
    \titlepage
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overview of Performance Metrics}
    \begin{block}{Definition}
        Performance metrics are essential tools used to evaluate the effectiveness of reinforcement learning (RL) models. They provide quantitative measures that help in understanding how well an agent is performing in a given environment.
    \end{block}
    \begin{block}{Key Topics}
        \begin{itemize}
            \item Importance of performance metrics
            \item Common types of performance metrics
            \item Key concepts related to their application
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Importance of Performance Metrics}
    \begin{enumerate}
        \item \textbf{Objective Evaluation}
            \begin{itemize}
                \item Provides an objective way to assess model performance.
            \end{itemize}
        \item \textbf{Guiding Model Selection}
            \begin{itemize}
                \item Helps in identifying the most effective model for a specific task.
            \end{itemize}
        \item \textbf{Identifying Areas for Improvement}
            \begin{itemize}
                \item Highlights weaknesses and informs further tuning.
            \end{itemize}
        \item \textbf{Benchmarking}
            \begin{itemize}
                \item Allows comparison against standard models to track progress.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Performance Metrics in RL}
    \begin{enumerate}
        \item \textbf{Cumulative Reward}
            \begin{block}{Definition}
                The total reward achieved by the agent over a specific episode:
                \begin{equation}
                    G_t = R_t + R_{t+1} + R_{t+2} + \ldots
                \end{equation}
            \end{block}
            \begin{block}{Example}
                If rewards are 3, 5, and -2, then:
                \[
                G_t = 3 + 5 - 2 = 6
                \]
            \end{block}
        \item \textbf{Average Reward}
            \begin{block}{Definition}
                The mean reward obtained over many episodes, indicating long-term performance stability.
            \end{block}
        \item \textbf{Success Rate}
            \begin{block}{Definition}
                The percentage of episodes where the agent successfully achieves its objective.
            \end{block}
            \begin{block}{Example}
                An agent completes the task in 8 out of 10 trials, yielding a success rate of 80\%.
            \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Common Performance Metrics in RL (Continued)}
    \begin{enumerate}
        \setcounter{enumi}{3}
        \item \textbf{Learning Curve}
            \begin{block}{Definition}
                A graphical representation showing how the agent's performance improves over time or iterations.
            \end{block}
            \begin{block}{Importance}
                Useful for visualizing convergence and understanding the learning speed.
            \end{block}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points to Emphasize}
    \begin{itemize}
        \item The choice of performance metric significantly impacts the perceived effectiveness of an RL model.
        \item Different tasks may require different metrics that align with specific objectives.
        \item Metrics like cumulative rewards provide a clear performance picture, while others may focus on stability and efficiency.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Conclusion}
    \begin{block}{Summary}
        Performance metrics are pivotal in reinforcement learning, serving as a foundation for evaluating and improving models. Understanding these metrics enhances your ability to critically assess RL agents and their learning processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Rewards - Definition}
    \begin{block}{Definition}
        Cumulative rewards, also known as total returns, are the sum of all rewards an agent receives over time during its interactions with the environment in a reinforcement learning (RL) setting.
    \end{block}
    \begin{equation}
        G_t = r_t + r_{t+1} + r_{t+2} + \ldots + r_T
    \end{equation}
    where \( T \) represents the final time step considered.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Rewards - Significance}
    \begin{itemize}
        \item \textbf{Performance Indicator:} Cumulative rewards serve as a primary metric for evaluating the performance of an RL agent. The goal is to maximize this reward.
        
        \item \textbf{Comparison Across Strategies:} They provide a quantifiable measure of different RL policies and their success over time.
        
        \item \textbf{Policy Evaluation:} Analyzing cumulative rewards allows for assessment of the value of policies across multiple episodes.
        
        \item \textbf{Discounting Future Rewards:} To reflect present value, cumulative rewards may incorporate a discount factor \( \gamma \) (0 < \( \gamma \) < 1):
        \begin{equation}
            G_t^{\gamma} = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
        \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Cumulative Rewards - Example Scenario}
    Consider an agent navigating through a maze:
    \begin{itemize}
        \item Rewards: +10 for reaching the goal, -1 for hitting a wall.
        \item Reward sequence: \( r_0 = 0, r_1 = 0, r_2 = 10 \).
    \end{itemize}
    The cumulative reward can be calculated as:
    \begin{equation}
        G_0 = 0 + 0 + 10 = 10
    \end{equation}
    This total indicates the agent's success in maximizing its goal.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Convergence Rates - Overview}
    \begin{block}{Definition}
        Convergence rates refer to the speed at which a reinforcement learning (RL) algorithm approaches its optimal solution or policy, indicating how quickly the algorithm's performance stabilizes.
    \end{block}
    \begin{block}{Importance}
        \begin{itemize}
            \item \textbf{Efficiency of Learning:} Faster convergence means quicker learning of optimal strategies.
            \item \textbf{Resource Optimization:} Quick convergence is vital in resource-constrained environments.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Convergence Rates - Key Concepts}
    \begin{enumerate}
        \item \textbf{Convergence Types:}
            \begin{itemize}
                \item \textbf{Pointwise Convergence:} Approaching a specific value with iterations.
                \item \textbf{Asymptotic Convergence:} Analyzing long-term behavior as iterations increase infinitely.
            \end{itemize}
        \item \textbf{Factors Affecting Convergence:}
            \begin{itemize}
                \item \textbf{Learning Rate (α):} Influences how quickly an agent learns.
                \item \textbf{Exploration vs. Exploitation:} Balance is crucial for effective action discovery.
                \item \textbf{State and Action Space Complexity:} Larger spaces usually slow down convergence.
            \end{itemize}
        \item \textbf{Convergence Theorems:} Conditions under which algorithms like Q-learning ensure convergence to optimal policy.
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Understanding Convergence Rates - Example & Implications}
    \begin{block}{Example Scenario}
        \begin{itemize}
            \item In a grid world, measure the convergence rate by episodes needed for the average cumulative reward to stabilize.
            \begin{itemize}
                \item \textbf{Fast Convergence:} Efficient exploration identifies the goal quickly.
                \item \textbf{Slow Convergence:} Stuck in local optima or exploring suboptimal paths extends learning time.
            \end{itemize}
        \end{itemize}
    \end{block}
    \begin{block}{Implications for Model Performance}
        \begin{itemize}
            \item \textbf{Training Time:} Faster convergence reduces overall training time.
            \item \textbf{Generalization Ability:} Quick convergence leads to more robust models.
            \item \textbf{Performance Guarantee:} Understanding convergence informs reliability of policies.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting in RL Models - Understanding Overfitting}
    \begin{block}{Definition}
        Overfitting occurs when a model learns not just the underlying patterns in the training data but also the noise and outliers.
    \end{block}
    \begin{block}{Implications}
        This results in a model that performs well on training data but poorly on unseen test data.
    \end{block}
    \begin{itemize}
        \item \textbf{Illustration:} A curve that perfectly fits all the training points may wobble excessively, leading to poor performance on new data points.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting in RL Models - Causes and Effects}
    \begin{block}{Overfitting in Reinforcement Learning}
        In RL, overfitting can occur when:
        \begin{itemize}
            \item The policy (\(\pi\)) or value function (\(Q\)) becomes too specialized to the training environment.
            \item Limited exploration leads to lack of robust strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Causes of Overfitting}
        \begin{itemize}
            \item Complex models may lead to memorization rather than generalization.
            \item Insufficient training data results in models that do not perform well under diverse conditions.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Overfitting in RL Models - Impact and Mitigation Strategies}
    \begin{block}{Impacts of Overfitting}
        \begin{itemize}
            \item Reduced generalization can lead to a model that excels in training simulations but fails in real-world scenarios.
            \item Increased maintenance costs as overfit models require continuous adjustments as environments change.
        \end{itemize}
    \end{block}
    
    \begin{block}{Strategies to Mitigate Overfitting}
        \begin{itemize}
            \item \textbf{Regularization Techniques:}
                \begin{itemize}
                    \item Dropout and weight regularization (L1 or L2).
                \end{itemize}
            \item \textbf{Ensemble Methods:} Averages predictions from multiple models to improve generalization.
            \item \textbf{Cross-Validation:} Splits training data into subsets for broader learning.
        \end{itemize}
    \end{block}
    
    \begin{block}{Conclusion}
        Understanding and mitigating overfitting is critical to developing effective RL models that adapt well to diverse real-world situations.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Validation Metrics - Introduction}
    \begin{block}{Overview}
        Validation metrics are essential for evaluating the performance of Reinforcement Learning (RL) models. They help identify how effectively a model is learning and adapting to its environment compared to more straightforward metrics used in supervised learning.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Validation Metrics - Key Metrics Part 1}
    \begin{enumerate}
        \item \textbf{Cumulative Reward (Return), \(G_t\)}:
        \begin{itemize}
            \item \textbf{Definition:} Total reward from time step \(t\) onward.
            \item \textbf{Formula:}
            \begin{equation}
                G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \ldots
            \end{equation}
            where \(R\) is the reward and \(\gamma\) (0 < \(\gamma\) < 1) is the discount factor.
            \item \textbf{Example:} If rewards are [1, 2, 3] with \(\gamma = 0.9\), then:
            \begin{equation}
                G_t = 3 + 0.9 \times 2 + 0.9^2 \times 1 \approx 4.71.
            \end{equation}
        \end{itemize}

        \item \textbf{Average Reward}:
        \begin{itemize}
            \item \textbf{Definition:} Mean of cumulative rewards over episodes.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Average Reward} = \frac{1}{N} \sum_{i=1}^{N} G_{t,i}
            \end{equation}
            where \(N\) is the number of episodes.
            \item \textbf{Example:} Cumulative rewards [10, 20, 15, 25, 30] yield:
            \begin{equation}
                \text{Average Reward} = \frac{10 + 20 + 15 + 25 + 30}{5} = 20.
            \end{equation}
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Validation Metrics - Key Metrics Part 2}
    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from the previous frame
        \item \textbf{Success Rate}:
        \begin{itemize}
            \item \textbf{Definition:} Proportion of episodes where the agent achieves its goal.
            \item \textbf{Formula:}
            \begin{equation}
                \text{Success Rate} = \frac{\text{Number of Successes}}{\text{Total Episodes}}
            \end{equation}
            \item \textbf{Example:} For 8 successes in 10 episodes:
            \begin{equation}
                \text{Success Rate} = \frac{8}{10} = 0.8 \text{ or } 80\%.
            \end{equation}
        \end{itemize}

        \item \textbf{Training Efficiency}:
        \begin{itemize}
            \item Measure of how quickly an agent learns an effective policy relative to interactions or episodes.
            \item Evaluated by the number of episodes to reach a predefined average reward threshold.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Validation Metrics - Importance and Conclusion}
    \begin{block}{Importance of Validation Metrics}
        \begin{itemize}
            \item \textbf{Performance Insight:} Indicates agent's performance to guide improvements.
            \item \textbf{Model Selection:} Aids in comparing models or algorithms for effectiveness in specific environments.
            \item \textbf{Debugging:} Helps identify issues from deviations in expected metric values.
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Understanding and accurately applying validation metrics in RL is critical for assessing model performance and robustness, facilitating data-informed enhancements to strategies.
    \end{block}
    
    \begin{block}{Key Takeaway}
        Utilizing appropriate validation metrics is vital for effectively monitoring RL model performance, emphasizing cumulative rewards, success rates, and training efficiency for achieving learning objectives.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Introduction}
    \begin{block}{Introduction to Performance Metrics in RL}
        Performance metrics are essential for evaluating the effectiveness of RL algorithms. 
        Different metrics can provide insights into various aspects of agent performance, 
        including efficiency, robustness, and adaptability. 
        This slide compares key metrics commonly used in RL and discusses their suitability for various applications.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Types of Performance Metrics}
    \begin{enumerate}
        \item \textbf{Cumulative Reward (CR)}
        \begin{itemize}
            \item \textbf{Explanation}: The total reward accumulated by the agent over a specified episode or time frame.
            \item \textbf{Suitability}: Used in high-level evaluations across diverse environments. 
            Helps assess overall learning performance but may overlook subtler behavioral aspects.
        \end{itemize}

        \item \textbf{Average Reward}
        \begin{itemize}
            \item \textbf{Explanation}: The mean reward received over an episode, calculated as 
            \( \text{Average Reward} = \frac{1}{N} \sum_{t=0}^{N-1} R_t \).
            \item \textbf{Suitability}: Useful for comparing agents across episodes and helps identify consistent performance levels.
        \end{itemize}

        \item \textbf{Goal Achievement Rate (GAR)}
        \begin{itemize}
            \item \textbf{Explanation}: The percentage of episodes in which the agent achieves defined goals.
            \item \textbf{Suitability}: Effective for tasks with clear objectives, providing a binary view of success.
        \end{itemize}

        \item \textbf{Time to Convergence (TTC)}
        \begin{itemize}
            \item \textbf{Explanation}: The duration taken by an agent to reach stable performance (i.e., when the learning curve plateaus).
            \item \textbf{Suitability}: Important for evaluating model efficiency, especially in real-time systems.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Illustrated Comparison}
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Metric} & \textbf{Pros} & \textbf{Cons} & \textbf{Best Suited For} \\
        \hline
        Cumulative Reward & Simple interpretation & No insight into variance & General performance evaluation \\
        \hline
        Average Reward & Smooths out noise & Can mask failures in critical episodes & Average performance assessment \\
        \hline
        Goal Achievement Rate & Clear success/failure metric & Ignores partial achievements & Task-oriented evaluations \\
        \hline
        Time to Convergence & Indicates learning efficiency & May not reflect overall successful learning & Real-time application scenarios \\
        \hline
    \end{tabular}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparison of Metrics - Key Points and Conclusion}
    \begin{block}{Key Points to Emphasize}
        \begin{itemize}
            \item The choice of performance metric significantly influences the interpretation of the agent's performance.
            \item A combination of multiple metrics is often advisable for a holistic view.
            \item Context is crucial; metrics may have different implications depending on the application domain (e.g., gaming vs. robotics).
        \end{itemize}
    \end{block}

    \begin{block}{Conclusion}
        Incorporating a variety of performance metrics enables better assessment, tuning, and understanding of RL models. 
        Identifying appropriate metrics based on application requirements can lead to improved outcomes and effective learning processes.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Real-World Examples - Introduction}
    \begin{block}{Introduction to Performance Metrics in RL}
        Performance metrics in Reinforcement Learning (RL) are essential for evaluating and comparing algorithm effectiveness. 
        They provide insights into how well an agent learns and interacts with its environment, measured through:
        \begin{itemize}
            \item Cumulative rewards
            \item Convergence speed
            \item Policy efficiency
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Autonomous Vehicles}
    \begin{block}{Context}
        In the development of self-driving cars, RL is utilized for making real-time driving decisions.
    \end{block}
    \begin{itemize}
        \item \textbf{Performance Metrics Used:}
        \begin{itemize}
            \item Cumulative Reward: Overall safety (avoiding accidents) and efficiency (completion time).
            \item Success Rate: Percentage of tasks completed without incidents.
        \end{itemize}
        \item \textbf{Example Analysis:} 
        A reward structure that incentivizes safe driving while minimizing travel time is analyzed based on cumulative rewards and success rates.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Game Playing (AlphaGo)}
    \begin{block}{Context}
        AlphaGo employed RL to play Go, achieving superhuman performance.
    \end{block}
    \begin{itemize}
        \item \textbf{Performance Metrics Used:}
        \begin{itemize}
            \item Win Rate: Ratio of games won to games played.
            \item Move Quality: Evaluation of game state changes after each move.
        \end{itemize}
        \item \textbf{Example Analysis:} 
        AlphaGo's performance was assessed through win rates and how its moves compared to expert-level play.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Case Study: Robotics}
    \begin{block}{Context}
        Robots trained for tasks such as grasping, walking, or navigation.
    \end{block}
    \begin{itemize}
        \item \textbf{Performance Metrics Used:}
        \begin{itemize}
            \item Task Completion Rate: Frequency of task success (e.g., picking up objects).
            \item Learning Efficiency: Time or episodes taken to learn a task.
        \end{itemize}
        \item \textbf{Example Analysis:} 
        Performance in grasping tasks can be monitored through task completion rates over repeated trials.
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Key Points & Conclusion}
    \begin{itemize}
        \item \textbf{Choosing the Right Metrics:} Essential for effective evaluation, depending on specific task goals.
        \item \textbf{Real-World Impact:} Innovations from RL and its metrics advance technology, enhancing areas like traffic systems and AI.
        \item \textbf{Continuous Improvement:} Monitoring allows for iterative enhancements in RL algorithms.
    \end{itemize}
    \begin{block}{Example Formula}
        Cumulative rewards over time (T):
        \[
        R_{total} = \sum_{t=0}^{T} r_t
        \]
        where \( r_t \) is the reward received at time \( t \).
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 1}
    \begin{block}{Understanding Performance Metrics in Reinforcement Learning (RL)}
        In reinforcement learning, performance metrics are crucial for evaluating and improving the effectiveness of learning algorithms. They quantify how well an agent interacts with its environment and achieves its goals.
    \end{block}
    
    \begin{block}{Key Concepts}
        \begin{itemize}
            \item \textbf{Performance Metrics Defined}
            \begin{itemize}
                \item \textbf{Cumulative Reward}: Total reward accumulated over time.
                \item \textbf{Success Rate}: Percentage of episodes achieving the desired outcome.
                \item \textbf{Learning Efficiency}: Speed at which an agent learns to achieve high rewards.
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 2}
    \begin{block}{Importance of Metrics}
        \begin{itemize}
            \item Compare effectiveness of different algorithms.
            \item Fine-tune parameters for optimal performance.
            \item Provide feedback to enhance learning strategies.
        \end{itemize}
    \end{block}
    
    \begin{block}{Examples}
        \textbf{Real-World Application: Autonomous Driving}
        - Metrics include safety (accidents), efficiency (fuel consumption), and completion time.
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Summary and Key Takeaways - Part 3}
    \begin{block}{Key Takeaways}
        \begin{enumerate}
            \item Use multiple metrics for a holistic evaluation.
            \item Recognize the dynamic nature of environments.
            \item Ensure metrics correlate with meaningful agent behavior.
            \item Benchmark against established standards.
        \end{enumerate}
    \end{block}

    \begin{block}{Illustrative Formula}
        Cumulative Reward Formula:
        \begin{equation}
            R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots
        \end{equation}
        Where:
        \begin{itemize}
            \item \( R_t \): cumulative reward at time \( t \)
            \item \( r_t \): reward received at time \( t \)
            \item \( \gamma \): discount factor (0 < \( \gamma \) ≤ 1)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}[fragile,plain]{Conclusion}
    Understanding and effectively utilizing performance metrics in reinforcement learning is imperative for developing effective and reliable agents. The right metrics guide improvements and ensure agents perform well in complex, dynamic environments.
\end{frame}


\end{document}